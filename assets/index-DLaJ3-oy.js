var e=Object.defineProperty,t=(t,n,i)=>((t,n,i)=>n in t?e(t,n,{enumerable:!0,configurable:!0,writable:!0,value:i}):t[n]=i)(t,"symbol"!=typeof n?n+"":n,i);import{r as n,c as i,a,b as o,w as r,d as s,F as l,e as c,T as d,o as p,f as u,t as m,g as h,h as g,i as f,j as b,k as y,v,l as w,m as x,n as _,p as k,q as A,s as S,u as I}from"./vendor-BwYalJMr.js";!function(){const e=document.createElement("link").relList;if(!(e&&e.supports&&e.supports("modulepreload"))){for(const e of document.querySelectorAll('link[rel="modulepreload"]'))t(e);new MutationObserver((e=>{for(const n of e)if("childList"===n.type)for(const e of n.addedNodes)"LINK"===e.tagName&&"modulepreload"===e.rel&&t(e)})).observe(document,{childList:!0,subtree:!0})}function t(e){if(e.ep)return;e.ep=!0;const t=function(e){const t={};return e.integrity&&(t.integrity=e.integrity),e.referrerPolicy&&(t.referrerPolicy=e.referrerPolicy),"use-credentials"===e.crossOrigin?t.credentials="include":"anonymous"===e.crossOrigin?t.credentials="omit":t.credentials="same-origin",t}(e);fetch(e.href,t)}}();const T=(e,t)=>{const n=e.__vccOpts||e;for(const[i,a]of t)n[i]=a;return n},E={class:"bg-gradient-to-r from-gray-900 to-gray-800 p-4 relative z-50 shadow-lg border-b border-gray-700"},C={class:"container mx-auto flex items-center justify-between px-4"},M={class:"w-6 h-6 text-white",fill:"none",stroke:"currentColor",viewBox:"0 0 24 24"},L={key:0,"stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"2",d:"M4 6h16M4 12h16M4 18h16"},j={key:1,"stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"2",d:"M6 18L18 6M6 6l12 12"},R={class:"hidden md:flex items-center space-x-6 text-gray-300"},U={key:0,class:"fixed inset-0 bg-gray-900 bg-opacity-95 backdrop-blur-sm flex flex-col items-center justify-center z-50"},D={class:"flex flex-col items-center space-y-8"},P=T({__name:"Navigation",setup(e){const t=n(!1),g=()=>t.value=!1,f=[{name:"Revers",path:"/revers"},{name:"Coup Droit",path:"/coup-droit"},{name:"Service&Retour",path:"/service&retour"},{name:"Filet",path:"/filet"},{name:"Footwork",path:"/footwork"},{name:"Tactique",path:"/tactique"},{name:"Musculation",path:"/musculation"}];return(e,n)=>{const b=s("router-link");return p(),i("nav",E,[a("div",C,[o(b,{to:"/",class:"group"},{default:r((()=>n[1]||(n[1]=[a("h1",{class:"text-white text-xl font-bold flex items-center space-x-3 transition-transform duration-300 transform group-hover:scale-105"},[a("svg",{xmlns:"http://www.w3.org/2000/svg",fill:"none",viewBox:"0 0 24 24","stroke-width":"1.5",stroke:"currentColor",class:"w-8 h-8 text-indigo-400"},[a("path",{"stroke-linecap":"round","stroke-linejoin":"round",d:"m2.25 12 8.954-8.955c.44-.439 1.152-.439 1.591 0L21.75 12M4.5 9.75v10.125c0 .621.504 1.125 1.125 1.125H9.75v-4.875c0-.621.504-1.125 1.125-1.125h2.25c.621 0 1.125.504 1.125 1.125V21h4.125c.621 0 1.125-.504 1.125-1.125V9.75M8.25 21h8.25"})]),a("span",{class:"bg-gradient-to-r from-indigo-400 to-blue-400 bg-clip-text text-transparent"}," BadmintonTraining ")],-1)]))),_:1}),a("button",{onClick:n[0]||(n[0]=e=>t.value=!t.value),class:"md:hidden p-2 rounded-lg transition-colors duration-200 hover:bg-gray-700 focus:outline-none focus:ring-2 focus:ring-indigo-500"},[(p(),i("svg",M,[t.value?(p(),i("path",j)):(p(),i("path",L))]))]),a("ul",R,[(p(),i(l,null,c(f,((e,t)=>a("li",{key:t},[o(b,{to:e.path,class:"px-3 py-2 rounded-md hover:text-white hover:bg-gray-700 transition-all duration-200"},{default:r((()=>[u(m(e.name),1)])),_:2},1032,["to"])]))),64)),a("li",null,[o(b,{to:"/entrainement",class:"px-4 py-2 bg-gradient-to-r from-indigo-600 to-indigo-500 text-white rounded-lg font-semibold transition-all duration-300 hover:from-indigo-500 hover:to-indigo-400 shadow-lg hover:shadow-indigo-500/50 transform hover:scale-105"},{default:r((()=>n[2]||(n[2]=[u(" Entraînement ")]))),_:1})])])]),o(d,{name:"slide-fade"},{default:r((()=>[t.value?(p(),i("div",U,[a("ul",D,[(p(),i(l,null,c(f,((e,t)=>a("li",{key:t},[o(b,{onClick:g,to:e.path,class:"text-gray-300 text-xl hover:text-white transition-colors duration-200"},{default:r((()=>[u(m(e.name),1)])),_:2},1032,["to"])]))),64)),a("li",null,[o(b,{onClick:g,to:"/entrainement",class:"px-6 py-3 bg-gradient-to-r from-indigo-600 to-indigo-500 text-white rounded-lg font-semibold text-xl shadow-lg hover:shadow-indigo-500/50 transition-all duration-300 hover:from-indigo-500 hover:to-indigo-400"},{default:r((()=>n[3]||(n[3]=[u(" Entraînement ")]))),_:1})])]),a("button",{onClick:g,class:"mt-12 w-12 h-12 flex items-center justify-center rounded-full border-2 border-white/80 text-white text-2xl hover:bg-white hover:text-gray-900 transition-all duration-300 transform hover:rotate-90 focus:outline-none"}," × ")])):h("",!0)])),_:1})])}}},[["__scopeId","data-v-96dcd3ee"]]),q={__name:"Accueil",setup:e=>(e,t)=>(p(),i("div",null,t[0]||(t[0]=[g('<div class="flex justify-center items-center h-screen text-center px-4"><div class="max-w-xl w-full"><h1 class="text-2xl font-semibold mb-4 sm:text-3xl md:text-4xl">Vidéo entraînement de badminton</h1><div class="text-lg sm:text-xl"><p class="mb-2">Source:</p><div class="space-y-2"><a href="https://www.instagram.com/rupeshp_bad/" target="_blank" class="text-blue-500 hover:underline"> rupeshp_bad </a><a href="https://www.instagram.com/birdie_badminton/reels/" target="_blank" class="text-blue-500 hover:underline"> birdie_badminton </a></div></div></div></div>',1)])))},N={__name:"App",setup:e=>(e,t)=>{const n=s("router-view");return p(),i(l,null,[o(P),o(n)],64)}},$={class:"flex flex-col items-center text-center max-w-4xl mx-auto border-b border-black-300 pb-8 mb-8"},O={class:"text-2xl font-semibold mb-4"},z={class:"flex flex-col md:flex-row items-center md:items-start gap-6 w-full justify-center"},B={controls:"",class:"rounded-lg shadow-lg w-80"},F=["src"],V={class:"text-left text-gray-700 max-w-md"};const H=T({props:{title:String,videoSrc:String,source:String,author:String}},[["render",function(e,t,n,o,r,s){return p(),i("div",$,[a("h2",O,m(n.title),1),a("div",z,[a("video",B,[a("source",{src:n.videoSrc,type:"video/mp4"},null,8,F),t[0]||(t[0]=u(" Votre navigateur ne supporte pas la lecture de vidéos. "))]),a("p",V," Auteur: "+m(n.author),1)])])}]]),G={class:"min-h-screen bg-white p-8"},Q={class:"text-center mb-12"},W={class:"text-4xl font-bold text-transparent bg-clip-text bg-gradient-to-r from-indigo-400 to-blue-400 mb-4"},K={class:"text-gray-400 max-w-2xl mx-auto"},J={class:"grid grid-cols-1 md:grid-cols-2 gap-8 max-w-7xl mx-auto"},X=T({__name:"TemplatePage",props:{title:{type:String,required:!0},description:{type:String,required:!0},videos:{type:Array,required:!0}},setup:e=>(t,n)=>(p(),i("div",G,[a("div",Q,[a("h1",W,m(e.title),1),a("p",K,m(e.description),1)]),a("div",J,[(p(!0),i(l,null,c(e.videos,((e,t)=>(p(),f(H,{key:t,title:e.title,videoSrc:e.videoSrc,author:e.author,source:e.source,class:"transform hover:scale-105 transition-transform duration-300"},null,8,["title","videoSrc","author","source"])))),128))])]))},[["__scopeId","data-v-ed5f7284"]]),Y=T({__name:"Revers",setup(e){const t=[{title:"Améliorer son amortie au fond de court",videoSrc:"/BadmintonTraining//videos/revers/backhand_close.mp4",author:"birdie_badminton",source:"",description:"Techniques et exercices pour perfectionner vos amorties revers"},{title:"Quand prendre son revers",videoSrc:"/BadmintonTraining//videos/revers/when_take_backhand.mp4",author:"birdie_badminton",source:"",description:"Apprenez à identifier les situations idéales pour utiliser votre revers"}];return(e,n)=>(p(),f(X,{title:"Le Revers",description:"Découvrez les techniques essentielles pour maîtriser votre revers au badminton",videos:t}))}},[["__scopeId","data-v-d13b0859"]]),Z=T({__name:"CoupDroit",setup(e){const t=[{title:"Smash avec saut en ciseaux",videoSrc:"/BadmintonTraining/videos/coupdroit/forehead_scissor_jump.mp4",author:"rupeshp_bad",source:"",description:"Maîtrisez la technique du smash avec saut en ciseaux"},{title:"Coup droit en retard",videoSrc:"/BadmintonTraining/videos/coupdroit/late_forehead_cross_clear.mp4",author:"rupeshp_bad",source:"",description:"Techniques pour gérer les situations de retard"},{title:"Puissance dans le smash",videoSrc:"/BadmintonTraining/videos/coupdroit/power_smash.mp4",author:"rupeshp_bad",source:"",description:"Développez la puissance de vos smashs"},{title:"Précision dans le smash",videoSrc:"/BadmintonTraining/videos/coupdroit/smash_accuracy.mp4",author:"rupeshp_bad",source:"",description:"Améliorez la précision de vos smashs"}];return(e,n)=>(p(),f(X,{videos:t,title:"Le Coup Droit",description:"Perfectionnez votre technique de coup droit et améliorez votre puissance de frappe"}))}},[["__scopeId","data-v-026f0a0c"]]),ee=T({__name:"Footwork",setup(e){const t=[{title:"Déplacement après un smash",videoSrc:"/BadmintonTraining/videos/footwork/fast_after_a_smash.mp4",author:"birdie_badminton",source:"",description:"Optimisez votre repositionnement après un smash"},{title:"Exercices de Footwork",videoSrc:"/BadmintonTraining/videos/footwork/footwork1.mp4",author:"rupeshp_bad",source:"",description:"Techniques fondamentales de déplacement"},{title:"Exercices de Footwork",videoSrc:"/BadmintonTraining/videos/footwork/footwork2.mp4",author:"rupeshp_bad",source:"",description:"Techniques avancées pour améliorer votre mobilité"},{title:"Exercice en shadow",videoSrc:"/BadmintonTraining/videos/footwork/shadow_training.mp4",author:"rupeshp_bad",source:"",description:"Entraînement shadow pour perfectionner vos déplacements"}];return(e,n)=>(p(),f(X,{title:"Déplacements et Footwork",description:"Améliorez votre rapidité et votre agilité sur le terrain avec ces exercices de déplacement",videos:t}))}},[["__scopeId","data-v-481969ec"]]),te=T({__name:"Service&Return",setup(e){const t=[{title:"Service Lobé",videoSrc:"/BadmintonTraining/videos/service/flick_servce.mp4",author:"rupeshp_bad",source:"",description:"Maîtrisez la technique du service lobé"},{title:"Retour de Service",videoSrc:"/BadmintonTraining/videos/service/service_receive.mp4",author:"rupeshp_bad",source:"",description:"Les fondamentaux du retour de service"},{title:"Retour de Service",videoSrc:"/BadmintonTraining/videos/service/service_receive2.mp4",author:"rupeshp_bad",source:"",description:"Techniques avancées pour améliorer vos retours"}];return(e,n)=>(p(),f(X,{title:"Service et Retour",description:"Maîtrisez les techniques de service et améliorez vos retours",videos:t}))}},[["__scopeId","data-v-f6efb7c8"]]),ne=T({__name:"Tactics",setup(e){const t=[{title:"Tactiques de Match",videoSrc:"/BadmintonTraining/videos/tactique/match_tactics.mp4",author:"rupeshp_bad",source:"",description:"Stratégies essentielles pour dominer vos matchs"}];return(e,n)=>(p(),f(X,{title:"Tactiques de Match",description:"Découvrez les stratégies et tactiques essentielles pour améliorer votre jeu",videos:t}))}},[["__scopeId","data-v-dbd94cc8"]]),ie=T({__name:"Filet",setup(e){const t=[{title:"Tir Déceptif",videoSrc:"/BadmintonTraining/videos/filet/deceptive_shoot.mp4",author:"rupeshp_bad",source:"",description:"Apprenez à masquer vos intentions au filet"},{title:"Variations Coup Droit",videoSrc:"/BadmintonTraining/videos/filet/forehand_straight_drop.mp4",author:"rupeshp_bad",source:"",description:"Différentes techniques de coup droit au filet"},{title:"Variations Revers",videoSrc:"/BadmintonTraining/videos/filet/forehead_straight_drop.mp4",author:"rupeshp_bad",source:"",description:"Maîtrisez les variations de revers au filet"},{title:"Déceptions Fluides",videoSrc:"/BadmintonTraining/videos/filet/smooth_deception.mp4",author:"rupeshp_bad",source:"",description:"Techniques avancées de déception au filet"}];return(e,n)=>(p(),f(X,{videos:t,title:"Techniques au Filet",description:"Maîtrisez les coups au filet et améliorez votre jeu proche du filet"}))}},[["__scopeId","data-v-c3f02b29"]]),ae=T({__name:"Muscu",setup(e){const t=[{title:"Fentes Bulgares",videoSrc:"/BadmintonTraining/videos/gym/gymExercice1.mp4",author:"birdie_badminton",source:"",description:"Renforcez vos jambes avec cet exercice efficace"},{title:"Rotation de la Hanche",videoSrc:"/BadmintonTraining/videos/gym/hips_rotation_exercise.mp4",author:"birdie_badminton",source:"",description:"Améliorez votre mobilité et votre stabilité"}];return(e,n)=>(p(),f(X,{title:"Exercices de Musculation",description:"Renforcez votre condition physique avec des exercices spécifiques pour le badminton",videos:t}))}},[["__scopeId","data-v-4c05e83b"]]),oe={class:"flex flex-col items-center p-4 justify-center bg-gray-700 h-screen text-white"},re={key:0,class:"mb-4"},se={class:"flex space-x-2"},le=["onClick"],ce={key:1,class:"mb-4 space-y-4"},de={class:"flex space-x-4"},pe={class:"text-6xl font-bold mb-4"},ue={class:"text-xl mb-4"},me={key:0,class:"text-yellow-500"},he={key:0,class:"text-green-500"},ge={key:1,class:"text-red-500"},fe={class:"space-x-4"},be={__name:"Timer",setup(e){const t=[{name:"30/30",work:30,rest:30,rounds:5},{name:"45/15",work:45,rest:15,rounds:6},{name:"20/10",work:20,rest:10,rounds:8},{name:"40/20",work:40,rest:20,rounds:4}],o=n(30),r=n(30),s=n(5),d=n(0),g=n(1),f=n(!0),w=n(!1),x=n(!1),_=n(5);let k=null;const A=n(null),S=(e=800,t=200,n=.5)=>{if(!A.value)return;const i=A.value.createOscillator(),a=A.value.createGain();i.connect(a),a.connect(A.value.destination),i.frequency.value=e,a.gain.value=n,i.start(A.value.currentTime),i.stop(A.value.currentTime+t/1e3)},I=()=>S(800,200,.5),T=()=>S(600,400,.5),E=e=>{const t=e%60;return`${Math.floor(e/60).toString().padStart(2,"0")}:${t.toString().padStart(2,"0")}`},C=()=>{x.value=!0,_.value=5;const e=setInterval((()=>{_.value>0?(S(700,100,.3),_.value--):(clearInterval(e),x.value=!1,M())}),1e3)},M=()=>{w.value=!0,d.value=o.value,f.value=!0,I(),k=setInterval(L,1e3)},L=()=>{d.value>0?d.value--:f.value?(T(),f.value=!1,d.value=r.value):g.value<s.value?(I(),g.value++,f.value=!0,d.value=o.value):(T(),R())},j=()=>{clearInterval(k),w.value=!1},R=()=>{clearInterval(k),w.value=!1,x.value=!1,d.value=0,g.value=1,f.value=!0};return b((()=>{clearInterval(k)})),(e,n)=>(p(),i("div",oe,[n[7]||(n[7]=a("div",{class:"mb-6"},[a("svg",{xmlns:"http://www.w3.org/2000/svg",class:"w-24 h-24 text-blue-400 opacity-90",fill:"none",viewBox:"0 0 24 24",stroke:"currentColor"},[a("path",{"stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"2",d:"M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"})])],-1)),w.value?h("",!0):(p(),i("div",re,[a("div",se,[(p(),i(l,null,c(t,((e,t)=>a("button",{key:t,onClick:t=>(e=>{o.value=e.work,r.value=e.rest,s.value=e.rounds})(e),class:"px-3 py-1 bg-blue-500 rounded hover:bg-blue-600 text-sm"},m(e.name),9,le))),64))])])),w.value?h("",!0):(p(),i("div",ce,[a("div",de,[a("div",null,[n[4]||(n[4]=a("label",{class:"block text-sm"},"Temps de travail (s)",-1)),y(a("input",{type:"number","onUpdate:modelValue":n[0]||(n[0]=e=>o.value=e),class:"w-24 px-2 py-1 text-black bg-white rounded",min:"1"},null,512),[[v,o.value]])]),a("div",null,[n[5]||(n[5]=a("label",{class:"block text-sm"},"Temps de repos (s)",-1)),y(a("input",{type:"number","onUpdate:modelValue":n[1]||(n[1]=e=>r.value=e),class:"w-24 px-2 py-1 text-black bg-white rounded",min:"1"},null,512),[[v,r.value]])]),a("div",null,[n[6]||(n[6]=a("label",{class:"block text-sm"},"Nombre de séries",-1)),y(a("input",{type:"number","onUpdate:modelValue":n[2]||(n[2]=e=>s.value=e),class:"w-24 px-2 py-1 text-black bg-white rounded",min:"1"},null,512),[[v,s.value]])])])])),a("div",pe,[x.value?(p(),i(l,{key:0},[u(m(_.value),1)],64)):(p(),i(l,{key:1},[u(m(E(d.value)),1)],64))]),a("div",ue,[x.value?(p(),i("span",me,"PRÉPARATION")):w.value?(p(),i(l,{key:1},[f.value?(p(),i("span",he,"TRAVAIL")):(p(),i("span",ge,"REPOS")),a("span",null,"- Série "+m(g.value)+"/"+m(s.value),1)],64)):h("",!0)]),a("div",fe,[w.value||x.value?h("",!0):(p(),i("button",{key:0,onClick:n[3]||(n[3]=()=>{A.value||(A.value=new(window.AudioContext||window.webkitAudioContext)),w.value||x.value||C()}),class:"bg-green-500 px-4 py-2 rounded hover:bg-green-600"}," Démarrer ")),w.value&&!x.value?(p(),i("button",{key:1,onClick:j,class:"bg-yellow-500 px-4 py-2 rounded hover:bg-yellow-600"}," Pause ")):h("",!0),a("button",{onClick:R,class:"bg-red-500 px-4 py-2 rounded hover:bg-red-600"}," Reset ")])]))}},ye={class:"container mx-auto px-4 py-8"},ve={class:"grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6"};const we=T({},[["render",function(e,t){const n=s("router-link");return p(),i("div",ye,[t[3]||(t[3]=a("h1",{class:"text-3xl font-bold text-white mb-8 text-center"},"Zone d'Entraînement",-1)),a("div",ve,[o(n,{to:"/timer",class:"block bg-gray-800 rounded-lg p-6 hover:bg-gray-700 transition-colors duration-300"},{default:r((()=>t[0]||(t[0]=[a("div",{class:"flex items-center mb-4"},[a("svg",{xmlns:"http://www.w3.org/2000/svg",class:"h-8 w-8 text-blue-500",fill:"none",viewBox:"0 0 24 24",stroke:"currentColor"},[a("path",{"stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"2",d:"M12 8v4l3 3m6-3a9 9 0 11-18 0 9 9 0 0118 0z"})]),a("h2",{class:"text-xl font-semibold text-white ml-3"},"Timer d'Entraînement")],-1),a("p",{class:"text-gray-400"}," Configurez vos intervalles d'entraînement avec des temps de travail et de repos personnalisés. ",-1)]))),_:1}),o(n,{to:"/shadow",class:"block bg-gray-800 rounded-lg p-6"},{default:r((()=>t[1]||(t[1]=[a("div",{class:"flex items-center mb-4"},[a("svg",{xmlns:"http://www.w3.org/2000/svg",class:"h-8 w-8 text-green-500",fill:"none",viewBox:"0 0 24 24",stroke:"currentColor"},[a("path",{"stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"2",d:"M19 11H5m14 0a2 2 0 012 2v6a2 2 0 01-2 2H5a2 2 0 01-2-2v-6a2 2 0 012-2m14 0V9a2 2 0 00-2-2M5 11V9a2 2 0 012-2m0 0V5a2 2 0 012-2h6a2 2 0 012 2v2M7 7h10"})]),a("h2",{class:"text-xl font-semibold text-white ml-3"},"Shadow")],-1),a("p",{class:"text-gray-400"}," Entraînement shadow avec direction aléatoire. ",-1)]))),_:1}),o(n,{to:"/gpt-training",class:"block bg-gray-800 rounded-lg p-6"},{default:r((()=>t[2]||(t[2]=[a("div",{class:"flex items-center mb-4"},[a("svg",{xmlns:"http://www.w3.org/2000/svg",class:"h-8 w-8 text-purple-500",fill:"none",viewBox:"0 0 24 24",stroke:"currentColor"},[a("path",{"stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"2",d:"M13 10V3L4 14h7v7l9-11h-7z"})]),a("h2",{class:"text-xl font-semibold text-white ml-3"},"Assistant IA")],-1),a("p",{class:"text-gray-400"}," Générez des entraînements personnalisés avec l'aide de l'IA. ",-1)]))),_:1})])])}]]),xe={class:"flex flex-col items-center justify-center min-h-screen bg-gray-800 p-4"},_e={key:0,class:"mb-8 space-y-4 bg-gray-700 p-4 rounded-lg"},ke={class:"flex space-x-4"},Ae={key:1,class:"relative"},Se={class:"absolute -top-16 left-1/2 transform -translate-x-1/2 text-3xl text-white font-mono"},Ie={key:0,class:"mt-8 text-4xl font-bold text-white text-center"},Te={key:2,class:"mt-8 space-x-4"},Ee=T({__name:"Shadow",setup(e){const t=n(1e3),o=n(!1),r=n(!1),s=n(!1),l=n("up"),c=n("text-white"),d=n(0);let u=null,g=null;const f=["up","up-right","right","down-right","down","down-left","left","up-left"],k=["text-white","text-blue-400","text-green-400","text-yellow-400","text-red-400","text-purple-400"],A=e=>{const t=e%60;return`${Math.floor(e/60).toString().padStart(2,"0")}:${t.toString().padStart(2,"0")}`},S=w((()=>({up:"rotate-0","up-right":"rotate-45",right:"rotate-90","down-right":"rotate-135",down:"rotate-180","down-left":"-rotate-135",left:"-rotate-90","up-left":"-rotate-45"}[l.value]))),I=w((()=>({up:"BAS","up-right":"BAS-GAUCHE",right:"GAUCHE","down-right":"HAUT-GAUCHE",down:"HAUT","down-left":"HAUT-DROITE",left:"DROITE","up-left":"BAS-DROITE"}[l.value]))),T=()=>{const e=f[Math.floor(Math.random()*f.length)],t=k[Math.floor(Math.random()*k.length)];l.value=e,c.value=t},E=()=>{r.value=!0,s.value=!1,d.value=0,T(),u=setInterval(T,t.value),g=setInterval((()=>{s.value||d.value++}),1e3)},C=()=>{s.value=!s.value,s.value?(clearInterval(u),u=null):u=setInterval(T,t.value)},M=()=>{clearInterval(u),clearInterval(g),u=null,g=null,r.value=!1,s.value=!1,d.value=0};return b((()=>{u&&clearInterval(u),g&&clearInterval(g)})),(e,n)=>(p(),i("div",xe,[r.value?h("",!0):(p(),i("div",_e,[a("div",ke,[a("div",null,[n[2]||(n[2]=a("label",{class:"block text-white mb-2"},"Intervalle (ms)",-1)),y(a("input",{type:"number","onUpdate:modelValue":n[0]||(n[0]=e=>t.value=e),class:"w-24 px-2 py-1 rounded bg-white",min:"500",step:"100"},null,512),[[v,t.value]])]),a("div",null,[n[3]||(n[3]=a("label",{class:"block text-white mb-2"},"Afficher opposé",-1)),y(a("input",{type:"checkbox","onUpdate:modelValue":n[1]||(n[1]=e=>o.value=e),class:"w-5 h-5"},null,512),[[x,o.value]])])]),a("button",{onClick:E,class:"bg-green-500 text-white px-4 py-2 rounded hover:bg-green-600"}," Démarrer ")])),r.value?(p(),i("div",Ae,[a("div",Se,m(A(d.value)),1),(p(),i("svg",{class:_(["w-64 h-64 transition-transform duration-200",c.value,S.value]),viewBox:"0 0 24 24",fill:"none",stroke:"currentColor","stroke-width":"2.5"},n[4]||(n[4]=[a("path",{"stroke-linecap":"round","stroke-linejoin":"round",d:"M5 10l7-7m0 0l7 7m-7-7v18"},null,-1)]),2)),o.value?(p(),i("div",Ie,m(I.value),1)):h("",!0)])):h("",!0),r.value?(p(),i("div",Te,[a("button",{onClick:C,class:_(["px-4 py-2 rounded text-white",s.value?"bg-green-500 hover:bg-green-600":"bg-yellow-500 hover:bg-yellow-600"])},m(s.value?"Reprendre":"Pause"),3),a("button",{onClick:M,class:"bg-red-500 text-white px-4 py-2 rounded hover:bg-red-600"}," Arrêter ")])):h("",!0)]))}},[["__scopeId","data-v-dc483e09"]]);var Ce=Object.freeze({Text:"Text",NumericLiteral:"NumericLiteral",BooleanLiteral:"BooleanLiteral",NullLiteral:"NullLiteral",StringLiteral:"StringLiteral",Identifier:"Identifier",Equals:"Equals",OpenParen:"OpenParen",CloseParen:"CloseParen",OpenStatement:"OpenStatement",CloseStatement:"CloseStatement",OpenExpression:"OpenExpression",CloseExpression:"CloseExpression",OpenSquareBracket:"OpenSquareBracket",CloseSquareBracket:"CloseSquareBracket",OpenCurlyBracket:"OpenCurlyBracket",CloseCurlyBracket:"CloseCurlyBracket",Comma:"Comma",Dot:"Dot",Colon:"Colon",Pipe:"Pipe",CallOperator:"CallOperator",AdditiveBinaryOperator:"AdditiveBinaryOperator",MultiplicativeBinaryOperator:"MultiplicativeBinaryOperator",ComparisonBinaryOperator:"ComparisonBinaryOperator",UnaryOperator:"UnaryOperator",Set:"Set",If:"If",For:"For",In:"In",Is:"Is",NotIn:"NotIn",Else:"Else",EndSet:"EndSet",EndIf:"EndIf",ElseIf:"ElseIf",EndFor:"EndFor",And:"And",Or:"Or",Not:"UnaryOperator",Macro:"Macro",EndMacro:"EndMacro"}),Me=Object.freeze({set:Ce.Set,for:Ce.For,in:Ce.In,is:Ce.Is,if:Ce.If,else:Ce.Else,endset:Ce.EndSet,endif:Ce.EndIf,elif:Ce.ElseIf,endfor:Ce.EndFor,and:Ce.And,or:Ce.Or,not:Ce.Not,"not in":Ce.NotIn,macro:Ce.Macro,endmacro:Ce.EndMacro,true:Ce.BooleanLiteral,false:Ce.BooleanLiteral,none:Ce.NullLiteral,True:Ce.BooleanLiteral,False:Ce.BooleanLiteral,None:Ce.NullLiteral}),Le=class{constructor(e,t){this.value=e,this.type=t}};function je(e){return/\w/.test(e)}function Re(e){return/[0-9]/.test(e)}var Ue=[["{%",Ce.OpenStatement],["%}",Ce.CloseStatement],["{{",Ce.OpenExpression],["}}",Ce.CloseExpression],["(",Ce.OpenParen],[")",Ce.CloseParen],["{",Ce.OpenCurlyBracket],["}",Ce.CloseCurlyBracket],["[",Ce.OpenSquareBracket],["]",Ce.CloseSquareBracket],[",",Ce.Comma],[".",Ce.Dot],[":",Ce.Colon],["|",Ce.Pipe],["<=",Ce.ComparisonBinaryOperator],[">=",Ce.ComparisonBinaryOperator],["==",Ce.ComparisonBinaryOperator],["!=",Ce.ComparisonBinaryOperator],["<",Ce.ComparisonBinaryOperator],[">",Ce.ComparisonBinaryOperator],["+",Ce.AdditiveBinaryOperator],["-",Ce.AdditiveBinaryOperator],["*",Ce.MultiplicativeBinaryOperator],["/",Ce.MultiplicativeBinaryOperator],["%",Ce.MultiplicativeBinaryOperator],["=",Ce.Equals]],De=new Map([["n","\n"],["t","\t"],["r","\r"],["b","\b"],["f","\f"],["v","\v"],["'","'"],['"','"'],["\\","\\"]]);var Pe=class{constructor(){t(this,"type","Statement")}},qe=class extends Pe{constructor(e){super(),t(this,"type","Program"),this.body=e}},Ne=class extends Pe{constructor(e,n,i){super(),t(this,"type","If"),this.test=e,this.body=n,this.alternate=i}},$e=class extends Pe{constructor(e,n,i,a){super(),t(this,"type","For"),this.loopvar=e,this.iterable=n,this.body=i,this.defaultBlock=a}},Oe=class extends Pe{constructor(e,n,i){super(),t(this,"type","Set"),this.assignee=e,this.value=n,this.body=i}},ze=class extends Pe{constructor(e,n,i){super(),t(this,"type","Macro"),this.name=e,this.args=n,this.body=i}},Be=class extends Pe{constructor(){super(...arguments),t(this,"type","Expression")}},Fe=class extends Be{constructor(e,n,i){super(),t(this,"type","MemberExpression"),this.object=e,this.property=n,this.computed=i}},Ve=class extends Be{constructor(e,n){super(),t(this,"type","CallExpression"),this.callee=e,this.args=n}},He=class extends Be{constructor(e){super(),t(this,"type","Identifier"),this.value=e}},Ge=class extends Be{constructor(e){super(),t(this,"type","Literal"),this.value=e}},Qe=class extends Ge{constructor(){super(...arguments),t(this,"type","NumericLiteral")}},We=class extends Ge{constructor(){super(...arguments),t(this,"type","StringLiteral")}},Ke=class extends Ge{constructor(){super(...arguments),t(this,"type","BooleanLiteral")}},Je=class extends Ge{constructor(){super(...arguments),t(this,"type","NullLiteral")}},Xe=class extends Ge{constructor(){super(...arguments),t(this,"type","ArrayLiteral")}},Ye=class extends Ge{constructor(){super(...arguments),t(this,"type","TupleLiteral")}},Ze=class extends Ge{constructor(){super(...arguments),t(this,"type","ObjectLiteral")}},et=class extends Be{constructor(e,n,i){super(),t(this,"type","BinaryExpression"),this.operator=e,this.left=n,this.right=i}},tt=class extends Be{constructor(e,n){super(),t(this,"type","FilterExpression"),this.operand=e,this.filter=n}},nt=class extends Be{constructor(e,n){super(),t(this,"type","SelectExpression"),this.iterable=e,this.test=n}},it=class extends Be{constructor(e,n,i){super(),t(this,"type","TestExpression"),this.operand=e,this.negate=n,this.test=i}},at=class extends Be{constructor(e,n){super(),t(this,"type","UnaryExpression"),this.operator=e,this.argument=n}},ot=class extends Be{constructor(e=void 0,n=void 0,i=void 0){super(),t(this,"type","SliceExpression"),this.start=e,this.stop=n,this.step=i}},rt=class extends Be{constructor(e,n){super(),t(this,"type","KeywordArgumentExpression"),this.key=e,this.value=n}};function st(e){const t=new qe([]);let n=0;function i(t,i){const a=e[n++];if(!a||a.type!==t)throw new Error(`Parser Error: ${i}. ${a.type} !== ${t}.`);return a}function a(){switch(e[n].type){case Ce.Text:return new We(i(Ce.Text,"Expected text token").value);case Ce.OpenStatement:return function(){let t;switch(i(Ce.OpenStatement,"Expected opening statement token"),e[n].type){case Ce.Set:++n,t=function(){var t,o;const s=c();if(r(Ce.Equals)){++n;const e=c();return new Oe(s,e,[])}{const r=[];for(i(Ce.CloseStatement,"Expected %} token");(null==(t=e[n])?void 0:t.type)!==Ce.OpenStatement||(null==(o=e[n+1])?void 0:o.type)!==Ce.EndSet;){const e=a();r.push(e)}return i(Ce.OpenStatement,"Expected {% token"),i(Ce.EndSet,"Expected endset token"),new Oe(s,null,r)}}(),i(Ce.CloseStatement,"Expected closing statement token");break;case Ce.If:++n,t=s(),i(Ce.OpenStatement,"Expected {% token"),i(Ce.EndIf,"Expected endif token"),i(Ce.CloseStatement,"Expected %} token");break;case Ce.Macro:++n,t=function(){const e=w();if("Identifier"!==e.type)throw new SyntaxError("Expected identifier following macro statement");const t=g();i(Ce.CloseStatement,"Expected closing statement token");const n=[];for(;o(Ce.OpenStatement,Ce.EndMacro);)n.push(a());return new ze(e,t,n)}(),i(Ce.OpenStatement,"Expected {% token"),i(Ce.EndMacro,"Expected endmacro token"),i(Ce.CloseStatement,"Expected %} token");break;case Ce.For:++n,t=function(){const e=l(!0);if(!(e instanceof He||e instanceof Ye))throw new SyntaxError(`Expected identifier/tuple for the loop variable, got ${e.type} instead`);i(Ce.In,"Expected `in` keyword following loop variable");const t=c();i(Ce.CloseStatement,"Expected closing statement token");const s=[];for(;o(Ce.OpenStatement,Ce.EndFor)&&o(Ce.OpenStatement,Ce.Else);)s.push(a());const d=[];if(r(Ce.OpenStatement,Ce.Else))for(++n,++n,i(Ce.CloseStatement,"Expected closing statement token");o(Ce.OpenStatement,Ce.EndFor);)d.push(a());return new $e(e,t,s,d)}(),i(Ce.OpenStatement,"Expected {% token"),i(Ce.EndFor,"Expected endfor token"),i(Ce.CloseStatement,"Expected %} token");break;default:throw new SyntaxError(`Unknown statement type: ${e[n].type}`)}return t}();case Ce.OpenExpression:return function(){i(Ce.OpenExpression,"Expected opening expression token");const e=c();return i(Ce.CloseExpression,"Expected closing expression token"),e}();default:throw new SyntaxError(`Unexpected token type: ${e[n].type}`)}}function o(...t){return n+t.length<=e.length&&t.some(((t,i)=>t!==e[n+i].type))}function r(...t){return n+t.length<=e.length&&t.every(((t,i)=>t===e[n+i].type))}function s(){var t,o,l,d,p,u,m,h;const g=c();i(Ce.CloseStatement,"Expected closing statement token");const f=[],b=[];for(;(null==(t=e[n])?void 0:t.type)!==Ce.OpenStatement||(null==(o=e[n+1])?void 0:o.type)!==Ce.ElseIf&&(null==(l=e[n+1])?void 0:l.type)!==Ce.Else&&(null==(d=e[n+1])?void 0:d.type)!==Ce.EndIf;)f.push(a());if((null==(p=e[n])?void 0:p.type)===Ce.OpenStatement&&(null==(u=e[n+1])?void 0:u.type)!==Ce.EndIf)if(++n,r(Ce.ElseIf))i(Ce.ElseIf,"Expected elseif token"),b.push(s());else for(i(Ce.Else,"Expected else token"),i(Ce.CloseStatement,"Expected closing statement token");(null==(m=e[n])?void 0:m.type)!==Ce.OpenStatement||(null==(h=e[n+1])?void 0:h.type)!==Ce.EndIf;)b.push(a());return new Ne(g,f,b)}function l(e=!1){const t=e?w:c,i=[t()],a=r(Ce.Comma);for(;a&&(++n,i.push(t()),r(Ce.Comma)););return a?new Ye(i):i[0]}function c(){return function(){const e=d();if(r(Ce.If)){++n;const t=d();if(r(Ce.Else)){++n;const i=d();return new Ne(t,[e],[i])}return new nt(e,t)}return e}()}function d(){let t=p();for(;r(Ce.Or);){const i=e[n];++n;const a=p();t=new et(i,t,a)}return t}function p(){let t=u();for(;r(Ce.And);){const i=e[n];++n;const a=u();t=new et(i,t,a)}return t}function u(){let t;for(;r(Ce.Not);){const i=e[n];++n;const a=u();t=new at(i,a)}return t??function(){let t=m();for(;r(Ce.ComparisonBinaryOperator)||r(Ce.In)||r(Ce.NotIn);){const i=e[n];++n;const a=m();t=new et(i,t,a)}return t}()}function m(){let t=y();for(;r(Ce.AdditiveBinaryOperator);){const i=e[n];++n;const a=y();t=new et(i,t,a)}return t}function h(e){let t=new Ve(e,g());return t=b(t),r(Ce.OpenParen)&&(t=h(t)),t}function g(){i(Ce.OpenParen,"Expected opening parenthesis for arguments list");const e=function(){const e=[];for(;!r(Ce.CloseParen);){let t=c();if(r(Ce.Equals)){if(++n,!(t instanceof He))throw new SyntaxError("Expected identifier for keyword argument");const e=c();t=new rt(t,e)}e.push(t),r(Ce.Comma)&&++n}return e}();return i(Ce.CloseParen,"Expected closing parenthesis for arguments list"),e}function f(){const e=[];let t=!1;for(;!r(Ce.CloseSquareBracket);)r(Ce.Colon)?(e.push(void 0),++n,t=!0):(e.push(c()),r(Ce.Colon)&&(++n,t=!0));if(0===e.length)throw new SyntaxError("Expected at least one argument for member/slice expression");if(t){if(e.length>3)throw new SyntaxError("Expected 0-3 arguments for slice expression");return new ot(...e)}return e[0]}function b(t){for(;r(Ce.Dot)||r(Ce.OpenSquareBracket);){const a=e[n];let o;++n;const r=a.type!==Ce.Dot;if(r)o=f(),i(Ce.CloseSquareBracket,"Expected closing square bracket");else if(o=w(),"Identifier"!==o.type)throw new SyntaxError("Expected identifier following dot operator");t=new Fe(t,o,r)}return t}function y(){let t=v();for(;r(Ce.MultiplicativeBinaryOperator);){const i=e[n];++n;const a=v();t=new et(i,t,a)}return t}function v(){let e=function(){let e=function(){const e=b(w());return r(Ce.OpenParen)?h(e):e}();for(;r(Ce.Pipe);){++n;let t=w();if(!(t instanceof He))throw new SyntaxError("Expected identifier for the filter");r(Ce.OpenParen)&&(t=h(t)),e=new tt(e,t)}return e}();for(;r(Ce.Is);){++n;const t=r(Ce.Not);t&&++n;let i=w();if(i instanceof Ke?i=new He(i.value.toString()):i instanceof Je&&(i=new He("none")),!(i instanceof He))throw new SyntaxError("Expected identifier for the test");e=new it(e,t,i)}return e}function w(){const t=e[n];switch(t.type){case Ce.NumericLiteral:return++n,new Qe(Number(t.value));case Ce.StringLiteral:return++n,new We(t.value);case Ce.BooleanLiteral:return++n,new Ke("true"===t.value.toLowerCase());case Ce.NullLiteral:return++n,new Je(null);case Ce.Identifier:return++n,new He(t.value);case Ce.OpenParen:{++n;const t=l();if(e[n].type!==Ce.CloseParen)throw new SyntaxError(`Expected closing parenthesis, got ${e[n].type} instead`);return++n,t}case Ce.OpenSquareBracket:{++n;const e=[];for(;!r(Ce.CloseSquareBracket);)e.push(c()),r(Ce.Comma)&&++n;return++n,new Xe(e)}case Ce.OpenCurlyBracket:{++n;const e=new Map;for(;!r(Ce.CloseCurlyBracket);){const t=c();i(Ce.Colon,"Expected colon between key and value in object literal");const a=c();e.set(t,a),r(Ce.Comma)&&++n}return++n,new Ze(e)}default:throw new SyntaxError(`Unexpected token: ${t.type}`)}}for(;n<e.length;)t.body.push(a());return t}function lt(e,t,n=1){void 0===t&&(t=e,e=0);const i=[];for(let a=e;a<t;a+=n)i.push(a);return i}function ct(e,t,n,i=1){const a=Math.sign(i);a>=0?(t=(t??(t=0))<0?Math.max(e.length+t,0):Math.min(t,e.length),n=(n??(n=e.length))<0?Math.max(e.length+n,0):Math.min(n,e.length)):(t=(t??(t=e.length-1))<0?Math.max(e.length+t,-1):Math.min(t,e.length-1),n=(n??(n=-1))<-1?Math.max(e.length+n,-1):Math.min(n,e.length-1));const o=[];for(let r=t;a*r<a*n;r+=i)o.push(e[r]);return o}function dt(e){return e.replace(/\b\w/g,(e=>e.toUpperCase()))}var pt=class{constructor(e=void 0){t(this,"type","RuntimeValue"),t(this,"value"),t(this,"builtins",new Map),this.value=e}__bool__(){return new ht(!!this.value)}},ut=class extends pt{constructor(){super(...arguments),t(this,"type","NumericValue")}},mt=class extends pt{constructor(){super(...arguments),t(this,"type","StringValue"),t(this,"builtins",new Map([["upper",new vt((()=>new mt(this.value.toUpperCase())))],["lower",new vt((()=>new mt(this.value.toLowerCase())))],["strip",new vt((()=>new mt(this.value.trim())))],["title",new vt((()=>new mt(dt(this.value))))],["length",new ut(this.value.length)],["rstrip",new vt((()=>new mt(this.value.trimEnd())))],["lstrip",new vt((()=>new mt(this.value.trimStart())))],["split",new vt((e=>{const t=e[0]??new wt;if(!(t instanceof mt||t instanceof wt))throw new Error("sep argument must be a string or null");const n=e[1]??new ut(-1);if(!(n instanceof ut))throw new Error("maxsplit argument must be a number");let i=[];if(t instanceof wt){const e=this.value.trimStart();for(const{0:t,index:a}of e.matchAll(/\S+/g)){if(-1!==n.value&&i.length>=n.value&&void 0!==a){i.push(t+e.slice(a+t.length));break}i.push(t)}}else{if(""===t.value)throw new Error("empty separator");i=this.value.split(t.value),-1!==n.value&&i.length>n.value&&i.push(i.splice(n.value).join(t.value))}return new bt(i.map((e=>new mt(e))))}))]]))}},ht=class extends pt{constructor(){super(...arguments),t(this,"type","BooleanValue")}},gt=class extends pt{constructor(){super(...arguments),t(this,"type","ObjectValue"),t(this,"builtins",new Map([["get",new vt((([e,t])=>{if(!(e instanceof mt))throw new Error(`Object key must be a string: got ${e.type}`);return this.value.get(e.value)??t??new wt}))],["items",new vt((()=>new bt(Array.from(this.value.entries()).map((([e,t])=>new bt([new mt(e),t]))))))]]))}__bool__(){return new ht(this.value.size>0)}},ft=class extends gt{constructor(){super(...arguments),t(this,"type","KeywordArgumentsValue")}},bt=class extends pt{constructor(){super(...arguments),t(this,"type","ArrayValue"),t(this,"builtins",new Map([["length",new ut(this.value.length)]]))}__bool__(){return new ht(this.value.length>0)}},yt=class extends bt{constructor(){super(...arguments),t(this,"type","TupleValue")}},vt=class extends pt{constructor(){super(...arguments),t(this,"type","FunctionValue")}},wt=class extends pt{constructor(){super(...arguments),t(this,"type","NullValue")}},xt=class extends pt{constructor(){super(...arguments),t(this,"type","UndefinedValue")}},_t=class{constructor(e){t(this,"variables",new Map([["namespace",new vt((e=>{if(0===e.length)return new gt(new Map);if(1!==e.length||!(e[0]instanceof gt))throw new Error("`namespace` expects either zero arguments or a single object argument");return e[0]}))]])),t(this,"tests",new Map([["boolean",e=>"BooleanValue"===e.type],["callable",e=>e instanceof vt],["odd",e=>{if("NumericValue"!==e.type)throw new Error(`Cannot apply test "odd" to type: ${e.type}`);return e.value%2!=0}],["even",e=>{if("NumericValue"!==e.type)throw new Error(`Cannot apply test "even" to type: ${e.type}`);return e.value%2==0}],["false",e=>"BooleanValue"===e.type&&!e.value],["true",e=>"BooleanValue"===e.type&&e.value],["none",e=>"NullValue"===e.type],["string",e=>"StringValue"===e.type],["number",e=>"NumericValue"===e.type],["integer",e=>"NumericValue"===e.type&&Number.isInteger(e.value)],["iterable",e=>"ArrayValue"===e.type||"StringValue"===e.type],["mapping",e=>"ObjectValue"===e.type],["lower",e=>{const t=e.value;return"StringValue"===e.type&&t===t.toLowerCase()}],["upper",e=>{const t=e.value;return"StringValue"===e.type&&t===t.toUpperCase()}],["none",e=>"NullValue"===e.type],["defined",e=>"UndefinedValue"!==e.type],["undefined",e=>"UndefinedValue"===e.type],["equalto",(e,t)=>e.value===t.value],["eq",(e,t)=>e.value===t.value]])),this.parent=e}set(e,t){return this.declareVariable(e,At(t))}declareVariable(e,t){if(this.variables.has(e))throw new SyntaxError(`Variable already declared: ${e}`);return this.variables.set(e,t),t}setVariable(e,t){return this.variables.set(e,t),t}resolve(e){if(this.variables.has(e))return this;if(this.parent)return this.parent.resolve(e);throw new Error(`Unknown variable: ${e}`)}lookupVariable(e){try{return this.resolve(e).variables.get(e)??new xt}catch{return new xt}}},kt=class{constructor(e){t(this,"global"),this.global=e??new _t}run(e){return this.evaluate(e,this.global)}evaluateBinaryExpression(e,t){const n=this.evaluate(e.left,t);switch(e.operator.value){case"and":return n.__bool__().value?this.evaluate(e.right,t):n;case"or":return n.__bool__().value?n:this.evaluate(e.right,t)}const i=this.evaluate(e.right,t);switch(e.operator.value){case"==":return new ht(n.value==i.value);case"!=":return new ht(n.value!=i.value)}if(n instanceof xt||i instanceof xt)throw new Error("Cannot perform operation on undefined values");if(n instanceof wt||i instanceof wt)throw new Error("Cannot perform operation on null values");if(n instanceof ut&&i instanceof ut)switch(e.operator.value){case"+":return new ut(n.value+i.value);case"-":return new ut(n.value-i.value);case"*":return new ut(n.value*i.value);case"/":return new ut(n.value/i.value);case"%":return new ut(n.value%i.value);case"<":return new ht(n.value<i.value);case">":return new ht(n.value>i.value);case">=":return new ht(n.value>=i.value);case"<=":return new ht(n.value<=i.value)}else if(n instanceof bt&&i instanceof bt){if("+"===e.operator.value)return new bt(n.value.concat(i.value))}else if(i instanceof bt){const t=void 0!==i.value.find((e=>e.value===n.value));switch(e.operator.value){case"in":return new ht(t);case"not in":return new ht(!t)}}if((n instanceof mt||i instanceof mt)&&"+"===e.operator.value)return new mt(n.value.toString()+i.value.toString());if(n instanceof mt&&i instanceof mt)switch(e.operator.value){case"in":return new ht(i.value.includes(n.value));case"not in":return new ht(!i.value.includes(n.value))}if(n instanceof mt&&i instanceof gt)switch(e.operator.value){case"in":return new ht(i.value.has(n.value));case"not in":return new ht(!i.value.has(n.value))}throw new SyntaxError(`Unknown operator "${e.operator.value}" between ${n.type} and ${i.type}`)}evaluateArguments(e,t){const n=[],i=new Map;for(const a of e)if("KeywordArgumentExpression"===a.type){const e=a;i.set(e.key.value,this.evaluate(e.value,t))}else{if(i.size>0)throw new Error("Positional arguments must come before keyword arguments");n.push(this.evaluate(a,t))}return[n,i]}evaluateFilterExpression(e,t){const n=this.evaluate(e.operand,t);if("Identifier"===e.filter.type){const t=e.filter;if("tojson"===t.value)return new mt(St(n));if(n instanceof bt)switch(t.value){case"list":return n;case"first":return n.value[0];case"last":return n.value[n.value.length-1];case"length":return new ut(n.value.length);case"reverse":return new bt(n.value.reverse());case"sort":return new bt(n.value.sort(((e,t)=>{if(e.type!==t.type)throw new Error(`Cannot compare different types: ${e.type} and ${t.type}`);switch(e.type){case"NumericValue":return e.value-t.value;case"StringValue":return e.value.localeCompare(t.value);default:throw new Error(`Cannot compare type: ${e.type}`)}})));case"join":return new mt(n.value.map((e=>e.value)).join(""));case"string":return new mt(St(n));default:throw new Error(`Unknown ArrayValue filter: ${t.value}`)}else if(n instanceof mt)switch(t.value){case"length":return new ut(n.value.length);case"upper":return new mt(n.value.toUpperCase());case"lower":return new mt(n.value.toLowerCase());case"title":return new mt(dt(n.value));case"capitalize":return new mt(n.value.charAt(0).toUpperCase()+n.value.slice(1));case"trim":return new mt(n.value.trim());case"indent":return new mt(n.value.split("\n").map(((e,t)=>0===t||0===e.length?e:"    "+e)).join("\n"));case"join":case"string":return n;default:throw new Error(`Unknown StringValue filter: ${t.value}`)}else{if(n instanceof ut){if("abs"===t.value)return new ut(Math.abs(n.value));throw new Error(`Unknown NumericValue filter: ${t.value}`)}if(n instanceof gt)switch(t.value){case"items":return new bt(Array.from(n.value.entries()).map((([e,t])=>new bt([new mt(e),t]))));case"length":return new ut(n.value.size);default:throw new Error(`Unknown ObjectValue filter: ${t.value}`)}}throw new Error(`Cannot apply filter "${t.value}" to type: ${n.type}`)}if("CallExpression"===e.filter.type){const i=e.filter;if("Identifier"!==i.callee.type)throw new Error(`Unknown filter: ${i.callee.type}`);const a=i.callee.value;if("tojson"===a){const[,e]=this.evaluateArguments(i.args,t),a=e.get("indent")??new wt;if(!(a instanceof ut||a instanceof wt))throw new Error("If set, indent must be a number");return new mt(St(n,a.value))}if("join"===a){let e;if(n instanceof mt)e=Array.from(n.value);else{if(!(n instanceof bt))throw new Error(`Cannot apply filter "${a}" to type: ${n.type}`);e=n.value.map((e=>e.value))}const[o,r]=this.evaluateArguments(i.args,t),s=o.at(0)??r.get("separator")??new mt("");if(!(s instanceof mt))throw new Error("separator must be a string");return new mt(e.join(s.value))}if(n instanceof bt){switch(a){case"selectattr":case"rejectattr":{const e="selectattr"===a;if(n.value.some((e=>!(e instanceof gt))))throw new Error(`\`${a}\` can only be applied to array of objects`);if(i.args.some((e=>"StringLiteral"!==e.type)))throw new Error(`arguments of \`${a}\` must be strings`);const[o,r,s]=i.args.map((e=>this.evaluate(e,t)));let l;if(r){const e=t.tests.get(r.value);if(!e)throw new Error(`Unknown test: ${r.value}`);l=e}else l=(...e)=>e[0].__bool__().value;const c=n.value.filter((t=>{const n=t.value.get(o.value),i=!!n&&l(n,s);return e?i:!i}));return new bt(c)}case"map":{const[,e]=this.evaluateArguments(i.args,t);if(e.has("attribute")){const t=e.get("attribute");if(!(t instanceof mt))throw new Error("attribute must be a string");const i=e.get("default"),a=n.value.map((e=>{if(!(e instanceof gt))throw new Error("items in map must be an object");return e.value.get(t.value)??i??new xt}));return new bt(a)}throw new Error("`map` expressions without `attribute` set are not currently supported.")}}throw new Error(`Unknown ArrayValue filter: ${a}`)}if(n instanceof mt){if("indent"===a){const[e,a]=this.evaluateArguments(i.args,t),o=e.at(0)??a.get("width")??new ut(4);if(!(o instanceof ut))throw new Error("width must be a number");const r=e.at(1)??a.get("first")??new ht(!1),s=e.at(2)??a.get("blank")??new ht(!1),l=n.value.split("\n"),c=" ".repeat(o.value),d=l.map(((e,t)=>!r.value&&0===t||!s.value&&0===e.length?e:c+e));return new mt(d.join("\n"))}throw new Error(`Unknown StringValue filter: ${a}`)}throw new Error(`Cannot apply filter "${a}" to type: ${n.type}`)}throw new Error(`Unknown filter: ${e.filter.type}`)}evaluateTestExpression(e,t){const n=this.evaluate(e.operand,t),i=t.tests.get(e.test.value);if(!i)throw new Error(`Unknown test: ${e.test.value}`);const a=i(n);return new ht(e.negate?!a:a)}evaluateUnaryExpression(e,t){const n=this.evaluate(e.argument,t);if("not"===e.operator.value)return new ht(!n.value);throw new SyntaxError(`Unknown operator: ${e.operator.value}`)}evalProgram(e,t){return this.evaluateBlock(e.body,t)}evaluateBlock(e,t){let n="";for(const i of e){const e=this.evaluate(i,t);"NullValue"!==e.type&&"UndefinedValue"!==e.type&&(n+=e.value)}return new mt(n)}evaluateIdentifier(e,t){return t.lookupVariable(e.value)}evaluateCallExpression(e,t){const[n,i]=this.evaluateArguments(e.args,t);i.size>0&&n.push(new ft(i));const a=this.evaluate(e.callee,t);if("FunctionValue"!==a.type)throw new Error(`Cannot call something that is not a function: got ${a.type}`);return a.value(n,t)}evaluateSliceExpression(e,t,n){if(!(e instanceof bt||e instanceof mt))throw new Error("Slice object must be an array or string");const i=this.evaluate(t.start,n),a=this.evaluate(t.stop,n),o=this.evaluate(t.step,n);if(!(i instanceof ut||i instanceof xt))throw new Error("Slice start must be numeric or undefined");if(!(a instanceof ut||a instanceof xt))throw new Error("Slice stop must be numeric or undefined");if(!(o instanceof ut||o instanceof xt))throw new Error("Slice step must be numeric or undefined");return e instanceof bt?new bt(ct(e.value,i.value,a.value,o.value)):new mt(ct(Array.from(e.value),i.value,a.value,o.value).join(""))}evaluateMemberExpression(e,t){const n=this.evaluate(e.object,t);let i,a;if(e.computed){if("SliceExpression"===e.property.type)return this.evaluateSliceExpression(n,e.property,t);i=this.evaluate(e.property,t)}else i=new mt(e.property.value);if(n instanceof gt){if(!(i instanceof mt))throw new Error(`Cannot access property with non-string: got ${i.type}`);a=n.value.get(i.value)??n.builtins.get(i.value)}else if(n instanceof bt||n instanceof mt)if(i instanceof ut)a=n.value.at(i.value),n instanceof mt&&(a=new mt(n.value.at(i.value)));else{if(!(i instanceof mt))throw new Error(`Cannot access property with non-string/non-number: got ${i.type}`);a=n.builtins.get(i.value)}else{if(!(i instanceof mt))throw new Error(`Cannot access property with non-string: got ${i.type}`);a=n.builtins.get(i.value)}return a instanceof pt?a:new xt}evaluateSet(e,t){const n=e.value?this.evaluate(e.value,t):this.evaluateBlock(e.body,t);if("Identifier"===e.assignee.type){const i=e.assignee.value;t.setVariable(i,n)}else{if("MemberExpression"!==e.assignee.type)throw new Error(`Invalid LHS inside assignment expression: ${JSON.stringify(e.assignee)}`);{const i=e.assignee,a=this.evaluate(i.object,t);if(!(a instanceof gt))throw new Error("Cannot assign to member of non-object");if("Identifier"!==i.property.type)throw new Error("Cannot assign to member with non-identifier property");a.value.set(i.property.value,n)}}return new wt}evaluateIf(e,t){const n=this.evaluate(e.test,t);return this.evaluateBlock(n.__bool__().value?e.body:e.alternate,t)}evaluateFor(e,t){const n=new _t(t);let i,a;if("SelectExpression"===e.iterable.type){const t=e.iterable;a=this.evaluate(t.iterable,n),i=t.test}else a=this.evaluate(e.iterable,n);if(!(a instanceof bt))throw new Error(`Expected iterable type in for loop: got ${a.type}`);const o=[],r=[];for(let c=0;c<a.value.length;++c){const t=new _t(n),s=a.value[c];let l;if("Identifier"===e.loopvar.type)l=t=>t.setVariable(e.loopvar.value,s);else{if("TupleLiteral"!==e.loopvar.type)throw new Error(`Invalid loop variable(s): ${e.loopvar.type}`);{const t=e.loopvar;if("ArrayValue"!==s.type)throw new Error(`Cannot unpack non-iterable type: ${s.type}`);const n=s;if(t.value.length!==n.value.length)throw new Error(`Too ${t.value.length>n.value.length?"few":"many"} items to unpack`);l=e=>{for(let i=0;i<t.value.length;++i){if("Identifier"!==t.value[i].type)throw new Error(`Cannot unpack non-identifier type: ${t.value[i].type}`);e.setVariable(t.value[i].value,n.value[i])}}}}if(i){l(t);if(!this.evaluate(i,t).__bool__().value)continue}o.push(s),r.push(l)}let s="",l=!0;for(let c=0;c<o.length;++c){const t=new Map([["index",new ut(c+1)],["index0",new ut(c)],["revindex",new ut(o.length-c)],["revindex0",new ut(o.length-c-1)],["first",new ht(0===c)],["last",new ht(c===o.length-1)],["length",new ut(o.length)],["previtem",c>0?o[c-1]:new xt],["nextitem",c<o.length-1?o[c+1]:new xt]]);n.setVariable("loop",new gt(t)),r[c](n);s+=this.evaluateBlock(e.body,n).value,l=!1}if(l){s+=this.evaluateBlock(e.defaultBlock,n).value}return new mt(s)}evaluateMacro(e,t){return t.setVariable(e.name.value,new vt(((t,n)=>{var i;const a=new _t(n);let o;"KeywordArgumentsValue"===(null==(i=(t=t.slice()).at(-1))?void 0:i.type)&&(o=t.pop());for(let r=0;r<e.args.length;++r){const n=e.args[r],i=t[r];if("Identifier"===n.type){const e=n;if(!i)throw new Error(`Missing positional argument: ${e.value}`);a.setVariable(e.value,i)}else{if("KeywordArgumentExpression"!==n.type)throw new Error(`Unknown argument type: ${n.type}`);{const e=n,t=i??(null==o?void 0:o.value.get(e.key.value))??this.evaluate(e.value,a);a.setVariable(e.key.value,t)}}}return this.evaluateBlock(e.body,a)}))),new wt}evaluate(e,t){if(void 0===e)return new xt;switch(e.type){case"Program":return this.evalProgram(e,t);case"Set":return this.evaluateSet(e,t);case"If":return this.evaluateIf(e,t);case"For":return this.evaluateFor(e,t);case"Macro":return this.evaluateMacro(e,t);case"NumericLiteral":return new ut(Number(e.value));case"StringLiteral":return new mt(e.value);case"BooleanLiteral":return new ht(e.value);case"NullLiteral":return new wt(e.value);case"ArrayLiteral":return new bt(e.value.map((e=>this.evaluate(e,t))));case"TupleLiteral":return new yt(e.value.map((e=>this.evaluate(e,t))));case"ObjectLiteral":{const n=new Map;for(const[i,a]of e.value){const e=this.evaluate(i,t);if(!(e instanceof mt))throw new Error(`Object keys must be strings: got ${e.type}`);n.set(e.value,this.evaluate(a,t))}return new gt(n)}case"Identifier":return this.evaluateIdentifier(e,t);case"CallExpression":return this.evaluateCallExpression(e,t);case"MemberExpression":return this.evaluateMemberExpression(e,t);case"UnaryExpression":return this.evaluateUnaryExpression(e,t);case"BinaryExpression":return this.evaluateBinaryExpression(e,t);case"FilterExpression":return this.evaluateFilterExpression(e,t);case"TestExpression":return this.evaluateTestExpression(e,t);default:throw new SyntaxError(`Unknown node type: ${e.type}`)}}};function At(e){switch(typeof e){case"number":return new ut(e);case"string":return new mt(e);case"boolean":return new ht(e);case"undefined":return new xt;case"object":return null===e?new wt:Array.isArray(e)?new bt(e.map(At)):new gt(new Map(Object.entries(e).map((([e,t])=>[e,At(t)]))));case"function":return new vt(((t,n)=>At(e(...t.map((e=>e.value)))??null)));default:throw new Error(`Cannot convert to runtime value: ${e}`)}}function St(e,t,n){const i=n??0;switch(e.type){case"NullValue":case"UndefinedValue":return"null";case"NumericValue":case"StringValue":case"BooleanValue":return JSON.stringify(e.value);case"ArrayValue":case"ObjectValue":{const n=t?" ".repeat(t):"",a="\n"+n.repeat(i),o=a+n;if("ArrayValue"===e.type){const n=e.value.map((e=>St(e,t,i+1)));return t?`[${o}${n.join(`,${o}`)}${a}]`:`[${n.join(", ")}]`}{const n=Array.from(e.value.entries()).map((([e,n])=>{const a=`"${e}": ${St(n,t,i+1)}`;return t?`${o}${a}`:a}));return t?`{${n.join(",")}${a}}`:`{${n.join(", ")}}`}}default:throw new Error(`Cannot convert to JSON: ${e.type}`)}}var It=class{constructor(e){t(this,"parsed");const n=function(e,t={}){var n,i,a;const o=[],r=function(e,t={}){return e.endsWith("\n")&&(e=e.slice(0,-1)),e=e.replace(/{#.*?#}/gs,"{##}"),t.lstrip_blocks&&(e=e.replace(/^[ \t]*({[#%])/gm,"$1")),t.trim_blocks&&(e=e.replace(/([#%]})\n/g,"$1")),e.replace(/{##}/g,"").replace(/-%}\s*/g,"%}").replace(/\s*{%-/g,"{%").replace(/-}}\s*/g,"}}").replace(/\s*{{-/g,"{{")}(e,t);let s=0;const l=e=>{let t="";for(;e(r[s]);)if("\\"!==r[s]){if(t+=r[s++],s>=r.length)throw new SyntaxError("Unexpected end of input")}else{if(++s,s>=r.length)throw new SyntaxError("Unexpected end of input");const e=r[s++],n=De.get(e);if(void 0===n)throw new SyntaxError(`Unexpected escaped character: ${e}`);t+=n}return t};e:for(;s<r.length;){const e=null==(n=o.at(-1))?void 0:n.type;if(void 0===e||e===Ce.CloseStatement||e===Ce.CloseExpression){let e="";for(;s<r.length&&("{"!==r[s]||"%"!==r[s+1]&&"{"!==r[s+1]);)e+=r[s++];if(e.length>0){o.push(new Le(e,Ce.Text));continue}}l((e=>/\s/.test(e)));const t=r[s];if("-"===t||"+"===t){const e=null==(i=o.at(-1))?void 0:i.type;if(e===Ce.Text||void 0===e)throw new SyntaxError(`Unexpected character: ${t}`);switch(e){case Ce.Identifier:case Ce.NumericLiteral:case Ce.BooleanLiteral:case Ce.NullLiteral:case Ce.StringLiteral:case Ce.CloseParen:case Ce.CloseSquareBracket:break;default:{++s;const e=l(Re);o.push(new Le(`${t}${e}`,e.length>0?Ce.NumericLiteral:Ce.UnaryOperator));continue}}}for(const[n,i]of Ue)if(r.slice(s,s+n.length)===n){o.push(new Le(n,i)),s+=n.length;continue e}if("'"!==t&&'"'!==t)if(Re(t)){const e=l(Re);o.push(new Le(e,Ce.NumericLiteral))}else{if(!je(t))throw new SyntaxError(`Unexpected character: ${t}`);{const e=l(je),t=Object.hasOwn(Me,e)?Me[e]:Ce.Identifier;t===Ce.In&&(null==(a=o.at(-1))?void 0:a.type)===Ce.Not?(o.pop(),o.push(new Le("not in",Ce.NotIn))):o.push(new Le(e,t))}}else{++s;const e=l((e=>e!==t));o.push(new Le(e,Ce.StringLiteral)),++s}}return o}(e,{lstrip_blocks:!0,trim_blocks:!0});this.parsed=st(n)}render(e){const t=new _t;if(t.set("false",!1),t.set("true",!0),t.set("raise_exception",(e=>{throw new Error(e)})),t.set("range",lt),e)for(const[n,i]of Object.entries(e))t.set(n,i);return new kt(t).run(this.parsed).value}};const Tt=["audio-classification","automatic-speech-recognition","depth-estimation","document-question-answering","feature-extraction","fill-mask","image-classification","image-feature-extraction","image-segmentation","image-to-image","image-to-text","image-text-to-text","mask-generation","object-detection","question-answering","summarization","table-question-answering","text2text-generation","text-classification","text-generation","text-to-audio","text-to-speech","token-classification","translation","video-classification","visual-question-answering","zero-shot-classification","zero-shot-image-classification","zero-shot-object-detection"],Et={"text-classification":{name:"Text Classification",subtasks:[{type:"acceptability-classification",name:"Acceptability Classification"},{type:"entity-linking-classification",name:"Entity Linking Classification"},{type:"fact-checking",name:"Fact Checking"},{type:"intent-classification",name:"Intent Classification"},{type:"language-identification",name:"Language Identification"},{type:"multi-class-classification",name:"Multi Class Classification"},{type:"multi-label-classification",name:"Multi Label Classification"},{type:"multi-input-text-classification",name:"Multi-input Text Classification"},{type:"natural-language-inference",name:"Natural Language Inference"},{type:"semantic-similarity-classification",name:"Semantic Similarity Classification"},{type:"sentiment-classification",name:"Sentiment Classification"},{type:"topic-classification",name:"Topic Classification"},{type:"semantic-similarity-scoring",name:"Semantic Similarity Scoring"},{type:"sentiment-scoring",name:"Sentiment Scoring"},{type:"sentiment-analysis",name:"Sentiment Analysis"},{type:"hate-speech-detection",name:"Hate Speech Detection"},{type:"text-scoring",name:"Text Scoring"}],modality:"nlp",color:"orange"},"token-classification":{name:"Token Classification",subtasks:[{type:"named-entity-recognition",name:"Named Entity Recognition"},{type:"part-of-speech",name:"Part of Speech"},{type:"parsing",name:"Parsing"},{type:"lemmatization",name:"Lemmatization"},{type:"word-sense-disambiguation",name:"Word Sense Disambiguation"},{type:"coreference-resolution",name:"Coreference-resolution"}],modality:"nlp",color:"blue"},"table-question-answering":{name:"Table Question Answering",modality:"nlp",color:"green"},"question-answering":{name:"Question Answering",subtasks:[{type:"extractive-qa",name:"Extractive QA"},{type:"open-domain-qa",name:"Open Domain QA"},{type:"closed-domain-qa",name:"Closed Domain QA"}],modality:"nlp",color:"blue"},"zero-shot-classification":{name:"Zero-Shot Classification",modality:"nlp",color:"yellow"},translation:{name:"Translation",modality:"nlp",color:"green"},summarization:{name:"Summarization",subtasks:[{type:"news-articles-summarization",name:"News Articles Summarization"},{type:"news-articles-headline-generation",name:"News Articles Headline Generation"}],modality:"nlp",color:"indigo"},"feature-extraction":{name:"Feature Extraction",modality:"nlp",color:"red"},"text-generation":{name:"Text Generation",subtasks:[{type:"dialogue-modeling",name:"Dialogue Modeling"},{type:"dialogue-generation",name:"Dialogue Generation"},{type:"conversational",name:"Conversational"},{type:"language-modeling",name:"Language Modeling"}],modality:"nlp",color:"indigo"},"text2text-generation":{name:"Text2Text Generation",subtasks:[{type:"text-simplification",name:"Text simplification"},{type:"explanation-generation",name:"Explanation Generation"},{type:"abstractive-qa",name:"Abstractive QA"},{type:"open-domain-abstractive-qa",name:"Open Domain Abstractive QA"},{type:"closed-domain-qa",name:"Closed Domain QA"},{type:"open-book-qa",name:"Open Book QA"},{type:"closed-book-qa",name:"Closed Book QA"}],modality:"nlp",color:"indigo"},"fill-mask":{name:"Fill-Mask",subtasks:[{type:"slot-filling",name:"Slot Filling"},{type:"masked-language-modeling",name:"Masked Language Modeling"}],modality:"nlp",color:"red"},"sentence-similarity":{name:"Sentence Similarity",modality:"nlp",color:"yellow"},"text-to-speech":{name:"Text-to-Speech",modality:"audio",color:"yellow"},"text-to-audio":{name:"Text-to-Audio",modality:"audio",color:"yellow"},"automatic-speech-recognition":{name:"Automatic Speech Recognition",modality:"audio",color:"yellow"},"audio-to-audio":{name:"Audio-to-Audio",modality:"audio",color:"blue"},"audio-classification":{name:"Audio Classification",subtasks:[{type:"keyword-spotting",name:"Keyword Spotting"},{type:"speaker-identification",name:"Speaker Identification"},{type:"audio-intent-classification",name:"Audio Intent Classification"},{type:"audio-emotion-recognition",name:"Audio Emotion Recognition"},{type:"audio-language-identification",name:"Audio Language Identification"}],modality:"audio",color:"green"},"audio-text-to-text":{name:"Audio-Text-to-Text",modality:"multimodal",color:"red",hideInDatasets:!0},"voice-activity-detection":{name:"Voice Activity Detection",modality:"audio",color:"red"},"depth-estimation":{name:"Depth Estimation",modality:"cv",color:"yellow"},"image-classification":{name:"Image Classification",subtasks:[{type:"multi-label-image-classification",name:"Multi Label Image Classification"},{type:"multi-class-image-classification",name:"Multi Class Image Classification"}],modality:"cv",color:"blue"},"object-detection":{name:"Object Detection",subtasks:[{type:"face-detection",name:"Face Detection"},{type:"vehicle-detection",name:"Vehicle Detection"}],modality:"cv",color:"yellow"},"image-segmentation":{name:"Image Segmentation",subtasks:[{type:"instance-segmentation",name:"Instance Segmentation"},{type:"semantic-segmentation",name:"Semantic Segmentation"},{type:"panoptic-segmentation",name:"Panoptic Segmentation"}],modality:"cv",color:"green"},"text-to-image":{name:"Text-to-Image",modality:"cv",color:"yellow"},"image-to-text":{name:"Image-to-Text",subtasks:[{type:"image-captioning",name:"Image Captioning"}],modality:"cv",color:"red"},"image-to-image":{name:"Image-to-Image",subtasks:[{type:"image-inpainting",name:"Image Inpainting"},{type:"image-colorization",name:"Image Colorization"},{type:"super-resolution",name:"Super Resolution"}],modality:"cv",color:"indigo"},"image-to-video":{name:"Image-to-Video",modality:"cv",color:"indigo"},"unconditional-image-generation":{name:"Unconditional Image Generation",modality:"cv",color:"green"},"video-classification":{name:"Video Classification",modality:"cv",color:"blue"},"reinforcement-learning":{name:"Reinforcement Learning",modality:"rl",color:"red"},robotics:{name:"Robotics",modality:"rl",subtasks:[{type:"grasping",name:"Grasping"},{type:"task-planning",name:"Task Planning"}],color:"blue"},"tabular-classification":{name:"Tabular Classification",modality:"tabular",subtasks:[{type:"tabular-multi-class-classification",name:"Tabular Multi Class Classification"},{type:"tabular-multi-label-classification",name:"Tabular Multi Label Classification"}],color:"blue"},"tabular-regression":{name:"Tabular Regression",modality:"tabular",subtasks:[{type:"tabular-single-column-regression",name:"Tabular Single Column Regression"}],color:"blue"},"tabular-to-text":{name:"Tabular to Text",modality:"tabular",subtasks:[{type:"rdf-to-text",name:"RDF to text"}],color:"blue",hideInModels:!0},"table-to-text":{name:"Table to Text",modality:"nlp",color:"blue",hideInModels:!0},"multiple-choice":{name:"Multiple Choice",subtasks:[{type:"multiple-choice-qa",name:"Multiple Choice QA"},{type:"multiple-choice-coreference-resolution",name:"Multiple Choice Coreference Resolution"}],modality:"nlp",color:"blue",hideInModels:!0},"text-ranking":{name:"Text Ranking",modality:"nlp",color:"red"},"text-retrieval":{name:"Text Retrieval",subtasks:[{type:"document-retrieval",name:"Document Retrieval"},{type:"utterance-retrieval",name:"Utterance Retrieval"},{type:"entity-linking-retrieval",name:"Entity Linking Retrieval"},{type:"fact-checking-retrieval",name:"Fact Checking Retrieval"}],modality:"nlp",color:"indigo",hideInModels:!0},"time-series-forecasting":{name:"Time Series Forecasting",modality:"tabular",subtasks:[{type:"univariate-time-series-forecasting",name:"Univariate Time Series Forecasting"},{type:"multivariate-time-series-forecasting",name:"Multivariate Time Series Forecasting"}],color:"blue"},"text-to-video":{name:"Text-to-Video",modality:"cv",color:"green"},"image-text-to-text":{name:"Image-Text-to-Text",modality:"multimodal",color:"red",hideInDatasets:!0},"visual-question-answering":{name:"Visual Question Answering",subtasks:[{type:"visual-question-answering",name:"Visual Question Answering"}],modality:"multimodal",color:"red"},"document-question-answering":{name:"Document Question Answering",subtasks:[{type:"document-question-answering",name:"Document Question Answering"}],modality:"multimodal",color:"blue",hideInDatasets:!0},"zero-shot-image-classification":{name:"Zero-Shot Image Classification",modality:"cv",color:"yellow"},"graph-ml":{name:"Graph Machine Learning",modality:"other",color:"green"},"mask-generation":{name:"Mask Generation",modality:"cv",color:"indigo"},"zero-shot-object-detection":{name:"Zero-Shot Object Detection",modality:"cv",color:"yellow"},"text-to-3d":{name:"Text-to-3D",modality:"cv",color:"yellow"},"image-to-3d":{name:"Image-to-3D",modality:"cv",color:"green"},"image-feature-extraction":{name:"Image Feature Extraction",modality:"cv",color:"indigo"},"video-text-to-text":{name:"Video-Text-to-Text",modality:"multimodal",color:"blue",hideInDatasets:!1},"keypoint-detection":{name:"Keypoint Detection",subtasks:[{type:"pose-estimation",name:"Pose Estimation"}],modality:"cv",color:"red",hideInDatasets:!0},"visual-document-retrieval":{name:"Visual Document Retrieval",modality:"multimodal",color:"yellow",hideInDatasets:!0},"any-to-any":{name:"Any-to-Any",modality:"multimodal",color:"yellow",hideInDatasets:!0},other:{name:"Other",modality:"other",color:"blue",hideInModels:!0,hideInDatasets:!0}},Ct=Object.keys(Et);Object.values(Et).flatMap((e=>"subtasks"in e?e.subtasks:[])).map((e=>e.type)),new Set(Ct);const Mt={datasets:[],demo:{inputs:[],outputs:[]},isPlaceholder:!0,metrics:[],models:[],spaces:[],summary:"",widgetModels:[],youtubeId:void 0,canonicalId:void 0},Lt={"audio-classification":["speechbrain","transformers","transformers.js"],"audio-to-audio":["asteroid","fairseq","speechbrain"],"automatic-speech-recognition":["espnet","nemo","speechbrain","transformers","transformers.js"],"audio-text-to-text":[],"depth-estimation":["transformers","transformers.js"],"document-question-answering":["transformers","transformers.js"],"feature-extraction":["sentence-transformers","transformers","transformers.js"],"fill-mask":["transformers","transformers.js"],"graph-ml":["transformers"],"image-classification":["keras","timm","transformers","transformers.js"],"image-feature-extraction":["timm","transformers"],"image-segmentation":["transformers","transformers.js"],"image-text-to-text":["transformers"],"image-to-image":["diffusers","transformers","transformers.js"],"image-to-text":["transformers","transformers.js"],"image-to-video":["diffusers"],"keypoint-detection":["transformers"],"video-classification":["transformers"],"mask-generation":["transformers"],"multiple-choice":["transformers"],"object-detection":["transformers","transformers.js","ultralytics"],other:[],"question-answering":["adapter-transformers","allennlp","transformers","transformers.js"],robotics:[],"reinforcement-learning":["transformers","stable-baselines3","ml-agents","sample-factory"],"sentence-similarity":["sentence-transformers","spacy","transformers.js"],summarization:["transformers","transformers.js"],"table-question-answering":["transformers"],"table-to-text":["transformers"],"tabular-classification":["sklearn"],"tabular-regression":["sklearn"],"tabular-to-text":["transformers"],"text-classification":["adapter-transformers","setfit","spacy","transformers","transformers.js"],"text-generation":["transformers","transformers.js"],"text-ranking":["sentence-transformers","transformers"],"text-retrieval":[],"text-to-image":["diffusers"],"text-to-speech":["espnet","tensorflowtts","transformers","transformers.js"],"text-to-audio":["transformers","transformers.js"],"text-to-video":["diffusers"],"text2text-generation":["transformers","transformers.js"],"time-series-forecasting":[],"token-classification":["adapter-transformers","flair","spacy","span-marker","stanza","transformers","transformers.js"],translation:["transformers","transformers.js"],"unconditional-image-generation":["diffusers"],"video-text-to-text":["transformers"],"visual-question-answering":["transformers","transformers.js"],"voice-activity-detection":[],"zero-shot-classification":["transformers","transformers.js"],"zero-shot-image-classification":["transformers","transformers.js"],"zero-shot-object-detection":["transformers","transformers.js"],"text-to-3d":["diffusers"],"image-to-3d":["diffusers"],"any-to-any":["transformers"],"visual-document-retrieval":["transformers"]};function jt(e,t=Mt){return{...t,id:e,label:Et[e].name,libraries:Lt[e]}}jt("any-to-any",Mt),jt("audio-classification",{datasets:[{description:"A benchmark of 10 different audio tasks.",id:"s3prl/superb"},{description:"A dataset of YouTube clips and their sound categories.",id:"agkphysics/AudioSet"}],demo:{inputs:[{filename:"audio.wav",type:"audio"}],outputs:[{data:[{label:"Up",score:.2},{label:"Down",score:.8}],type:"chart"}]},metrics:[{description:"",id:"accuracy"},{description:"",id:"recall"},{description:"",id:"precision"},{description:"",id:"f1"}],models:[{description:"An easy-to-use model for command recognition.",id:"speechbrain/google_speech_command_xvector"},{description:"An emotion recognition model.",id:"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"},{description:"A language identification model.",id:"facebook/mms-lid-126"}],spaces:[{description:"An application that can classify music into different genre.",id:"kurianbenoy/audioclassification"}],summary:"Audio classification is the task of assigning a label or class to a given audio. It can be used for recognizing which command a user is giving or the emotion of a statement, as well as identifying a speaker.",widgetModels:["MIT/ast-finetuned-audioset-10-10-0.4593"],youtubeId:"KWwzcmG98Ds"}),jt("audio-to-audio",{datasets:[{description:"512-element X-vector embeddings of speakers from CMU ARCTIC dataset.",id:"Matthijs/cmu-arctic-xvectors"}],demo:{inputs:[{filename:"input.wav",type:"audio"}],outputs:[{filename:"label-0.wav",type:"audio"},{filename:"label-1.wav",type:"audio"}]},metrics:[{description:"The Signal-to-Noise ratio is the relationship between the target signal level and the background noise level. It is calculated as the logarithm of the target signal divided by the background noise, in decibels.",id:"snri"},{description:"The Signal-to-Distortion ratio is the relationship between the target signal and the sum of noise, interference, and artifact errors",id:"sdri"}],models:[{description:"A speech enhancement model.",id:"ResembleAI/resemble-enhance"},{description:"A model that can change the voice in a speech recording.",id:"microsoft/speecht5_vc"}],spaces:[{description:"An application for speech separation.",id:"younver/speechbrain-speech-separation"},{description:"An application for audio style transfer.",id:"nakas/audio-diffusion_style_transfer"}],summary:"Audio-to-Audio is a family of tasks in which the input is an audio and the output is one or multiple generated audios. Some example tasks are speech enhancement and source separation.",widgetModels:["speechbrain/sepformer-wham"],youtubeId:"iohj7nCCYoM"}),jt("audio-text-to-text",Mt),jt("automatic-speech-recognition",{datasets:[{description:"31,175 hours of multilingual audio-text dataset in 108 languages.",id:"mozilla-foundation/common_voice_17_0"},{description:"Multilingual and diverse audio dataset with 101k hours of audio.",id:"amphion/Emilia-Dataset"},{description:"A dataset with 44.6k hours of English speaker data and 6k hours of other language speakers.",id:"parler-tts/mls_eng"},{description:"A multilingual audio dataset with 370K hours of audio.",id:"espnet/yodas"}],demo:{inputs:[{filename:"input.flac",type:"audio"}],outputs:[{label:"Transcript",content:"Going along slushy country roads and speaking to damp audiences in...",type:"text"}]},metrics:[{description:"",id:"wer"},{description:"",id:"cer"}],models:[{description:"A powerful ASR model by OpenAI.",id:"openai/whisper-large-v3"},{description:"A good generic speech model by MetaAI for fine-tuning.",id:"facebook/w2v-bert-2.0"},{description:"An end-to-end model that performs ASR and Speech Translation by MetaAI.",id:"facebook/seamless-m4t-v2-large"},{description:"A powerful multilingual ASR and Speech Translation model by Nvidia.",id:"nvidia/canary-1b"},{description:"Powerful speaker diarization model.",id:"pyannote/speaker-diarization-3.1"}],spaces:[{description:"A powerful general-purpose speech recognition application.",id:"hf-audio/whisper-large-v3"},{description:"Latest ASR model from Useful Sensors.",id:"mrfakename/Moonshinex"},{description:"A high quality speech and text translation model by Meta.",id:"facebook/seamless_m4t"},{description:"A powerful multilingual ASR and Speech Translation model by Nvidia",id:"nvidia/canary-1b"}],summary:"Automatic Speech Recognition (ASR), also known as Speech to Text (STT), is the task of transcribing a given audio to text. It has many applications, such as voice user interfaces.",widgetModels:["openai/whisper-large-v3"],youtubeId:"TksaY_FDgnk"}),jt("depth-estimation",{datasets:[{description:"NYU Depth V2 Dataset: Video dataset containing both RGB and depth sensor data.",id:"sayakpaul/nyu_depth_v2"},{description:"Monocular depth estimation benchmark based without noise and errors.",id:"depth-anything/DA-2K"}],demo:{inputs:[{filename:"depth-estimation-input.jpg",type:"img"}],outputs:[{filename:"depth-estimation-output.png",type:"img"}]},metrics:[],models:[{description:"Cutting-edge depth estimation model.",id:"depth-anything/Depth-Anything-V2-Large"},{description:"A strong monocular depth estimation model.",id:"jingheya/lotus-depth-g-v1-0"},{description:"A depth estimation model that predicts depth in videos.",id:"tencent/DepthCrafter"},{description:"A robust depth estimation model.",id:"apple/DepthPro-hf"}],spaces:[{description:"An application that predicts the depth of an image and then reconstruct the 3D model as voxels.",id:"radames/dpt-depth-estimation-3d-voxels"},{description:"An application for bleeding-edge depth estimation.",id:"akhaliq/depth-pro"},{description:"An application on cutting-edge depth estimation in videos.",id:"tencent/DepthCrafter"},{description:"A human-centric depth estimation application.",id:"facebook/sapiens-depth"}],summary:"Depth estimation is the task of predicting depth of the objects present in an image.",widgetModels:[""],youtubeId:""}),jt("document-question-answering",{datasets:[{description:"Largest document understanding dataset.",id:"HuggingFaceM4/Docmatix"},{description:"Dataset from the 2020 DocVQA challenge. The documents are taken from the UCSF Industry Documents Library.",id:"eliolio/docvqa"}],demo:{inputs:[{label:"Question",content:"What is the idea behind the consumer relations efficiency team?",type:"text"},{filename:"document-question-answering-input.png",type:"img"}],outputs:[{label:"Answer",content:"Balance cost efficiency with quality customer service",type:"text"}]},metrics:[{description:"The evaluation metric for the DocVQA challenge is the Average Normalized Levenshtein Similarity (ANLS). This metric is flexible to character regognition errors and compares the predicted answer with the ground truth answer.",id:"anls"},{description:"Exact Match is a metric based on the strict character match of the predicted answer and the right answer. For answers predicted correctly, the Exact Match will be 1. Even if only one character is different, Exact Match will be 0",id:"exact-match"}],models:[{description:"A robust document question answering model.",id:"impira/layoutlm-document-qa"},{description:"A document question answering model specialized in invoices.",id:"impira/layoutlm-invoices"},{description:"A special model for OCR-free document question answering.",id:"microsoft/udop-large"},{description:"A powerful model for document question answering.",id:"google/pix2struct-docvqa-large"}],spaces:[{description:"A robust document question answering application.",id:"impira/docquery"},{description:"An application that can answer questions from invoices.",id:"impira/invoices"},{description:"An application to compare different document question answering models.",id:"merve/compare_docvqa_models"}],summary:"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.",widgetModels:["impira/layoutlm-invoices"],youtubeId:""}),jt("visual-document-retrieval",Mt),jt("feature-extraction",{datasets:[{description:"Wikipedia dataset containing cleaned articles of all languages. Can be used to train `feature-extraction` models.",id:"wikipedia"}],demo:{inputs:[{label:"Input",content:"India, officially the Republic of India, is a country in South Asia.",type:"text"}],outputs:[{table:[["Dimension 1","Dimension 2","Dimension 3"],["2.583383083343506","2.757075071334839","0.9023529887199402"],["8.29393482208252","1.1071064472198486","2.03399395942688"],["-0.7754912972450256","-1.647324562072754","-0.6113331913948059"],["0.07087723910808563","1.5942802429199219","1.4610432386398315"]],type:"tabular"}]},metrics:[],models:[{description:"A powerful feature extraction model for natural language processing tasks.",id:"thenlper/gte-large"},{description:"A strong feature extraction model for retrieval.",id:"Alibaba-NLP/gte-Qwen1.5-7B-instruct"}],spaces:[{description:"A leaderboard to rank text feature extraction models based on a benchmark.",id:"mteb/leaderboard"},{description:"A leaderboard to rank best feature extraction models based on human feedback.",id:"mteb/arena"}],summary:"Feature extraction is the task of extracting features learnt in a model.",widgetModels:["facebook/bart-base"]}),jt("fill-mask",{datasets:[{description:"A common dataset that is used to train models for many languages.",id:"wikipedia"},{description:"A large English dataset with text crawled from the web.",id:"c4"}],demo:{inputs:[{label:"Input",content:"The <mask> barked at me",type:"text"}],outputs:[{type:"chart",data:[{label:"wolf",score:.487},{label:"dog",score:.061},{label:"cat",score:.058},{label:"fox",score:.047},{label:"squirrel",score:.025}]}]},metrics:[{description:"Cross Entropy is a metric that calculates the difference between two probability distributions. Each probability distribution is the distribution of predicted words",id:"cross_entropy"},{description:"Perplexity is the exponential of the cross-entropy loss. It evaluates the probabilities assigned to the next word by the model. Lower perplexity indicates better performance",id:"perplexity"}],models:[{description:"State-of-the-art masked language model.",id:"answerdotai/ModernBERT-large"},{description:"A multilingual model trained on 100 languages.",id:"FacebookAI/xlm-roberta-base"}],spaces:[],summary:"Masked language modeling is the task of masking some of the words in a sentence and predicting which words should replace those masks. These models are useful when we want to get a statistical understanding of the language in which the model is trained in.",widgetModels:["distilroberta-base"],youtubeId:"mqElG5QJWUg"}),jt("image-classification",{datasets:[{description:"Benchmark dataset used for image classification with images that belong to 100 classes.",id:"cifar100"},{description:"Dataset consisting of images of garments.",id:"fashion_mnist"}],demo:{inputs:[{filename:"image-classification-input.jpeg",type:"img"}],outputs:[{type:"chart",data:[{label:"Egyptian cat",score:.514},{label:"Tabby cat",score:.193},{label:"Tiger cat",score:.068}]}]},metrics:[{description:"",id:"accuracy"},{description:"",id:"recall"},{description:"",id:"precision"},{description:"",id:"f1"}],models:[{description:"A strong image classification model.",id:"google/vit-base-patch16-224"},{description:"A robust image classification model.",id:"facebook/deit-base-distilled-patch16-224"},{description:"A strong image classification model.",id:"facebook/convnext-large-224"}],spaces:[{description:"A leaderboard to evaluate different image classification models.",id:"timm/leaderboard"}],summary:"Image classification is the task of assigning a label or class to an entire image. Images are expected to have only one class for each image. Image classification models take an image as input and return a prediction about which class the image belongs to.",widgetModels:["google/vit-base-patch16-224"],youtubeId:"tjAIM7BOYhw"}),jt("image-feature-extraction",{datasets:[{description:"ImageNet-1K is a image classification dataset in which images are used to train image-feature-extraction models.",id:"imagenet-1k"}],demo:{inputs:[{filename:"mask-generation-input.png",type:"img"}],outputs:[{table:[["Dimension 1","Dimension 2","Dimension 3"],["0.21236686408519745","1.0919708013534546","0.8512550592422485"],["0.809657871723175","-0.18544459342956543","-0.7851548194885254"],["1.3103108406066895","-0.2479034662246704","-0.9107287526130676"],["1.8536205291748047","-0.36419737339019775","0.09717650711536407"]],type:"tabular"}]},metrics:[],models:[{description:"A powerful image feature extraction model.",id:"timm/vit_large_patch14_dinov2.lvd142m"},{description:"A strong image feature extraction model.",id:"nvidia/MambaVision-T-1K"},{description:"A robust image feature extraction model.",id:"facebook/dino-vitb16"},{description:"Cutting-edge image feature extraction model.",id:"apple/aimv2-large-patch14-336-distilled"},{description:"Strong image feature extraction model that can be used on images and documents.",id:"OpenGVLab/InternViT-6B-448px-V1-2"}],spaces:[{description:"A leaderboard to evaluate different image-feature-extraction models on classification performances",id:"timm/leaderboard"}],summary:"Image feature extraction is the task of extracting features learnt in a computer vision model.",widgetModels:[]}),jt("image-segmentation",{datasets:[{description:"Scene segmentation dataset.",id:"scene_parse_150"}],demo:{inputs:[{filename:"image-segmentation-input.jpeg",type:"img"}],outputs:[{filename:"image-segmentation-output.png",type:"img"}]},metrics:[{description:"Average Precision (AP) is the Area Under the PR Curve (AUC-PR). It is calculated for each semantic class separately",id:"Average Precision"},{description:"Mean Average Precision (mAP) is the overall average of the AP values",id:"Mean Average Precision"},{description:"Intersection over Union (IoU) is the overlap of segmentation masks. Mean IoU is the average of the IoU of all semantic classes",id:"Mean Intersection over Union"},{description:"APα is the Average Precision at the IoU threshold of a α value, for example, AP50 and AP75",id:"APα"}],models:[{description:"Solid semantic segmentation model trained on ADE20k.",id:"openmmlab/upernet-convnext-small"},{description:"Background removal model.",id:"briaai/RMBG-1.4"},{description:"A multipurpose image segmentation model for high resolution images.",id:"ZhengPeng7/BiRefNet"},{description:"Powerful human-centric image segmentation model.",id:"facebook/sapiens-seg-1b"},{description:"Panoptic segmentation model trained on the COCO (common objects) dataset.",id:"facebook/mask2former-swin-large-coco-panoptic"}],spaces:[{description:"A semantic segmentation application that can predict unseen instances out of the box.",id:"facebook/ov-seg"},{description:"One of the strongest segmentation applications.",id:"jbrinkma/segment-anything"},{description:"A human-centric segmentation model.",id:"facebook/sapiens-pose"},{description:"An instance segmentation application to predict neuronal cell types from microscopy images.",id:"rashmi/sartorius-cell-instance-segmentation"},{description:"An application that segments videos.",id:"ArtGAN/Segment-Anything-Video"},{description:"An panoptic segmentation application built for outdoor environments.",id:"segments/panoptic-segment-anything"}],summary:"Image Segmentation divides an image into segments where each pixel in the image is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation.",widgetModels:["nvidia/segformer-b0-finetuned-ade-512-512"],youtubeId:"dKE8SIt9C-w"}),jt("image-to-image",{datasets:[{description:"Synthetic dataset, for image relighting",id:"VIDIT"},{description:"Multiple images of celebrities, used for facial expression translation",id:"huggan/CelebA-faces"},{description:"12M image-caption pairs.",id:"Spawning/PD12M"}],demo:{inputs:[{filename:"image-to-image-input.jpeg",type:"img"}],outputs:[{filename:"image-to-image-output.png",type:"img"}]},isPlaceholder:!1,metrics:[{description:"Peak Signal to Noise Ratio (PSNR) is an approximation of the human perception, considering the ratio of the absolute intensity with respect to the variations. Measured in dB, a high value indicates a high fidelity.",id:"PSNR"},{description:"Structural Similarity Index (SSIM) is a perceptual metric which compares the luminance, contrast and structure of two images. The values of SSIM range between -1 and 1, and higher values indicate closer resemblance to the original image.",id:"SSIM"},{description:"Inception Score (IS) is an analysis of the labels predicted by an image classification model when presented with a sample of the generated images.",id:"IS"}],models:[{description:"An image-to-image model to improve image resolution.",id:"fal/AuraSR-v2"},{description:"A model that increases the resolution of an image.",id:"keras-io/super-resolution"},{description:"A model for applying edits to images through image controls.",id:"Yuanshi/OminiControl"},{description:"A model that generates images based on segments in the input image and the text prompt.",id:"mfidabel/controlnet-segment-anything"},{description:"Strong model for inpainting and outpainting.",id:"black-forest-labs/FLUX.1-Fill-dev"},{description:"Strong model for image editing using depth maps.",id:"black-forest-labs/FLUX.1-Depth-dev-lora"}],spaces:[{description:"Image enhancer application for low light.",id:"keras-io/low-light-image-enhancement"},{description:"Style transfer application.",id:"keras-io/neural-style-transfer"},{description:"An application that generates images based on segment control.",id:"mfidabel/controlnet-segment-anything"},{description:"Image generation application that takes image control and text prompt.",id:"hysts/ControlNet"},{description:"Colorize any image using this app.",id:"ioclab/brightness-controlnet"},{description:"Edit images with instructions.",id:"timbrooks/instruct-pix2pix"}],summary:"Image-to-image is the task of transforming an input image through a variety of possible manipulations and enhancements, such as super-resolution, image inpainting, colorization, and more.",widgetModels:["stabilityai/stable-diffusion-2-inpainting"],youtubeId:""}),jt("image-text-to-text",{datasets:[{description:"Instructions composed of image and text.",id:"liuhaotian/LLaVA-Instruct-150K"},{description:"Collection of image-text pairs on scientific topics.",id:"DAMO-NLP-SG/multimodal_textbook"},{description:"A collection of datasets made for model fine-tuning.",id:"HuggingFaceM4/the_cauldron"},{description:"Screenshots of websites with their HTML/CSS codes.",id:"HuggingFaceM4/WebSight"}],demo:{inputs:[{filename:"image-text-to-text-input.png",type:"img"},{label:"Text Prompt",content:"Describe the position of the bee in detail.",type:"text"}],outputs:[{label:"Answer",content:"The bee is sitting on a pink flower, surrounded by other flowers. The bee is positioned in the center of the flower, with its head and front legs sticking out.",type:"text"}]},metrics:[],models:[{description:"Small and efficient yet powerful vision language model.",id:"HuggingFaceTB/SmolVLM-Instruct"},{description:"A screenshot understanding model used to control computers.",id:"microsoft/OmniParser-v2.0"},{description:"Cutting-edge vision language model.",id:"allenai/Molmo-7B-D-0924"},{description:"Small yet powerful model.",id:"vikhyatk/moondream2"},{description:"Strong image-text-to-text model.",id:"Qwen/Qwen2.5-VL-7B-Instruct"},{description:"Image-text-to-text model with agentic capabilities.",id:"microsoft/Magma-8B"},{description:"Strong image-text-to-text model focused on documents.",id:"allenai/olmOCR-7B-0225-preview"},{description:"Small yet strong image-text-to-text model.",id:"ibm-granite/granite-vision-3.2-2b"}],spaces:[{description:"Leaderboard to evaluate vision language models.",id:"opencompass/open_vlm_leaderboard"},{description:"Vision language models arena, where models are ranked by votes of users.",id:"WildVision/vision-arena"},{description:"Powerful vision-language model assistant.",id:"akhaliq/Molmo-7B-D-0924"},{description:"Powerful vision language assistant that can understand multiple images.",id:"HuggingFaceTB/SmolVLM2"},{description:"An application for chatting with an image-text-to-text model.",id:"GanymedeNil/Qwen2-VL-7B"},{description:"An application that parses screenshots into actions.",id:"showlab/ShowUI"},{description:"An application that detects gaze.",id:"moondream/gaze-demo"}],summary:"Image-text-to-text models take in an image and text prompt and output text. These models are also called vision-language models, or VLMs. The difference from image-to-text models is that these models take an additional text input, not restricting the model to certain use cases like image captioning, and may also be trained to accept a conversation as input.",widgetModels:["Qwen/Qwen2-VL-7B-Instruct"],youtubeId:"IoGaGfU1CIg"}),jt("image-to-text",{datasets:[{description:"Dataset from 12M image-text of Reddit",id:"red_caps"},{description:"Dataset from 3.3M images of Google",id:"datasets/conceptual_captions"}],demo:{inputs:[{filename:"savanna.jpg",type:"img"}],outputs:[{label:"Detailed description",content:"a herd of giraffes and zebras grazing in a field",type:"text"}]},metrics:[],models:[{description:"A robust image captioning model.",id:"Salesforce/blip2-opt-2.7b"},{description:"A powerful and accurate image-to-text model that can also localize concepts in images.",id:"microsoft/kosmos-2-patch14-224"},{description:"A strong optical character recognition model.",id:"facebook/nougat-base"},{description:"A powerful model that lets you have a conversation with the image.",id:"llava-hf/llava-1.5-7b-hf"}],spaces:[{description:"An application that compares various image captioning models.",id:"nielsr/comparing-captioning-models"},{description:"A robust image captioning application.",id:"flax-community/image-captioning"},{description:"An application that transcribes handwritings into text.",id:"nielsr/TrOCR-handwritten"},{description:"An application that can caption images and answer questions about a given image.",id:"Salesforce/BLIP"},{description:"An application that can caption images and answer questions with a conversational agent.",id:"Salesforce/BLIP2"},{description:"An image captioning application that demonstrates the effect of noise on captions.",id:"johko/capdec-image-captioning"}],summary:"Image to text models output a text from a given image. Image captioning or optical character recognition can be considered as the most common applications of image to text.",widgetModels:["Salesforce/blip-image-captioning-large"],youtubeId:""}),jt("keypoint-detection",{datasets:[{description:"A dataset of hand keypoints of over 500k examples.",id:"Vincent-luo/hagrid-mediapipe-hands"}],demo:{inputs:[{filename:"keypoint-detection-input.png",type:"img"}],outputs:[{filename:"keypoint-detection-output.png",type:"img"}]},metrics:[],models:[{description:"A robust keypoint detection model.",id:"magic-leap-community/superpoint"},{description:"A robust keypoint matching model.",id:"magic-leap-community/superglue_outdoor"},{description:"Strong keypoint detection model used to detect human pose.",id:"facebook/sapiens-pose-1b"},{description:"Powerful keypoint detection model used to detect human pose.",id:"usyd-community/vitpose-plus-base"}],spaces:[{description:"An application that detects hand keypoints in real-time.",id:"datasciencedojo/Hand-Keypoint-Detection-Realtime"},{description:"An application to try a universal keypoint detection model.",id:"merve/SuperPoint"}],summary:"Keypoint detection is the task of identifying meaningful distinctive points or features in an image.",widgetModels:[],youtubeId:""}),jt("mask-generation",{datasets:[{description:"Widely used benchmark dataset for multiple Vision tasks.",id:"merve/coco2017"},{description:"Medical Imaging dataset of the Human Brain for segmentation and mask generating tasks",id:"rocky93/BraTS_segmentation"}],demo:{inputs:[{filename:"mask-generation-input.png",type:"img"}],outputs:[{filename:"mask-generation-output.png",type:"img"}]},metrics:[{description:"IoU is used to measure the overlap between predicted mask and the ground truth mask.",id:"Intersection over Union (IoU)"}],models:[{description:"Small yet powerful mask generation model.",id:"Zigeng/SlimSAM-uniform-50"},{description:"Very strong mask generation model.",id:"facebook/sam2-hiera-large"}],spaces:[{description:"An application that combines a mask generation model with a zero-shot object detection model for text-guided image segmentation.",id:"merve/OWLSAM2"},{description:"An application that compares the performance of a large and a small mask generation model.",id:"merve/slimsam"},{description:"An application based on an improved mask generation model.",id:"SkalskiP/segment-anything-model-2"},{description:"An application to remove objects from videos using mask generation models.",id:"SkalskiP/SAM_and_ProPainter"}],summary:"Mask generation is the task of generating masks that identify a specific object or region of interest in a given image. Masks are often used in segmentation tasks, where they provide a precise way to isolate the object of interest for further processing or analysis.",widgetModels:[],youtubeId:""}),jt("object-detection",{datasets:[{description:"Widely used benchmark dataset for multiple vision tasks.",id:"merve/coco2017"},{description:"Multi-task computer vision benchmark.",id:"merve/pascal-voc"}],demo:{inputs:[{filename:"object-detection-input.jpg",type:"img"}],outputs:[{filename:"object-detection-output.jpg",type:"img"}]},metrics:[{description:"The Average Precision (AP) metric is the Area Under the PR Curve (AUC-PR). It is calculated for each class separately",id:"Average Precision"},{description:"The Mean Average Precision (mAP) metric is the overall average of the AP values",id:"Mean Average Precision"},{description:"The APα metric is the Average Precision at the IoU threshold of a α value, for example, AP50 and AP75",id:"APα"}],models:[{description:"Solid object detection model pre-trained on the COCO 2017 dataset.",id:"facebook/detr-resnet-50"},{description:"Accurate object detection model.",id:"IDEA-Research/dab-detr-resnet-50"},{description:"Fast and accurate object detection model.",id:"PekingU/rtdetr_v2_r50vd"},{description:"Object detection model for low-lying objects.",id:"StephanST/WALDO30"}],spaces:[{description:"Leaderboard to compare various object detection models across several metrics.",id:"hf-vision/object_detection_leaderboard"},{description:"An application that contains various object detection models to try from.",id:"Gradio-Blocks/Object-Detection-With-DETR-and-YOLOS"},{description:"A cutting-edge object detection application.",id:"sunsmarterjieleaf/yolov12"},{description:"An object tracking, segmentation and inpainting application.",id:"VIPLab/Track-Anything"},{description:"Very fast object tracking application based on object detection.",id:"merve/RT-DETR-tracking-coco"}],summary:"Object Detection models allow users to identify objects of certain defined classes. Object detection models receive an image as input and output the images with bounding boxes and labels on detected objects.",widgetModels:["facebook/detr-resnet-50"],youtubeId:"WdAeKSOpxhw"}),jt("video-classification",{datasets:[{description:"Benchmark dataset used for video classification with videos that belong to 400 classes.",id:"kinetics400"}],demo:{inputs:[{filename:"video-classification-input.gif",type:"img"}],outputs:[{type:"chart",data:[{label:"Playing Guitar",score:.514},{label:"Playing Tennis",score:.193},{label:"Cooking",score:.068}]}]},metrics:[{description:"",id:"accuracy"},{description:"",id:"recall"},{description:"",id:"precision"},{description:"",id:"f1"}],models:[{description:"Strong Video Classification model trained on the Kinetics 400 dataset.",id:"google/vivit-b-16x2-kinetics400"},{description:"Strong Video Classification model trained on the Kinetics 400 dataset.",id:"microsoft/xclip-base-patch32"}],spaces:[{description:"An application that classifies video at different timestamps.",id:"nateraw/lavila"},{description:"An application that classifies video.",id:"fcakyon/video-classification"}],summary:"Video classification is the task of assigning a label or class to an entire video. Videos are expected to have only one class for each video. Video classification models take a video as input and return a prediction about which class the video belongs to.",widgetModels:[],youtubeId:""}),jt("question-answering",{datasets:[{description:"A famous question answering dataset based on English articles from Wikipedia.",id:"squad_v2"},{description:"A dataset of aggregated anonymized actual queries issued to the Google search engine.",id:"natural_questions"}],demo:{inputs:[{label:"Question",content:"Which name is also used to describe the Amazon rainforest in English?",type:"text"},{label:"Context",content:"The Amazon rainforest, also known in English as Amazonia or the Amazon Jungle",type:"text"}],outputs:[{label:"Answer",content:"Amazonia",type:"text"}]},metrics:[{description:"Exact Match is a metric based on the strict character match of the predicted answer and the right answer. For answers predicted correctly, the Exact Match will be 1. Even if only one character is different, Exact Match will be 0",id:"exact-match"},{description:" The F1-Score metric is useful if we value both false positives and false negatives equally. The F1-Score is calculated on each word in the predicted sequence against the correct answer",id:"f1"}],models:[{description:"A robust baseline model for most question answering domains.",id:"deepset/roberta-base-squad2"},{description:"Small yet robust model that can answer questions.",id:"distilbert/distilbert-base-cased-distilled-squad"},{description:"A special model that can answer questions from tables.",id:"google/tapas-base-finetuned-wtq"}],spaces:[{description:"An application that can answer a long question from Wikipedia.",id:"deepset/wikipedia-assistant"}],summary:"Question Answering models can retrieve the answer to a question from a given text, which is useful for searching for an answer in a document. Some question answering models can generate answers without context!",widgetModels:["deepset/roberta-base-squad2"],youtubeId:"ajPx5LwJD-I"}),jt("reinforcement-learning",{datasets:[{description:"A curation of widely used datasets for Data Driven Deep Reinforcement Learning (D4RL)",id:"edbeeching/decision_transformer_gym_replay"}],demo:{inputs:[{label:"State",content:"Red traffic light, pedestrians are about to pass.",type:"text"}],outputs:[{label:"Action",content:"Stop the car.",type:"text"},{label:"Next State",content:"Yellow light, pedestrians have crossed.",type:"text"}]},metrics:[{description:"Accumulated reward across all time steps discounted by a factor that ranges between 0 and 1 and determines how much the agent optimizes for future relative to immediate rewards. Measures how good is the policy ultimately found by a given algorithm considering uncertainty over the future.",id:"Discounted Total Reward"},{description:"Average return obtained after running the policy for a certain number of evaluation episodes. As opposed to total reward, mean reward considers how much reward a given algorithm receives while learning.",id:"Mean Reward"},{description:"Measures how good a given algorithm is after a predefined time. Some algorithms may be guaranteed to converge to optimal behavior across many time steps. However, an agent that reaches an acceptable level of optimality after a given time horizon may be preferable to one that ultimately reaches optimality but takes a long time.",id:"Level of Performance After Some Time"}],models:[{description:"A Reinforcement Learning model trained on expert data from the Gym Hopper environment",id:"edbeeching/decision-transformer-gym-hopper-expert"},{description:"A PPO agent playing seals/CartPole-v0 using the stable-baselines3 library and the RL Zoo.",id:"HumanCompatibleAI/ppo-seals-CartPole-v0"}],spaces:[{description:"An application for a cute puppy agent learning to catch a stick.",id:"ThomasSimonini/Huggy"},{description:"An application to play Snowball Fight with a reinforcement learning agent.",id:"ThomasSimonini/SnowballFight"}],summary:"Reinforcement learning is the computational approach of learning from action by interacting with an environment through trial and error and receiving rewards (negative or positive) as feedback",widgetModels:[],youtubeId:"q0BiUn5LiBc"}),jt("sentence-similarity",{datasets:[{description:"Bing queries with relevant passages from various web sources.",id:"microsoft/ms_marco"}],demo:{inputs:[{label:"Source sentence",content:"Machine learning is so easy.",type:"text"},{label:"Sentences to compare to",content:"Deep learning is so straightforward.",type:"text"},{label:"",content:"This is so difficult, like rocket science.",type:"text"},{label:"",content:"I can't believe how much I struggled with this.",type:"text"}],outputs:[{type:"chart",data:[{label:"Deep learning is so straightforward.",score:.623},{label:"This is so difficult, like rocket science.",score:.413},{label:"I can't believe how much I struggled with this.",score:.256}]}]},metrics:[{description:"Reciprocal Rank is a measure used to rank the relevancy of documents given a set of documents. Reciprocal Rank is the reciprocal of the rank of the document retrieved, meaning, if the rank is 3, the Reciprocal Rank is 0.33. If the rank is 1, the Reciprocal Rank is 1",id:"Mean Reciprocal Rank"},{description:"The similarity of the embeddings is evaluated mainly on cosine similarity. It is calculated as the cosine of the angle between two vectors. It is particularly useful when your texts are not the same length",id:"Cosine Similarity"}],models:[{description:"This model works well for sentences and paragraphs and can be used for clustering/grouping and semantic searches.",id:"sentence-transformers/all-mpnet-base-v2"},{description:"A multilingual robust sentence similarity model.",id:"BAAI/bge-m3"},{description:"A robust sentence similarity model.",id:"HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5"}],spaces:[{description:"An application that leverages sentence similarity to answer questions from YouTube videos.",id:"Gradio-Blocks/Ask_Questions_To_YouTube_Videos"},{description:"An application that retrieves relevant PubMed abstracts for a given online article which can be used as further references.",id:"Gradio-Blocks/pubmed-abstract-retriever"},{description:"An application that leverages sentence similarity to summarize text.",id:"nickmuchi/article-text-summarizer"},{description:"A guide that explains how Sentence Transformers can be used for semantic search.",id:"sentence-transformers/Sentence_Transformers_for_semantic_search"}],summary:"Sentence Similarity is the task of determining how similar two texts are. Sentence similarity models convert input texts into vectors (embeddings) that capture semantic information and calculate how close (similar) they are between them. This task is particularly useful for information retrieval and clustering/grouping.",widgetModels:["BAAI/bge-small-en-v1.5"],youtubeId:"VCZq5AkbNEU"}),jt("summarization",{canonicalId:"text2text-generation",datasets:[{description:"News articles in five different languages along with their summaries. Widely used for benchmarking multilingual summarization models.",id:"mlsum"},{description:"English conversations and their summaries. Useful for benchmarking conversational agents.",id:"samsum"}],demo:{inputs:[{label:"Input",content:"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres. Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.",type:"text"}],outputs:[{label:"Output",content:"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. It was the first structure to reach a height of 300 metres.",type:"text"}]},metrics:[{description:"The generated sequence is compared against its summary, and the overlap of tokens are counted. ROUGE-N refers to overlap of N subsequent tokens, ROUGE-1 refers to overlap of single tokens and ROUGE-2 is the overlap of two subsequent tokens.",id:"rouge"}],models:[{description:"A strong summarization model trained on English news articles. Excels at generating factual summaries.",id:"facebook/bart-large-cnn"},{description:"A summarization model trained on medical articles.",id:"Falconsai/medical_summarization"}],spaces:[{description:"An application that can summarize long paragraphs.",id:"pszemraj/summarize-long-text"},{description:"A much needed summarization application for terms and conditions.",id:"ml6team/distilbart-tos-summarizer-tosdr"},{description:"An application that summarizes long documents.",id:"pszemraj/document-summarization"},{description:"An application that can detect errors in abstractive summarization.",id:"ml6team/post-processing-summarization"}],summary:"Summarization is the task of producing a shorter version of a document while preserving its important information. Some models can extract text from the original input, while other models can generate entirely new text.",widgetModels:["facebook/bart-large-cnn"],youtubeId:"yHnr5Dk2zCI"}),jt("table-question-answering",{datasets:[{description:"The WikiTableQuestions dataset is a large-scale dataset for the task of question answering on semi-structured tables.",id:"wikitablequestions"},{description:"WikiSQL is a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia.",id:"wikisql"}],demo:{inputs:[{table:[["Rank","Name","No.of reigns","Combined days"],["1","lou Thesz","3","3749"],["2","Ric Flair","8","3103"],["3","Harley Race","7","1799"]],type:"tabular"},{label:"Question",content:"What is the number of reigns for Harley Race?",type:"text"}],outputs:[{label:"Result",content:"7",type:"text"}]},metrics:[{description:"Checks whether the predicted answer(s) is the same as the ground-truth answer(s).",id:"Denotation Accuracy"}],models:[{description:"A table question answering model that is capable of neural SQL execution, i.e., employ TAPEX to execute a SQL query on a given table.",id:"microsoft/tapex-base"},{description:"A robust table question answering model.",id:"google/tapas-base-finetuned-wtq"}],spaces:[{description:"An application that answers questions based on table CSV files.",id:"katanaml/table-query"}],summary:"Table Question Answering (Table QA) is the answering a question about an information on a given table.",widgetModels:["google/tapas-base-finetuned-wtq"]}),jt("tabular-classification",{datasets:[{description:"A comprehensive curation of datasets covering all benchmarks.",id:"inria-soda/tabular-benchmark"}],demo:{inputs:[{table:[["Glucose","Blood Pressure ","Skin Thickness","Insulin","BMI"],["148","72","35","0","33.6"],["150","50","30","0","35.1"],["141","60","29","1","39.2"]],type:"tabular"}],outputs:[{table:[["Diabetes"],["1"],["1"],["0"]],type:"tabular"}]},metrics:[{description:"",id:"accuracy"},{description:"",id:"recall"},{description:"",id:"precision"},{description:"",id:"f1"}],models:[{description:"Breast cancer prediction model based on decision trees.",id:"scikit-learn/cancer-prediction-trees"}],spaces:[{description:"An application that can predict defective products on a production line.",id:"scikit-learn/tabular-playground"},{description:"An application that compares various tabular classification techniques on different datasets.",id:"scikit-learn/classification"}],summary:"Tabular classification is the task of classifying a target category (a group) based on set of attributes.",widgetModels:["scikit-learn/tabular-playground"],youtubeId:""}),jt("tabular-regression",{datasets:[{description:"A comprehensive curation of datasets covering all benchmarks.",id:"inria-soda/tabular-benchmark"}],demo:{inputs:[{table:[["Car Name","Horsepower","Weight"],["ford torino","140","3,449"],["amc hornet","97","2,774"],["toyota corolla","65","1,773"]],type:"tabular"}],outputs:[{table:[["MPG (miles per gallon)"],["17"],["18"],["31"]],type:"tabular"}]},metrics:[{description:"",id:"mse"},{description:"Coefficient of determination (or R-squared) is a measure of how well the model fits the data. Higher R-squared is considered a better fit.",id:"r-squared"}],models:[{description:"Fish weight prediction based on length measurements and species.",id:"scikit-learn/Fish-Weight"}],spaces:[{description:"An application that can predict weight of a fish based on set of attributes.",id:"scikit-learn/fish-weight-prediction"}],summary:"Tabular regression is the task of predicting a numerical value given a set of attributes.",widgetModels:["scikit-learn/Fish-Weight"],youtubeId:""}),jt("text-classification",{datasets:[{description:"A widely used dataset used to benchmark multiple variants of text classification.",id:"nyu-mll/glue"},{description:"A text classification dataset used to benchmark natural language inference models",id:"stanfordnlp/snli"}],demo:{inputs:[{label:"Input",content:"I love Hugging Face!",type:"text"}],outputs:[{type:"chart",data:[{label:"POSITIVE",score:.9},{label:"NEUTRAL",score:.1},{label:"NEGATIVE",score:0}]}]},metrics:[{description:"",id:"accuracy"},{description:"",id:"recall"},{description:"",id:"precision"},{description:"The F1 metric is the harmonic mean of the precision and recall. It can be calculated as: F1 = 2 * (precision * recall) / (precision + recall)",id:"f1"}],models:[{description:"A robust model trained for sentiment analysis.",id:"distilbert/distilbert-base-uncased-finetuned-sst-2-english"},{description:"A sentiment analysis model specialized in financial sentiment.",id:"ProsusAI/finbert"},{description:"A sentiment analysis model specialized in analyzing tweets.",id:"cardiffnlp/twitter-roberta-base-sentiment-latest"},{description:"A model that can classify languages.",id:"papluca/xlm-roberta-base-language-detection"},{description:"A model that can classify text generation attacks.",id:"meta-llama/Prompt-Guard-86M"}],spaces:[{description:"An application that can classify financial sentiment.",id:"IoannisTr/Tech_Stocks_Trading_Assistant"},{description:"A dashboard that contains various text classification tasks.",id:"miesnerjacob/Multi-task-NLP"},{description:"An application that analyzes user reviews in healthcare.",id:"spacy/healthsea-demo"}],summary:"Text Classification is the task of assigning a label or class to a given text. Some use cases are sentiment analysis, natural language inference, and assessing grammatical correctness.",widgetModels:["distilbert/distilbert-base-uncased-finetuned-sst-2-english"],youtubeId:"leNG9fN9FQU"}),jt("text-generation",{datasets:[{description:"Multilingual dataset used to evaluate text generation models.",id:"CohereForAI/Global-MMLU"},{description:"High quality multilingual data used to train text-generation models.",id:"HuggingFaceFW/fineweb-2"},{description:"Truly open-source, curated and cleaned dialogue dataset.",id:"HuggingFaceH4/ultrachat_200k"},{description:"A reasoning dataset.",id:"open-r1/OpenThoughts-114k-math"},{description:"A multilingual instruction dataset with preference ratings on responses.",id:"allenai/tulu-3-sft-mixture"},{description:"A large synthetic dataset for alignment of text generation models.",id:"HuggingFaceTB/smoltalk"},{description:"A dataset made for training text generation models solving math questions.",id:"HuggingFaceTB/finemath"}],demo:{inputs:[{label:"Input",content:"Once upon a time,",type:"text"}],outputs:[{label:"Output",content:"Once upon a time, we knew that our ancestors were on the verge of extinction. The great explorers and poets of the Old World, from Alexander the Great to Chaucer, are dead and gone. A good many of our ancient explorers and poets have",type:"text"}]},metrics:[{description:"Cross Entropy is a metric that calculates the difference between two probability distributions. Each probability distribution is the distribution of predicted words",id:"Cross Entropy"},{description:"The Perplexity metric is the exponential of the cross-entropy loss. It evaluates the probabilities assigned to the next word by the model. Lower perplexity indicates better performance",id:"Perplexity"}],models:[{description:"A text-generation model trained to follow instructions.",id:"google/gemma-2-2b-it"},{description:"Smaller variant of one of the most powerful models.",id:"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"},{description:"Very powerful text generation model trained to follow instructions.",id:"meta-llama/Meta-Llama-3.1-8B-Instruct"},{description:"Powerful text generation model by Microsoft.",id:"microsoft/phi-4"},{description:"A very powerful model with reasoning capabilities.",id:"simplescaling/s1.1-32B"},{description:"Strong conversational model that supports very long instructions.",id:"Qwen/Qwen2.5-7B-Instruct-1M"},{description:"Text generation model used to write code.",id:"Qwen/Qwen2.5-Coder-32B-Instruct"},{description:"Powerful reasoning based open large language model.",id:"deepseek-ai/DeepSeek-R1"}],spaces:[{description:"A leaderboard to compare different open-source text generation models based on various benchmarks.",id:"open-llm-leaderboard/open_llm_leaderboard"},{description:"A leaderboard for comparing chain-of-thought performance of models.",id:"logikon/open_cot_leaderboard"},{description:"An text generation based application based on a very powerful LLaMA2 model.",id:"ysharma/Explore_llamav2_with_TGI"},{description:"An text generation based application to converse with Zephyr model.",id:"HuggingFaceH4/zephyr-chat"},{description:"A leaderboard that ranks text generation models based on blind votes from people.",id:"lmsys/chatbot-arena-leaderboard"},{description:"An chatbot to converse with a very powerful text generation model.",id:"mlabonne/phixtral-chat"}],summary:"Generating text is the task of generating new text given another text. These models can, for example, fill in incomplete text or paraphrase.",widgetModels:["mistralai/Mistral-Nemo-Instruct-2407"],youtubeId:"e9gNEAlsOvU"}),jt("text-ranking",{datasets:[{description:"Bing queries with relevant passages from various web sources.",id:"microsoft/ms_marco"}],demo:{inputs:[{label:"Source sentence",content:"Machine learning is so easy.",type:"text"},{label:"Sentences to compare to",content:"Deep learning is so straightforward.",type:"text"},{label:"",content:"This is so difficult, like rocket science.",type:"text"},{label:"",content:"I can't believe how much I struggled with this.",type:"text"}],outputs:[{type:"chart",data:[{label:"Deep learning is so straightforward.",score:2.2006407},{label:"This is so difficult, like rocket science.",score:-6.2634873},{label:"I can't believe how much I struggled with this.",score:-10.251488}]}]},metrics:[{description:"Discounted Cumulative Gain (DCG) measures the gain, or usefulness, of search results discounted by their position. The normalization is done by dividing the DCG by the ideal DCG, which is the DCG of the perfect ranking.",id:"Normalized Discounted Cumulative Gain"},{description:"Reciprocal Rank is a measure used to rank the relevancy of documents given a set of documents. Reciprocal Rank is the reciprocal of the rank of the document retrieved, meaning, if the rank is 3, the Reciprocal Rank is 0.33. If the rank is 1, the Reciprocal Rank is 1",id:"Mean Reciprocal Rank"},{description:"Mean Average Precision (mAP) is the overall average of the Average Precision (AP) values, where AP is the Area Under the PR Curve (AUC-PR)",id:"Mean Average Precision"}],models:[{description:"An extremely efficient text ranking model trained on a web search dataset.",id:"cross-encoder/ms-marco-MiniLM-L6-v2"},{description:"A strong multilingual text reranker model.",id:"Alibaba-NLP/gte-multilingual-reranker-base"},{description:"An efficient text ranking model that punches above its weight.",id:"Alibaba-NLP/gte-reranker-modernbert-base"}],spaces:[],summary:"Text Ranking is the task of ranking a set of texts based on their relevance to a query. Text ranking models are trained on large datasets of queries and relevant documents to learn how to rank documents based on their relevance to the query. This task is particularly useful for search engines and information retrieval systems.",widgetModels:["cross-encoder/ms-marco-MiniLM-L6-v2"],youtubeId:""}),jt("text-to-image",{datasets:[{description:"RedCaps is a large-scale dataset of 12M image-text pairs collected from Reddit.",id:"red_caps"},{description:"Conceptual Captions is a dataset consisting of ~3.3M images annotated with captions.",id:"conceptual_captions"},{description:"12M image-caption pairs.",id:"Spawning/PD12M"}],demo:{inputs:[{label:"Input",content:"A city above clouds, pastel colors, Victorian style",type:"text"}],outputs:[{filename:"image.jpeg",type:"img"}]},metrics:[{description:"The Inception Score (IS) measure assesses diversity and meaningfulness. It uses a generated image sample to predict its label. A higher score signifies more diverse and meaningful images.",id:"IS"},{description:"The Fréchet Inception Distance (FID) calculates the distance between distributions between synthetic and real samples. A lower FID score indicates better similarity between the distributions of real and generated images.",id:"FID"},{description:"R-precision assesses how the generated image aligns with the provided text description. It uses the generated images as queries to retrieve relevant text descriptions. The top 'r' relevant descriptions are selected and used to calculate R-precision as r/R, where 'R' is the number of ground truth descriptions associated with the generated images. A higher R-precision value indicates a better model.",id:"R-Precision"}],models:[{description:"One of the most powerful image generation models that can generate realistic outputs.",id:"black-forest-labs/FLUX.1-dev"},{description:"A powerful yet fast image generation model.",id:"latent-consistency/lcm-lora-sdxl"},{description:"Text-to-image model for photorealistic generation.",id:"Kwai-Kolors/Kolors"},{description:"A powerful text-to-image model.",id:"stabilityai/stable-diffusion-3-medium-diffusers"}],spaces:[{description:"A powerful text-to-image application.",id:"stabilityai/stable-diffusion-3-medium"},{description:"A text-to-image application to generate comics.",id:"jbilcke-hf/ai-comic-factory"},{description:"An application to match multiple custom image generation models.",id:"multimodalart/flux-lora-lab"},{description:"A powerful yet very fast image generation application.",id:"latent-consistency/lcm-lora-for-sdxl"},{description:"A gallery to explore various text-to-image models.",id:"multimodalart/LoraTheExplorer"},{description:"An application for `text-to-image`, `image-to-image` and image inpainting.",id:"ArtGAN/Stable-Diffusion-ControlNet-WebUI"},{description:"An application to generate realistic images given photos of a person and a prompt.",id:"InstantX/InstantID"}],summary:"Text-to-image is the task of generating images from input text. These pipelines can also be used to modify and edit images based on text prompts.",widgetModels:["black-forest-labs/FLUX.1-dev"],youtubeId:""}),jt("text-to-speech",{canonicalId:"text-to-audio",datasets:[{description:"10K hours of multi-speaker English dataset.",id:"parler-tts/mls_eng_10k"},{description:"Multi-speaker English dataset.",id:"mythicinfinity/libritts_r"},{description:"Multi-lingual dataset.",id:"facebook/multilingual_librispeech"}],demo:{inputs:[{label:"Input",content:"I love audio models on the Hub!",type:"text"}],outputs:[{filename:"audio.wav",type:"audio"}]},metrics:[{description:"The Mel Cepstral Distortion (MCD) metric is used to calculate the quality of generated speech.",id:"mel cepstral distortion"}],models:[{description:"A prompt based, powerful TTS model.",id:"parler-tts/parler-tts-large-v1"},{description:"A powerful TTS model that supports English and Chinese.",id:"SWivid/F5-TTS"},{description:"A massively multi-lingual TTS model.",id:"fishaudio/fish-speech-1.5"},{description:"A powerful TTS model.",id:"OuteAI/OuteTTS-0.1-350M"},{description:"Small yet powerful TTS model.",id:"hexgrad/Kokoro-82M"}],spaces:[{description:"An application for generate high quality speech in different languages.",id:"hexgrad/Kokoro-TTS"},{description:"A multilingual text-to-speech application.",id:"fishaudio/fish-speech-1"},{description:"An application that generates speech in different styles in English and Chinese.",id:"mrfakename/E2-F5-TTS"},{description:"An application that synthesizes emotional speech for diverse speaker prompts.",id:"parler-tts/parler-tts-expresso"},{description:"An application that generates podcast episodes.",id:"ngxson/kokoro-podcast-generator"}],summary:"Text-to-Speech (TTS) is the task of generating natural sounding speech given text input. TTS models can be extended to have a single model that generates speech for multiple speakers and multiple languages.",widgetModels:["suno/bark"],youtubeId:"NW62DpzJ274"}),jt("text-to-video",{datasets:[{description:"Microsoft Research Video to Text is a large-scale dataset for open domain video captioning",id:"iejMac/CLIP-MSR-VTT"},{description:"UCF101 Human Actions dataset consists of 13,320 video clips from YouTube, with 101 classes.",id:"quchenyuan/UCF101-ZIP"},{description:"A high-quality dataset for human action recognition in YouTube videos.",id:"nateraw/kinetics"},{description:"A dataset of video clips of humans performing pre-defined basic actions with everyday objects.",id:"HuggingFaceM4/something_something_v2"},{description:"This dataset consists of text-video pairs and contains noisy samples with irrelevant video descriptions",id:"HuggingFaceM4/webvid"},{description:"A dataset of short Flickr videos for the temporal localization of events with descriptions.",id:"iejMac/CLIP-DiDeMo"}],demo:{inputs:[{label:"Input",content:"Darth Vader is surfing on the waves.",type:"text"}],outputs:[{filename:"text-to-video-output.gif",type:"img"}]},metrics:[{description:"Inception Score uses an image classification model that predicts class labels and evaluates how distinct and diverse the images are. A higher score indicates better video generation.",id:"is"},{description:"Frechet Inception Distance uses an image classification model to obtain image embeddings. The metric compares mean and standard deviation of the embeddings of real and generated images. A smaller score indicates better video generation.",id:"fid"},{description:"Frechet Video Distance uses a model that captures coherence for changes in frames and the quality of each frame. A smaller score indicates better video generation.",id:"fvd"},{description:"CLIPSIM measures similarity between video frames and text using an image-text similarity model. A higher score indicates better video generation.",id:"clipsim"}],models:[{description:"A strong model for consistent video generation.",id:"tencent/HunyuanVideo"},{description:"A text-to-video model with high fidelity motion and strong prompt adherence.",id:"Lightricks/LTX-Video"},{description:"A text-to-video model focusing on physics-aware applications like robotics.",id:"nvidia/Cosmos-1.0-Diffusion-7B-Text2World"},{description:"A robust model for video generation.",id:"Wan-AI/Wan2.1-T2V-1.3B"}],spaces:[{description:"An application that generates video from text.",id:"VideoCrafter/VideoCrafter"},{description:"Consistent video generation application.",id:"Wan-AI/Wan2.1"},{description:"A cutting edge video generation application.",id:"Pyramid-Flow/pyramid-flow"}],summary:"Text-to-video models can be used in any application that requires generating consistent sequence of images from text. ",widgetModels:["Wan-AI/Wan2.1-T2V-14B"],youtubeId:void 0}),jt("token-classification",{datasets:[{description:"A widely used dataset useful to benchmark named entity recognition models.",id:"eriktks/conll2003"},{description:"A multilingual dataset of Wikipedia articles annotated for named entity recognition in over 150 different languages.",id:"unimelb-nlp/wikiann"}],demo:{inputs:[{label:"Input",content:"My name is Omar and I live in Zürich.",type:"text"}],outputs:[{text:"My name is Omar and I live in Zürich.",tokens:[{type:"PERSON",start:11,end:15},{type:"GPE",start:30,end:36}],type:"text-with-tokens"}]},metrics:[{description:"",id:"accuracy"},{description:"",id:"recall"},{description:"",id:"precision"},{description:"",id:"f1"}],models:[{description:"A robust performance model to identify people, locations, organizations and names of miscellaneous entities.",id:"dslim/bert-base-NER"},{description:"A strong model to identify people, locations, organizations and names in multiple languages.",id:"FacebookAI/xlm-roberta-large-finetuned-conll03-english"},{description:"A token classification model specialized on medical entity recognition.",id:"blaze999/Medical-NER"},{description:"Flair models are typically the state of the art in named entity recognition tasks.",id:"flair/ner-english"}],spaces:[{description:"An application that can recognizes entities, extracts noun chunks and recognizes various linguistic features of each token.",id:"spacy/gradio_pipeline_visualizer"}],summary:"Token classification is a natural language understanding task in which a label is assigned to some tokens in a text. Some popular token classification subtasks are Named Entity Recognition (NER) and Part-of-Speech (PoS) tagging. NER models could be trained to identify specific entities in a text, such as dates, individuals and places; and PoS tagging would identify, for example, which words in a text are verbs, nouns, and punctuation marks.",widgetModels:["FacebookAI/xlm-roberta-large-finetuned-conll03-english"],youtubeId:"wVHdVlPScxA"}),jt("translation",{canonicalId:"text2text-generation",datasets:[{description:"A dataset of copyright-free books translated into 16 different languages.",id:"Helsinki-NLP/opus_books"},{description:"An example of translation between programming languages. This dataset consists of functions in Java and C#.",id:"google/code_x_glue_cc_code_to_code_trans"}],demo:{inputs:[{label:"Input",content:"My name is Omar and I live in Zürich.",type:"text"}],outputs:[{label:"Output",content:"Mein Name ist Omar und ich wohne in Zürich.",type:"text"}]},metrics:[{description:"BLEU score is calculated by counting the number of shared single or subsequent tokens between the generated sequence and the reference. Subsequent n tokens are called “n-grams”. Unigram refers to a single token while bi-gram refers to token pairs and n-grams refer to n subsequent tokens. The score ranges from 0 to 1, where 1 means the translation perfectly matched and 0 did not match at all",id:"bleu"},{description:"",id:"sacrebleu"}],models:[{description:"Very powerful model that can translate many languages between each other, especially low-resource languages.",id:"facebook/nllb-200-1.3B"},{description:"A general-purpose Transformer that can be used to translate from English to German, French, or Romanian.",id:"google-t5/t5-base"}],spaces:[{description:"An application that can translate between 100 languages.",id:"Iker/Translate-100-languages"},{description:"An application that can translate between many languages.",id:"Geonmo/nllb-translation-demo"}],summary:"Translation is the task of converting text from one language to another.",widgetModels:["facebook/mbart-large-50-many-to-many-mmt"],youtubeId:"1JvfrvZgi6c"}),jt("unconditional-image-generation",{datasets:[{description:"The CIFAR-100 dataset consists of 60000 32x32 colour images in 100 classes, with 600 images per class.",id:"cifar100"},{description:"Multiple images of celebrities, used for facial expression translation.",id:"CelebA"}],demo:{inputs:[{label:"Seed",content:"42",type:"text"},{label:"Number of images to generate:",content:"4",type:"text"}],outputs:[{filename:"unconditional-image-generation-output.jpeg",type:"img"}]},metrics:[{description:"The inception score (IS) evaluates the quality of generated images. It measures the diversity of the generated images (the model predictions are evenly distributed across all possible labels) and their 'distinction' or 'sharpness' (the model confidently predicts a single label for each image).",id:"Inception score (IS)"},{description:"The Fréchet Inception Distance (FID) evaluates the quality of images created by a generative model by calculating the distance between feature vectors for real and generated images.",id:"Frećhet Inception Distance (FID)"}],models:[{description:"High-quality image generation model trained on the CIFAR-10 dataset. It synthesizes images of the ten classes presented in the dataset using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.",id:"google/ddpm-cifar10-32"},{description:"High-quality image generation model trained on the 256x256 CelebA-HQ dataset. It synthesizes images of faces using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.",id:"google/ddpm-celebahq-256"}],spaces:[{description:"An application that can generate realistic faces.",id:"CompVis/celeba-latent-diffusion"}],summary:"Unconditional image generation is the task of generating images with no condition in any context (like a prompt text or another image). Once trained, the model will create images that resemble its training data distribution.",widgetModels:[""],youtubeId:""}),jt("video-text-to-text",{datasets:[{description:"Multiple-choice questions and answers about videos.",id:"lmms-lab/Video-MME"},{description:"A dataset of instructions and question-answer pairs about videos.",id:"lmms-lab/VideoChatGPT"},{description:"Large video understanding dataset.",id:"HuggingFaceFV/finevideo"}],demo:{inputs:[{filename:"video-text-to-text-input.gif",type:"img"},{label:"Text Prompt",content:"What is happening in this video?",type:"text"}],outputs:[{label:"Answer",content:"The video shows a series of images showing a fountain with water jets and a variety of colorful flowers and butterflies in the background.",type:"text"}]},metrics:[],models:[{description:"A robust video-text-to-text model.",id:"Vision-CAIR/LongVU_Qwen2_7B"},{description:"Strong video-text-to-text model with reasoning capabilities.",id:"GoodiesHere/Apollo-LMMs-Apollo-7B-t32"},{description:"Strong video-text-to-text model.",id:"HuggingFaceTB/SmolVLM2-2.2B-Instruct"}],spaces:[{description:"An application to chat with a video-text-to-text model.",id:"llava-hf/video-llava"},{description:"A leaderboard for various video-text-to-text models.",id:"opencompass/openvlm_video_leaderboard"},{description:"An application to generate highlights from a video.",id:"HuggingFaceTB/SmolVLM2-HighlightGenerator"}],summary:"Video-text-to-text models take in a video and a text prompt and output text. These models are also called video-language models.",widgetModels:[""],youtubeId:""}),jt("visual-question-answering",{datasets:[{description:"A widely used dataset containing questions (with answers) about images.",id:"Graphcore/vqa"},{description:"A dataset to benchmark visual reasoning based on text in images.",id:"facebook/textvqa"}],demo:{inputs:[{filename:"elephant.jpeg",type:"img"},{label:"Question",content:"What is in this image?",type:"text"}],outputs:[{type:"chart",data:[{label:"elephant",score:.97},{label:"elephants",score:.06},{label:"animal",score:.003}]}]},isPlaceholder:!1,metrics:[{description:"",id:"accuracy"},{description:"Measures how much a predicted answer differs from the ground truth based on the difference in their semantic meaning.",id:"wu-palmer similarity"}],models:[{description:"A visual question answering model trained to convert charts and plots to text.",id:"google/deplot"},{description:"A visual question answering model trained for mathematical reasoning and chart derendering from images.",id:"google/matcha-base"},{description:"A strong visual question answering that answers questions from book covers.",id:"google/pix2struct-ocrvqa-large"}],spaces:[{description:"An application that compares visual question answering models across different tasks.",id:"merve/pix2struct"},{description:"An application that can answer questions based on images.",id:"nielsr/vilt-vqa"},{description:"An application that can caption images and answer questions about a given image. ",id:"Salesforce/BLIP"},{description:"An application that can caption images and answer questions about a given image. ",id:"vumichien/Img2Prompt"}],summary:"Visual Question Answering is the task of answering open-ended questions based on an image. They output natural language responses to natural language questions.",widgetModels:["dandelin/vilt-b32-finetuned-vqa"],youtubeId:""}),jt("zero-shot-classification",{datasets:[{description:"A widely used dataset used to benchmark multiple variants of text classification.",id:"nyu-mll/glue"},{description:"The Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information.",id:"nyu-mll/multi_nli"},{description:"FEVER is a publicly available dataset for fact extraction and verification against textual sources.",id:"fever/fever"}],demo:{inputs:[{label:"Text Input",content:"Dune is the best movie ever.",type:"text"},{label:"Candidate Labels",content:"CINEMA, ART, MUSIC",type:"text"}],outputs:[{type:"chart",data:[{label:"CINEMA",score:.9},{label:"ART",score:.1},{label:"MUSIC",score:0}]}]},metrics:[],models:[{description:"Powerful zero-shot text classification model.",id:"facebook/bart-large-mnli"},{description:"Cutting-edge zero-shot multilingual text classification model.",id:"MoritzLaurer/ModernBERT-large-zeroshot-v2.0"},{description:"Zero-shot text classification model that can be used for topic and sentiment classification.",id:"knowledgator/gliclass-modern-base-v2.0-init"}],spaces:[],summary:"Zero-shot text classification is a task in natural language processing where a model is trained on a set of labeled examples but is then able to classify new examples from previously unseen classes.",widgetModels:["facebook/bart-large-mnli"]}),jt("zero-shot-image-classification",{datasets:[{description:"",id:""}],demo:{inputs:[{filename:"image-classification-input.jpeg",type:"img"},{label:"Classes",content:"cat, dog, bird",type:"text"}],outputs:[{type:"chart",data:[{label:"Cat",score:.664},{label:"Dog",score:.329},{label:"Bird",score:.008}]}]},metrics:[{description:"Computes the number of times the correct label appears in top K labels predicted",id:"top-K accuracy"}],models:[{description:"Multilingual image classification model for 80 languages.",id:"visheratin/mexma-siglip"},{description:"Strong zero-shot image classification model.",id:"google/siglip2-base-patch16-224"},{description:"Robust zero-shot image classification model.",id:"intfloat/mmE5-mllama-11b-instruct"},{description:"Powerful zero-shot image classification model supporting 94 languages.",id:"jinaai/jina-clip-v2"},{description:"Strong image classification model for biomedical domain.",id:"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224"}],spaces:[{description:"An application that leverages zero-shot image classification to find best captions to generate an image. ",id:"pharma/CLIP-Interrogator"},{description:"An application to compare different zero-shot image classification models. ",id:"merve/compare_clip_siglip"}],summary:"Zero-shot image classification is the task of classifying previously unseen classes during training of a model.",widgetModels:["google/siglip-so400m-patch14-224"],youtubeId:""}),jt("zero-shot-object-detection",{datasets:[],demo:{inputs:[{filename:"zero-shot-object-detection-input.jpg",type:"img"},{label:"Classes",content:"cat, dog, bird",type:"text"}],outputs:[{filename:"zero-shot-object-detection-output.jpg",type:"img"}]},metrics:[{description:"The Average Precision (AP) metric is the Area Under the PR Curve (AUC-PR). It is calculated for each class separately",id:"Average Precision"},{description:"The Mean Average Precision (mAP) metric is the overall average of the AP values",id:"Mean Average Precision"},{description:"The APα metric is the Average Precision at the IoU threshold of a α value, for example, AP50 and AP75",id:"APα"}],models:[{description:"Solid zero-shot object detection model.",id:"IDEA-Research/grounding-dino-base"},{description:"Cutting-edge zero-shot object detection model.",id:"google/owlv2-base-patch16-ensemble"}],spaces:[{description:"A demo to try the state-of-the-art zero-shot object detection model, OWLv2.",id:"merve/owlv2"},{description:"A demo that combines a zero-shot object detection and mask generation model for zero-shot segmentation.",id:"merve/OWLSAM"}],summary:"Zero-shot object detection is a computer vision task to detect objects and their classes in images, without any prior training or knowledge of the classes. Zero-shot object detection models receive an image as input, as well as a list of candidate classes, and output the bounding boxes and labels where the objects have been detected.",widgetModels:[],youtubeId:""}),jt("text-to-3d",{datasets:[{description:"A large dataset of over 10 million 3D objects.",id:"allenai/objaverse-xl"},{description:"Descriptive captions for 3D objects in Objaverse.",id:"tiange/Cap3D"}],demo:{inputs:[{label:"Prompt",content:"a cat statue",type:"text"}],outputs:[{label:"Result",content:"text-to-3d-3d-output-filename.glb",type:"text"}]},metrics:[],models:[{description:"Text-to-3D mesh model by OpenAI",id:"openai/shap-e"},{description:"Generative 3D gaussian splatting model.",id:"ashawkey/LGM"}],spaces:[{description:"Text-to-3D demo with mesh outputs.",id:"hysts/Shap-E"},{description:"Text/image-to-3D demo with splat outputs.",id:"ashawkey/LGM"}],summary:"Text-to-3D models take in text input and produce 3D output.",widgetModels:[],youtubeId:""}),jt("image-to-3d",{datasets:[{description:"A large dataset of over 10 million 3D objects.",id:"allenai/objaverse-xl"},{description:"A dataset of isolated object images for evaluating image-to-3D models.",id:"dylanebert/iso3d"}],demo:{inputs:[{filename:"image-to-3d-image-input.png",type:"img"}],outputs:[{label:"Result",content:"image-to-3d-3d-output-filename.glb",type:"text"}]},metrics:[],models:[{description:"Fast image-to-3D mesh model by Tencent.",id:"TencentARC/InstantMesh"},{description:"Fast image-to-3D mesh model by StabilityAI",id:"stabilityai/TripoSR"},{description:"A scaled up image-to-3D mesh model derived from TripoSR.",id:"hwjiang/Real3D"},{description:"Consistent image-to-3d generation model.",id:"stabilityai/stable-point-aware-3d"}],spaces:[{description:"Leaderboard to evaluate image-to-3D models.",id:"dylanebert/3d-arena"},{description:"Image-to-3D demo with mesh outputs.",id:"TencentARC/InstantMesh"},{description:"Image-to-3D demo.",id:"stabilityai/stable-point-aware-3d"},{description:"Image-to-3D demo with mesh outputs.",id:"hwjiang/Real3D"},{description:"Image-to-3D demo with splat outputs.",id:"dylanebert/LGM-mini"}],summary:"Image-to-3D models take in image input and produce 3D output.",widgetModels:[],youtubeId:""});const Rt=e=>e.tags.includes("conversational")?"text-generation"===e.pipeline_tag?[{role:"user",content:"What is the capital of France?"}]:[{role:"user",content:[{type:"text",text:"Describe this image in one sentence."},{type:"image_url",image_url:{url:"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg"}}]}]:'"Can you please let us know more details about your "',Ut=()=>'\'{"Height":[11.52,12.48],"Length1":[23.2,24.0],"Length2":[25.4,26.3],"Species": ["Bream","Bream"]}\'',Dt={"audio-to-audio":()=>'"sample1.flac"',"audio-classification":()=>'"sample1.flac"',"automatic-speech-recognition":()=>'"sample1.flac"',"document-question-answering":()=>'{\n        "image": "cat.png",\n        "question": "What is in this image?"\n    }',"feature-extraction":()=>'"Today is a sunny day and I will get some ice cream."',"fill-mask":e=>`"The answer to the universe is ${e.mask_token}."`,"image-classification":()=>'"cats.jpg"',"image-to-text":()=>'"cats.jpg"',"image-to-image":()=>'{\n    "image": "cat.png",\n    "prompt": "Turn the cat into a tiger."\n}',"image-segmentation":()=>'"cats.jpg"',"object-detection":()=>'"cats.jpg"',"question-answering":()=>'{\n    "question": "What is my name?",\n    "context": "My name is Clara and I live in Berkeley."\n}',"sentence-similarity":()=>'{\n    "source_sentence": "That is a happy person",\n    "sentences": [\n        "That is a happy dog",\n        "That is a very happy person",\n        "Today is a sunny day"\n    ]\n}',summarization:()=>'"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct."',"table-question-answering":()=>'{\n    "query": "How many stars does the transformers repository have?",\n    "table": {\n        "Repository": ["Transformers", "Datasets", "Tokenizers"],\n        "Stars": ["36542", "4512", "3934"],\n        "Contributors": ["651", "77", "34"],\n        "Programming language": [\n            "Python",\n            "Python",\n            "Rust, Python and NodeJS"\n        ]\n    }\n}',"tabular-regression":Ut,"tabular-classification":Ut,"text-classification":()=>'"I like you. I love you"',"text-generation":Rt,"image-text-to-text":Rt,"text-to-image":()=>'"Astronaut riding a horse"',"text-to-video":()=>'"A young man walking on the street"',"text-to-speech":()=>'"The answer to the universe is 42"',"text-to-audio":()=>'"liquid drum and bass, atmospheric synths, airy sounds"',"text2text-generation":()=>'"The answer to the universe is"',"token-classification":()=>'"My name is Sarah Jessica Parker but you can call me Jessica"',translation:()=>'"Меня зовут Вольфганг и я живу в Берлине"',"zero-shot-classification":()=>'"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!"',"zero-shot-image-classification":()=>'"cats.jpg"'};function Pt(e,t=!1,n=!1){if(e.pipeline_tag){const i=Dt[e.pipeline_tag];if(i){let a=i(e);if("string"==typeof a&&(t&&(a=a.replace(/(?:(?:\r?\n|\r)\t*)|\t+/g," ")),n)){const e=/^"(.+)"$/s,t=a.match(e);a=t?t[1]:a}return a}}return"No input example has been defined for this model task."}const qt="custom_code";function Nt(e){const t=e.split("/");return 1===t.length?t[0]:t[1]}function $t(e){var t,n;return(null==(n=null==(t=e.cardData)?void 0:t.base_model)?void 0:n.toString())??"fill-in-base-model"}function Ot(e){var t,n,i;const a=(null==(n=null==(t=e.widgetData)?void 0:t[0])?void 0:n.text)??(null==(i=e.cardData)?void 0:i.instance_prompt);if(a)return o=a,JSON.stringify(o).slice(1,-1);var o}const zt="Astronaut in a jungle, cold color palette, muted colors, detailed, 8k",Bt={CausalLM:e=>`\nimport keras_hub\n\n# Load CausalLM model (optional: use half precision for inference)\ncausal_lm = keras_hub.models.CausalLM.from_preset("hf://${e}", dtype="bfloat16")\ncausal_lm.compile(sampler="greedy")  # (optional) specify a sampler\n\n# Generate text\ncausal_lm.generate("Keras: deep learning for", max_length=64)\n`,TextToImage:e=>`\nimport keras_hub\n\n# Load TextToImage model (optional: use half precision for inference)\ntext_to_image = keras_hub.models.TextToImage.from_preset("hf://${e}", dtype="bfloat16")\n\n# Generate images with a TextToImage model.\ntext_to_image.generate("Astronaut in a jungle")\n`,TextClassifier:e=>`\nimport keras_hub\n\n# Load TextClassifier model\ntext_classifier = keras_hub.models.TextClassifier.from_preset(\n    "hf://${e}",\n    num_classes=2,\n)\n# Fine-tune\ntext_classifier.fit(x=["Thilling adventure!", "Total snoozefest."], y=[1, 0])\n# Classify text\ntext_classifier.predict(["Not my cup of tea."])\n`,ImageClassifier:e=>`\nimport keras_hub\nimport keras\n\n# Load ImageClassifier model\nimage_classifier = keras_hub.models.ImageClassifier.from_preset(\n    "hf://${e}",\n    num_classes=2,\n)\n# Fine-tune\nimage_classifier.fit(\n    x=keras.random.randint((32, 64, 64, 3), 0, 256),\n    y=keras.random.randint((32, 1), 0, 2),\n)\n# Classify image\nimage_classifier.predict(keras.random.randint((1, 64, 64, 3), 0, 256))\n`},Ft=(e,t)=>`\nimport keras_hub\n\n# Create a ${e} model\ntask = keras_hub.models.${e}.from_preset("hf://${t}")\n`;const Vt=e=>{const t=e.tags.find((e=>e.match(/^yolov\d+$/))),n=t?`YOLOv${t.slice(4)}`:"YOLOvXX";return[(t?"":"# Couldn't find a valid YOLO version tag.\n# Replace XX with the correct version.\n")+`from ultralytics import ${n}\n\nmodel = ${n}.from_pretrained("${e.id}")\nsource = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nmodel.predict(source=source, save=True)`]},Ht={"adapter-transformers":{prettyLabel:"Adapters",repoName:"adapters",repoUrl:"https://github.com/Adapter-Hub/adapters",docsUrl:"https://huggingface.co/docs/hub/adapters",snippets:e=>{var t,n;return[`from adapters import AutoAdapterModel\n\nmodel = AutoAdapterModel.from_pretrained("${null==(n=null==(t=e.config)?void 0:t.adapter_transformers)?void 0:n.model_name}")\nmodel.load_adapter("${e.id}", set_active=True)`]},filter:!0,countDownloads:'path:"adapter_config.json"'},allennlp:{prettyLabel:"AllenNLP",repoName:"AllenNLP",repoUrl:"https://github.com/allenai/allennlp",docsUrl:"https://huggingface.co/docs/hub/allennlp",snippets:e=>e.tags.includes("question-answering")?(e=>[`import allennlp_models\nfrom allennlp.predictors.predictor import Predictor\n\npredictor = Predictor.from_path("hf://${e.id}")\npredictor_input = {"passage": "My name is Wolfgang and I live in Berlin", "question": "Where do I live?"}\npredictions = predictor.predict_json(predictor_input)`])(e):(e=>[`import allennlp_models\nfrom allennlp.predictors.predictor import Predictor\n\npredictor = Predictor.from_path("hf://${e.id}")`])(e),filter:!0},anemoi:{prettyLabel:"AnemoI",repoName:"AnemoI",repoUrl:"https://github.com/ecmwf/anemoi-inference",docsUrl:"https://anemoi-docs.readthedocs.io/en/latest/",filter:!1,countDownloads:'path_extension:"ckpt"',snippets:e=>[`from anemoi.inference.runners.default import DefaultRunner\nfrom anemoi.inference.config import Configuration\n# Create Configuration\nconfig = Configuration(checkpoint = {"huggingface":{"repo_id":"${e.id}"}})\n# Load Runner\nrunner = DefaultRunner(config)`]},araclip:{prettyLabel:"AraClip",repoName:"AraClip",repoUrl:"https://huggingface.co/Arabic-Clip/araclip",filter:!1,snippets:e=>[`from araclip import AraClip\n\nmodel = AraClip.from_pretrained("${e.id}")`]},asteroid:{prettyLabel:"Asteroid",repoName:"Asteroid",repoUrl:"https://github.com/asteroid-team/asteroid",docsUrl:"https://huggingface.co/docs/hub/asteroid",snippets:e=>[`from asteroid.models import BaseModel\n\nmodel = BaseModel.from_pretrained("${e.id}")`],filter:!0,countDownloads:'path:"pytorch_model.bin"'},audiocraft:{prettyLabel:"Audiocraft",repoName:"audiocraft",repoUrl:"https://github.com/facebookresearch/audiocraft",snippets:e=>e.tags.includes("musicgen")?(e=>[`from audiocraft.models import MusicGen\n\nmodel = MusicGen.get_pretrained("${e.id}")\n\ndescriptions = ['happy rock', 'energetic EDM', 'sad jazz']\nwav = model.generate(descriptions)  # generates 3 samples.`])(e):e.tags.includes("audiogen")?(e=>[`from audiocraft.models import AudioGen\n\t\nmodel = AudioGen.get_pretrained("${e.id}")\nmodel.set_generation_params(duration=5)  # generate 5 seconds.\ndescriptions = ['dog barking', 'sirene of an emergency vehicle', 'footsteps in a corridor']\nwav = model.generate(descriptions)  # generates 3 samples.`])(e):e.tags.includes("magnet")?(e=>[`from audiocraft.models import MAGNeT\n\t\nmodel = MAGNeT.get_pretrained("${e.id}")\n\ndescriptions = ['disco beat', 'energetic EDM', 'funky groove']\nwav = model.generate(descriptions)  # generates 3 samples.`])(e):["# Type of model unknown."],filter:!1,countDownloads:'path:"state_dict.bin"'},audioseal:{prettyLabel:"AudioSeal",repoName:"audioseal",repoUrl:"https://github.com/facebookresearch/audioseal",filter:!1,countDownloads:'path_extension:"pth"',snippets:e=>[`# Watermark Generator\nfrom audioseal import AudioSeal\n\nmodel = AudioSeal.load_generator("${e.id}")\n# pass a tensor (tensor_wav) of shape (batch, channels, samples) and a sample rate\nwav, sr = tensor_wav, 16000\n\t\nwatermark = model.get_watermark(wav, sr)\nwatermarked_audio = wav + watermark`,`# Watermark Detector\nfrom audioseal import AudioSeal\n\ndetector = AudioSeal.load_detector("${e.id}")\n\t\nresult, message = detector.detect_watermark(watermarked_audio, sr)`]},ben2:{prettyLabel:"BEN2",repoName:"BEN2",repoUrl:"https://github.com/PramaLLC/BEN2",snippets:e=>[`import requests\nfrom PIL import Image\nfrom ben2 import AutoModel\n\nurl = "https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = AutoModel.from_pretrained("${e.id}")\nmodel.to("cuda").eval()\nforeground = model.inference(image)\n`],filter:!1},bertopic:{prettyLabel:"BERTopic",repoName:"BERTopic",repoUrl:"https://github.com/MaartenGr/BERTopic",snippets:e=>[`from bertopic import BERTopic\n\nmodel = BERTopic.load("${e.id}")`],filter:!0},big_vision:{prettyLabel:"Big Vision",repoName:"big_vision",repoUrl:"https://github.com/google-research/big_vision",filter:!1,countDownloads:'path_extension:"npz"'},birder:{prettyLabel:"Birder",repoName:"Birder",repoUrl:"https://gitlab.com/birder/birder",filter:!1,countDownloads:'path_extension:"pt"'},birefnet:{prettyLabel:"BiRefNet",repoName:"BiRefNet",repoUrl:"https://github.com/ZhengPeng7/BiRefNet",snippets:e=>[`# Option 1: use with transformers\n\nfrom transformers import AutoModelForImageSegmentation\nbirefnet = AutoModelForImageSegmentation.from_pretrained("${e.id}", trust_remote_code=True)\n`,`# Option 2: use with BiRefNet\n\n# Install from https://github.com/ZhengPeng7/BiRefNet\n\nfrom models.birefnet import BiRefNet\nmodel = BiRefNet.from_pretrained("${e.id}")`],filter:!1},bm25s:{prettyLabel:"BM25S",repoName:"bm25s",repoUrl:"https://github.com/xhluca/bm25s",snippets:e=>[`from bm25s.hf import BM25HF\n\nretriever = BM25HF.load_from_hub("${e.id}")`],filter:!1,countDownloads:'path:"params.index.json"'},champ:{prettyLabel:"Champ",repoName:"Champ",repoUrl:"https://github.com/fudan-generative-vision/champ",countDownloads:'path:"champ/motion_module.pth"'},chat_tts:{prettyLabel:"ChatTTS",repoName:"ChatTTS",repoUrl:"https://github.com/2noise/ChatTTS.git",snippets:()=>['import ChatTTS\nimport torchaudio\n\nchat = ChatTTS.Chat()\nchat.load_models(compile=False) # Set to True for better performance\n\ntexts = ["PUT YOUR TEXT HERE",]\n\nwavs = chat.infer(texts, )\n\ntorchaudio.save("output1.wav", torch.from_numpy(wavs[0]), 24000)'],filter:!1,countDownloads:'path:"asset/GPT.pt"'},colpali:{prettyLabel:"ColPali",repoName:"ColPali",repoUrl:"https://github.com/ManuelFay/colpali",filter:!1,countDownloads:'path:"adapter_config.json"'},comet:{prettyLabel:"COMET",repoName:"COMET",repoUrl:"https://github.com/Unbabel/COMET/",countDownloads:'path:"hparams.yaml"'},cosmos:{prettyLabel:"Cosmos",repoName:"Cosmos",repoUrl:"https://github.com/NVIDIA/Cosmos",countDownloads:'path:"config.json" OR path_extension:"pt"'},"cxr-foundation":{prettyLabel:"CXR Foundation",repoName:"cxr-foundation",repoUrl:"https://github.com/google-health/cxr-foundation",snippets:()=>["# pip install git+https://github.com/Google-Health/cxr-foundation.git#subdirectory=python\n\n# Load image as grayscale (Stillwaterising, CC0, via Wikimedia Commons)\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\"\nimg = Image.open(requests.get(image_url, headers={'User-Agent': 'Demo'}, stream=True).raw).convert('L')\n\n# Run inference\nfrom clientside.clients import make_hugging_face_client\ncxr_client = make_hugging_face_client('cxr_model')\nprint(cxr_client.get_image_embeddings_from_images([img]))"],filter:!1,countDownloads:'path:"precomputed_embeddings/embeddings.npz" OR path:"pax-elixr-b-text/saved_model.pb"'},deepforest:{prettyLabel:"DeepForest",repoName:"deepforest",docsUrl:"https://deepforest.readthedocs.io/en/latest/",repoUrl:"https://github.com/weecology/DeepForest"},"depth-anything-v2":{prettyLabel:"DepthAnythingV2",repoName:"Depth Anything V2",repoUrl:"https://github.com/DepthAnything/Depth-Anything-V2",snippets:e=>{let t,n,i;return t="<ENCODER>",n="<NUMBER_OF_FEATURES>",i="<OUT_CHANNELS>","depth-anything/Depth-Anything-V2-Small"===e.id?(t="vits",n="64",i="[48, 96, 192, 384]"):"depth-anything/Depth-Anything-V2-Base"===e.id?(t="vitb",n="128",i="[96, 192, 384, 768]"):"depth-anything/Depth-Anything-V2-Large"===e.id&&(t="vitl",n="256",i="[256, 512, 1024, 1024"),[`\n# Install from https://github.com/DepthAnything/Depth-Anything-V2\n\n# Load the model and infer depth from an image\nimport cv2\nimport torch\n\nfrom depth_anything_v2.dpt import DepthAnythingV2\n\n# instantiate the model\nmodel = DepthAnythingV2(encoder="${t}", features=${n}, out_channels=${i})\n\n# load the weights\nfilepath = hf_hub_download(repo_id="${e.id}", filename="depth_anything_v2_${t}.pth", repo_type="model")\nstate_dict = torch.load(filepath, map_location="cpu")\nmodel.load_state_dict(state_dict).eval()\n\nraw_img = cv2.imread("your/image/path")\ndepth = model.infer_image(raw_img) # HxW raw depth map in numpy\n    `]},filter:!1,countDownloads:'path_extension:"pth"'},"depth-pro":{prettyLabel:"Depth Pro",repoName:"Depth Pro",repoUrl:"https://github.com/apple/ml-depth-pro",countDownloads:'path_extension:"pt"',snippets:e=>[`# Download checkpoint\npip install huggingface-hub\nhuggingface-cli download --local-dir checkpoints ${e.id}`,'import depth_pro\n\n# Load model and preprocessing transform\nmodel, transform = depth_pro.create_model_and_transforms()\nmodel.eval()\n\n# Load and preprocess an image.\nimage, _, f_px = depth_pro.load_rgb("example.png")\nimage = transform(image)\n\n# Run inference.\nprediction = model.infer(image, f_px=f_px)\n\n# Results: 1. Depth in meters\ndepth = prediction["depth"]\n# Results: 2. Focal length in pixels\nfocallength_px = prediction["focallength_px"]'],filter:!1},"derm-foundation":{prettyLabel:"Derm Foundation",repoName:"derm-foundation",repoUrl:"https://github.com/google-health/derm-foundation",snippets:()=>['from huggingface_hub import from_pretrained_keras\nimport tensorflow as tf, requests\n\n# Load and format input\nIMAGE_URL = "https://storage.googleapis.com/dx-scin-public-data/dataset/images/3445096909671059178.png"\ninput_tensor = tf.train.Example(\n    features=tf.train.Features(\n        feature={\n            "image/encoded": tf.train.Feature(\n                bytes_list=tf.train.BytesList(value=[requests.get(IMAGE_URL, stream=True).content])\n            )\n        }\n    )\n).SerializeToString()\n\n# Load model and run inference\nloaded_model = from_pretrained_keras("google/derm-foundation")\ninfer = loaded_model.signatures["serving_default"]\nprint(infer(inputs=tf.constant([input_tensor])))'],filter:!1,countDownloads:'path:"scin_dataset_precomputed_embeddings.npz" OR path:"saved_model.pb"'},diffree:{prettyLabel:"Diffree",repoName:"Diffree",repoUrl:"https://github.com/OpenGVLab/Diffree",filter:!1,countDownloads:'path:"diffree-step=000010999.ckpt"'},diffusers:{prettyLabel:"Diffusers",repoName:"🤗/diffusers",repoUrl:"https://github.com/huggingface/diffusers",docsUrl:"https://huggingface.co/docs/hub/diffusers",snippets:e=>e.tags.includes("controlnet")?(e=>[`from diffusers import ControlNetModel, StableDiffusionControlNetPipeline\n\ncontrolnet = ControlNetModel.from_pretrained("${e.id}")\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n\t"${$t(e)}", controlnet=controlnet\n)`])(e):e.tags.includes("lora")?(e=>[`from diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained("${$t(e)}")\npipe.load_lora_weights("${e.id}")\n\nprompt = "${Ot(e)??zt}"\nimage = pipe(prompt).images[0]`])(e):e.tags.includes("textual_inversion")?(e=>[`from diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained("${$t(e)}")\npipe.load_textual_inversion("${e.id}")`])(e):(e=>[`from diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained("${e.id}")\n\nprompt = "${Ot(e)??zt}"\nimage = pipe(prompt).images[0]`])(e),filter:!0},diffusionkit:{prettyLabel:"DiffusionKit",repoName:"DiffusionKit",repoUrl:"https://github.com/argmaxinc/DiffusionKit",snippets:e=>{const t=`# Pipeline for Stable Diffusion 3\nfrom diffusionkit.mlx import DiffusionPipeline\n\npipeline = DiffusionPipeline(\n\tshift=3.0,\n\tuse_t5=False,\n\tmodel_version=${e.id},\n\tlow_memory_mode=True,\n\ta16=True,\n\tw16=True,\n)`,n=`# Pipeline for Flux\nfrom diffusionkit.mlx import FluxPipeline\n\npipeline = FluxPipeline(\n  shift=1.0,\n  model_version=${e.id},\n  low_memory_mode=True,\n  a16=True,\n  w16=True,\n)`,i=`# Image Generation\nHEIGHT = 512\nWIDTH = 512\nNUM_STEPS = ${e.tags.includes("flux")?4:50}\nCFG_WEIGHT = ${e.tags.includes("flux")?0:5}\n\nimage, _ = pipeline.generate_image(\n  "a photo of a cat",\n  cfg_weight=CFG_WEIGHT,\n  num_steps=NUM_STEPS,\n  latent_size=(HEIGHT // 8, WIDTH // 8),\n)`;return[e.tags.includes("flux")?n:t,i]}},doctr:{prettyLabel:"docTR",repoName:"doctr",repoUrl:"https://github.com/mindee/doctr"},cartesia_pytorch:{prettyLabel:"Cartesia Pytorch",repoName:"Cartesia Pytorch",repoUrl:"https://github.com/cartesia-ai/cartesia_pytorch",snippets:e=>[`# pip install --no-binary :all: cartesia-pytorch\nfrom cartesia_pytorch import ReneLMHeadModel\nfrom transformers import AutoTokenizer\n\nmodel = ReneLMHeadModel.from_pretrained("${e.id}")\ntokenizer = AutoTokenizer.from_pretrained("allenai/OLMo-1B-hf")\n\nin_message = ["Rene Descartes was"]\ninputs = tokenizer(in_message, return_tensors="pt")\n\noutputs = model.generate(inputs.input_ids, max_length=50, top_k=100, top_p=0.99)\nout_message = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n\nprint(out_message)\n)`]},cartesia_mlx:{prettyLabel:"Cartesia MLX",repoName:"Cartesia MLX",repoUrl:"https://github.com/cartesia-ai/cartesia_mlx",snippets:e=>[`import mlx.core as mx\nimport cartesia_mlx as cmx\n\nmodel = cmx.from_pretrained("${e.id}")\nmodel.set_dtype(mx.float32)   \n\nprompt = "Rene Descartes was"\n\nfor text in model.generate(\n    prompt,\n    max_tokens=500,\n    eval_every_n=5,\n    verbose=True,\n    top_p=0.99,\n    temperature=0.85,\n):\n    print(text, end="", flush=True)\n`]},clipscope:{prettyLabel:"clipscope",repoName:"clipscope",repoUrl:"https://github.com/Lewington-pitsos/clipscope",filter:!1,countDownloads:'path_extension:"pt"'},cosyvoice:{prettyLabel:"CosyVoice",repoName:"CosyVoice",repoUrl:"https://github.com/FunAudioLLM/CosyVoice",filter:!1,countDownloads:'path_extension:"onnx" OR path_extension:"pt"'},cotracker:{prettyLabel:"CoTracker",repoName:"CoTracker",repoUrl:"https://github.com/facebookresearch/co-tracker",filter:!1,countDownloads:'path_extension:"pth"'},edsnlp:{prettyLabel:"EDS-NLP",repoName:"edsnlp",repoUrl:"https://github.com/aphp/edsnlp",docsUrl:"https://aphp.github.io/edsnlp/latest/",filter:!1,snippets:e=>{const t=Nt(e.id).replaceAll("-","_");return[`# Load it from the Hub directly\nimport edsnlp\nnlp = edsnlp.load("${e.id}")\n`,`# Or install it as a package\n!pip install git+https://huggingface.co/${e.id}\n\n# and import it as a module\nimport ${t}\n\nnlp = ${t}.load()  # or edsnlp.load("${t}")\n`]},countDownloads:'path_filename:"config" AND path_extension:"cfg"'},elm:{prettyLabel:"ELM",repoName:"elm",repoUrl:"https://github.com/slicex-ai/elm",filter:!1,countDownloads:'path_filename:"slicex_elm_config" AND path_extension:"json"'},espnet:{prettyLabel:"ESPnet",repoName:"ESPnet",repoUrl:"https://github.com/espnet/espnet",docsUrl:"https://huggingface.co/docs/hub/espnet",snippets:e=>e.tags.includes("text-to-speech")?(e=>[`from espnet2.bin.tts_inference import Text2Speech\n\nmodel = Text2Speech.from_pretrained("${e.id}")\n\nspeech, *_ = model("text to generate speech from")`])(e):e.tags.includes("automatic-speech-recognition")?(e=>[`from espnet2.bin.asr_inference import Speech2Text\n\nmodel = Speech2Text.from_pretrained(\n  "${e.id}"\n)\n\nspeech, rate = soundfile.read("speech.wav")\ntext, *_ = model(speech)[0]`])(e):["unknown model type (must be text-to-speech or automatic-speech-recognition)"],filter:!0},fairseq:{prettyLabel:"Fairseq",repoName:"fairseq",repoUrl:"https://github.com/pytorch/fairseq",snippets:e=>[`from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\n\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\n    "${e.id}"\n)`],filter:!0},fastai:{prettyLabel:"fastai",repoName:"fastai",repoUrl:"https://github.com/fastai/fastai",docsUrl:"https://huggingface.co/docs/hub/fastai",snippets:e=>[`from huggingface_hub import from_pretrained_fastai\n\nlearn = from_pretrained_fastai("${e.id}")`],filter:!0},fasttext:{prettyLabel:"fastText",repoName:"fastText",repoUrl:"https://fasttext.cc/",snippets:e=>[`from huggingface_hub import hf_hub_download\nimport fasttext\n\nmodel = fasttext.load_model(hf_hub_download("${e.id}", "model.bin"))`],filter:!0,countDownloads:'path_extension:"bin"'},flair:{prettyLabel:"Flair",repoName:"Flair",repoUrl:"https://github.com/flairNLP/flair",docsUrl:"https://huggingface.co/docs/hub/flair",snippets:e=>[`from flair.models import SequenceTagger\n\ntagger = SequenceTagger.load("${e.id}")`],filter:!0,countDownloads:'path:"pytorch_model.bin"'},"gemma.cpp":{prettyLabel:"gemma.cpp",repoName:"gemma.cpp",repoUrl:"https://github.com/google/gemma.cpp",filter:!1,countDownloads:'path_extension:"sbs"'},"geometry-crafter":{prettyLabel:"GeometryCrafter",repoName:"GeometryCrafter",repoUrl:"https://github.com/TencentARC/GeometryCrafter",countDownloads:'path:"point_map_vae/diffusion_pytorch_model.safetensors"'},gliner:{prettyLabel:"GLiNER",repoName:"GLiNER",repoUrl:"https://github.com/urchade/GLiNER",snippets:e=>[`from gliner import GLiNER\n\nmodel = GLiNER.from_pretrained("${e.id}")`],filter:!1,countDownloads:'path:"gliner_config.json"'},"glyph-byt5":{prettyLabel:"Glyph-ByT5",repoName:"Glyph-ByT5",repoUrl:"https://github.com/AIGText/Glyph-ByT5",filter:!1,countDownloads:'path:"checkpoints/byt5_model.pt"'},grok:{prettyLabel:"Grok",repoName:"Grok",repoUrl:"https://github.com/xai-org/grok-1",filter:!1,countDownloads:'path:"ckpt/tensor00000_000" OR path:"ckpt-0/tensor00000_000"'},hallo:{prettyLabel:"Hallo",repoName:"Hallo",repoUrl:"https://github.com/fudan-generative-vision/hallo",countDownloads:'path:"hallo/net.pth"'},hezar:{prettyLabel:"Hezar",repoName:"Hezar",repoUrl:"https://github.com/hezarai/hezar",docsUrl:"https://hezarai.github.io/hezar",countDownloads:'path:"model_config.yaml" OR path:"embedding/embedding_config.yaml"'},htrflow:{prettyLabel:"HTRflow",repoName:"HTRflow",repoUrl:"https://github.com/AI-Riksarkivet/htrflow",docsUrl:"https://ai-riksarkivet.github.io/htrflow",snippets:e=>["# CLI usage\n# see docs: https://ai-riksarkivet.github.io/htrflow/latest/getting_started/quick_start.html\nhtrflow pipeline <path/to/pipeline.yaml> <path/to/image>",`# Python usage\nfrom htrflow.pipeline.pipeline import Pipeline\nfrom htrflow.pipeline.steps import Task\nfrom htrflow.models.framework.model import ModelClass\n\npipeline = Pipeline(\n    [\n        Task(\n            ModelClass, {"model": "${e.id}"}, {}\n        ),\n    ])`]},"hunyuan-dit":{prettyLabel:"HunyuanDiT",repoName:"HunyuanDiT",repoUrl:"https://github.com/Tencent/HunyuanDiT",countDownloads:'path:"pytorch_model_ema.pt" OR path:"pytorch_model_distill.pt"'},"hunyuan3d-2":{prettyLabel:"Hunyuan3D-2",repoName:"Hunyuan3D-2",repoUrl:"https://github.com/Tencent/Hunyuan3D-2",countDownloads:'path_filename:"model_index" OR path_filename:"config"'},imstoucan:{prettyLabel:"IMS Toucan",repoName:"IMS-Toucan",repoUrl:"https://github.com/DigitalPhonetics/IMS-Toucan",countDownloads:'path:"embedding_gan.pt" OR path:"Vocoder.pt" OR path:"ToucanTTS.pt"'},"infinite-you":{prettyLabel:"InfiniteYou",repoName:"InfiniteYou",repoUrl:"https://github.com/bytedance/InfiniteYou",filter:!1,countDownloads:'path:"infu_flux_v1.0/sim_stage1/image_proj_model.bin" OR path:"infu_flux_v1.0/aes_stage2/image_proj_model.bin"'},keras:{prettyLabel:"Keras",repoName:"Keras",repoUrl:"https://github.com/keras-team/keras",docsUrl:"https://huggingface.co/docs/hub/keras",snippets:e=>[`# Available backend options are: "jax", "torch", "tensorflow".\nimport os\nos.environ["KERAS_BACKEND"] = "jax"\n\t\nimport keras\n\nmodel = keras.saving.load_model("hf://${e.id}")\n`],filter:!0,countDownloads:'path:"config.json" OR path_extension:"keras"'},"tf-keras":{prettyLabel:"TF-Keras",repoName:"TF-Keras",repoUrl:"https://github.com/keras-team/tf-keras",docsUrl:"https://huggingface.co/docs/hub/tf-keras",snippets:e=>[`# Note: 'keras<3.x' or 'tf_keras' must be installed (legacy)\n# See https://github.com/keras-team/tf-keras for more details.\nfrom huggingface_hub import from_pretrained_keras\n\nmodel = from_pretrained_keras("${e.id}")\n`],countDownloads:'path:"saved_model.pb"'},"keras-hub":{prettyLabel:"KerasHub",repoName:"KerasHub",repoUrl:"https://github.com/keras-team/keras-hub",docsUrl:"https://keras.io/keras_hub/",snippets:e=>{var t,n;const i=e.id,a=(null==(n=null==(t=e.config)?void 0:t.keras_hub)?void 0:n.tasks)??[],o=[];for(const[r,s]of Object.entries(Bt))a.includes(r)&&o.push(s(i));for(const r of a)Object.keys(Bt).includes(r)||o.push(Ft(r,i));return o.push((e=>`\nimport keras_hub\n\n# Create a Backbone model unspecialized for any task\nbackbone = keras_hub.models.Backbone.from_preset("hf://${e}")\n`)(i)),o},filter:!0},k2:{prettyLabel:"K2",repoName:"k2",repoUrl:"https://github.com/k2-fsa/k2"},"lightning-ir":{prettyLabel:"Lightning IR",repoName:"Lightning IR",repoUrl:"https://github.com/webis-de/lightning-ir",snippets:e=>e.tags.includes("bi-encoder")?[`#install from https://github.com/webis-de/lightning-ir\n\nfrom lightning_ir import BiEncoderModule\nmodel = BiEncoderModule("${e.id}")\n\nmodel.score("query", ["doc1", "doc2", "doc3"])`]:e.tags.includes("cross-encoder")?[`#install from https://github.com/webis-de/lightning-ir\n\nfrom lightning_ir import CrossEncoderModule\nmodel = CrossEncoderModule("${e.id}")\n\nmodel.score("query", ["doc1", "doc2", "doc3"])`]:[`#install from https://github.com/webis-de/lightning-ir\n\nfrom lightning_ir import BiEncoderModule, CrossEncoderModule\n\n# depending on the model type, use either BiEncoderModule or CrossEncoderModule\nmodel = BiEncoderModule("${e.id}") \n# model = CrossEncoderModule("${e.id}")\n\nmodel.score("query", ["doc1", "doc2", "doc3"])`]},liveportrait:{prettyLabel:"LivePortrait",repoName:"LivePortrait",repoUrl:"https://github.com/KwaiVGI/LivePortrait",filter:!1,countDownloads:'path:"liveportrait/landmark.onnx"'},"llama-cpp-python":{prettyLabel:"llama-cpp-python",repoName:"llama-cpp-python",repoUrl:"https://github.com/abetlen/llama-cpp-python",snippets:e=>{const t=[`from llama_cpp import Llama\n\nllm = Llama.from_pretrained(\n\trepo_id="${e.id}",\n\tfilename="{{GGUF_FILE}}",\n)\n`];if(e.tags.includes("conversational")){const n=Pt(e);t.push(`llm.create_chat_completion(\n\tmessages = ${function(e,t){let n=JSON.stringify(e,null,"\t");return(null==t?void 0:t.indent)&&(n=n.replaceAll("\n",`\n${t.indent}`)),(null==t?void 0:t.attributeKeyQuotes)||(n=n.replace(/"([^"]+)":/g,"$1:")),(null==t?void 0:t.customContentEscaper)&&(n=t.customContentEscaper(n)),n}(n,{attributeKeyQuotes:!0,indent:"\t"})}\n)`)}else t.push('output = llm(\n\t"Once upon a time,",\n\tmax_tokens=512,\n\techo=True\n)\nprint(output)');return t}},"mini-omni2":{prettyLabel:"Mini-Omni2",repoName:"Mini-Omni2",repoUrl:"https://github.com/gpt-omni/mini-omni2",countDownloads:'path:"model_config.yaml"'},mindspore:{prettyLabel:"MindSpore",repoName:"mindspore",repoUrl:"https://github.com/mindspore-ai/mindspore"},"mamba-ssm":{prettyLabel:"MambaSSM",repoName:"MambaSSM",repoUrl:"https://github.com/state-spaces/mamba",filter:!1,snippets:e=>[`from mamba_ssm import MambaLMHeadModel\n\nmodel = MambaLMHeadModel.from_pretrained("${e.id}")`]},"mars5-tts":{prettyLabel:"MARS5-TTS",repoName:"MARS5-TTS",repoUrl:"https://github.com/Camb-ai/MARS5-TTS",filter:!1,countDownloads:'path:"mars5_ar.safetensors"',snippets:e=>[`# Install from https://github.com/Camb-ai/MARS5-TTS\n\nfrom inference import Mars5TTS\nmars5 = Mars5TTS.from_pretrained("${e.id}")`]},matanyone:{prettyLabel:"MatAnyone",repoName:"MatAnyone",repoUrl:"https://github.com/pq-yang/MatAnyone",snippets:e=>[`# Install from https://github.com/pq-yang/MatAnyone.git\n\nfrom matanyone.model.matanyone import MatAnyone\nmodel = MatAnyone.from_pretrained("${e.id}")`],filter:!1},"mesh-anything":{prettyLabel:"MeshAnything",repoName:"MeshAnything",repoUrl:"https://github.com/buaacyw/MeshAnything",filter:!1,countDownloads:'path:"MeshAnything_350m.pth"',snippets:()=>["# Install from https://github.com/buaacyw/MeshAnything.git\n\nfrom MeshAnything.models.meshanything import MeshAnything\n\n# refer to https://github.com/buaacyw/MeshAnything/blob/main/main.py#L91 on how to define args\n# and https://github.com/buaacyw/MeshAnything/blob/main/app.py regarding usage\nmodel = MeshAnything(args)"]},merlin:{prettyLabel:"Merlin",repoName:"Merlin",repoUrl:"https://github.com/StanfordMIMI/Merlin",filter:!1,countDownloads:'path_extension:"pt"'},medvae:{prettyLabel:"MedVAE",repoName:"MedVAE",repoUrl:"https://github.com/StanfordMIMI/MedVAE",filter:!1,countDownloads:'path_extension:"ckpt"'},mitie:{prettyLabel:"MITIE",repoName:"MITIE",repoUrl:"https://github.com/mit-nlp/MITIE",countDownloads:'path_filename:"total_word_feature_extractor"'},"ml-agents":{prettyLabel:"ml-agents",repoName:"ml-agents",repoUrl:"https://github.com/Unity-Technologies/ml-agents",docsUrl:"https://huggingface.co/docs/hub/ml-agents",snippets:e=>[`mlagents-load-from-hf --repo-id="${e.id}" --local-dir="./download: string[]s"`],filter:!0,countDownloads:'path_extension:"onnx"'},mlx:{prettyLabel:"MLX",repoName:"MLX",repoUrl:"https://github.com/ml-explore/mlx-examples/tree/main",snippets:e=>[`pip install huggingface_hub hf_transfer\n\nexport HF_HUB_ENABLE_HF_TRANSFER=1\nhuggingface-cli download --local-dir ${Nt(e.id)} ${e.id}`],filter:!0},"mlx-image":{prettyLabel:"mlx-image",repoName:"mlx-image",repoUrl:"https://github.com/riccardomusmeci/mlx-image",docsUrl:"https://huggingface.co/docs/hub/mlx-image",snippets:e=>[`from mlxim.model import create_model\n\nmodel = create_model(${e.id})`],filter:!1,countDownloads:'path:"model.safetensors"'},"mlc-llm":{prettyLabel:"MLC-LLM",repoName:"MLC-LLM",repoUrl:"https://github.com/mlc-ai/mlc-llm",docsUrl:"https://llm.mlc.ai/docs/",filter:!1,countDownloads:'path:"mlc-chat-config.json"'},model2vec:{prettyLabel:"Model2Vec",repoName:"model2vec",repoUrl:"https://github.com/MinishLab/model2vec",snippets:e=>[`from model2vec import StaticModel\n\nmodel = StaticModel.from_pretrained("${e.id}")`],filter:!1},moshi:{prettyLabel:"Moshi",repoName:"Moshi",repoUrl:"https://github.com/kyutai-labs/moshi",filter:!1,countDownloads:'path:"tokenizer-e351c8d8-checkpoint125.safetensors"'},nemo:{prettyLabel:"NeMo",repoName:"NeMo",repoUrl:"https://github.com/NVIDIA/NeMo",snippets:e=>{let t;return e.tags.includes("automatic-speech-recognition")&&(t=((e,t)=>{if("ASR"===e)return[`import nemo.collections.asr as nemo_asr\nasr_model = nemo_asr.models.ASRModel.from_pretrained("${t.id}")\n\ntranscriptions = asr_model.transcribe(["file.wav"])`]})("ASR",e)),t??["# tag did not correspond to a valid NeMo domain."]},filter:!0,countDownloads:'path_extension:"nemo" OR path:"model_config.yaml"'},"open-oasis":{prettyLabel:"open-oasis",repoName:"open-oasis",repoUrl:"https://github.com/etched-ai/open-oasis",countDownloads:'path:"oasis500m.safetensors"'},open_clip:{prettyLabel:"OpenCLIP",repoName:"OpenCLIP",repoUrl:"https://github.com/mlfoundations/open_clip",snippets:e=>[`import open_clip\n\nmodel, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:${e.id}')\ntokenizer = open_clip.get_tokenizer('hf-hub:${e.id}')`],filter:!0,countDownloads:'path:"open_clip_model.safetensors"\n\t\t\tOR path:"model.safetensors"\n\t\t\tOR path:"open_clip_pytorch_model.bin"\n\t\t\tOR path:"pytorch_model.bin"'},"open-sora":{prettyLabel:"Open-Sora",repoName:"Open-Sora",repoUrl:"https://github.com/hpcaitech/Open-Sora",filter:!1,countDownloads:'path:"Open_Sora_v2.safetensors"'},paddlenlp:{prettyLabel:"paddlenlp",repoName:"PaddleNLP",repoUrl:"https://github.com/PaddlePaddle/PaddleNLP",docsUrl:"https://huggingface.co/docs/hub/paddlenlp",snippets:e=>{var t,n;if(null==(n=null==(t=e.config)?void 0:t.architectures)?void 0:n[0]){const t=e.config.architectures[0];return[[`from paddlenlp.transformers import AutoTokenizer, ${t}`,"",`tokenizer = AutoTokenizer.from_pretrained("${e.id}", from_hf_hub=True)`,`model = ${t}.from_pretrained("${e.id}", from_hf_hub=True)`].join("\n")]}return[["# ⚠️ Type of model unknown","from paddlenlp.transformers import AutoTokenizer, AutoModel","",`tokenizer = AutoTokenizer.from_pretrained("${e.id}", from_hf_hub=True)`,`model = AutoModel.from_pretrained("${e.id}", from_hf_hub=True)`].join("\n")]},filter:!0,countDownloads:'path:"model_config.json"'},peft:{prettyLabel:"PEFT",repoName:"PEFT",repoUrl:"https://github.com/huggingface/peft",snippets:e=>{var t;const{base_model_name_or_path:n,task_type:i}=(null==(t=e.config)?void 0:t.peft)??{},a=(e=>{switch(e){case"CAUSAL_LM":return"CausalLM";case"SEQ_2_SEQ_LM":return"Seq2SeqLM";case"TOKEN_CLS":return"TokenClassification";case"SEQ_CLS":return"SequenceClassification";default:return}})(i);return a?n?[`from peft import PeftModel\nfrom transformers import AutoModelFor${a}\n\nbase_model = AutoModelFor${a}.from_pretrained("${n}")\nmodel = PeftModel.from_pretrained(base_model, "${e.id}")`]:["Base model is not found."]:["Task type is invalid."]},filter:!0,countDownloads:'path:"adapter_config.json"'},pxia:{prettyLabel:"pxia",repoName:"pxia",repoUrl:"https://github.com/not-lain/pxia",snippets:e=>[`from pxia import AutoModel\n\nmodel = AutoModel.from_pretrained("${e.id}")`],filter:!1},"pyannote-audio":{prettyLabel:"pyannote.audio",repoName:"pyannote-audio",repoUrl:"https://github.com/pyannote/pyannote-audio",snippets:e=>e.tags.includes("pyannote-audio-pipeline")?(e=>[`from pyannote.audio import Pipeline\n  \npipeline = Pipeline.from_pretrained("${e.id}")\n\n# inference on the whole file\npipeline("file.wav")\n\n# inference on an excerpt\nfrom pyannote.core import Segment\nexcerpt = Segment(start=2.0, end=5.0)\n\nfrom pyannote.audio import Audio\nwaveform, sample_rate = Audio().crop("file.wav", excerpt)\npipeline({"waveform": waveform, "sample_rate": sample_rate})`])(e):(e=>[`from pyannote.audio import Model, Inference\n\nmodel = Model.from_pretrained("${e.id}")\ninference = Inference(model)\n\n# inference on the whole file\ninference("file.wav")\n\n# inference on an excerpt\nfrom pyannote.core import Segment\nexcerpt = Segment(start=2.0, end=5.0)\ninference.crop("file.wav", excerpt)`])(e),filter:!0},"py-feat":{prettyLabel:"Py-Feat",repoName:"Py-Feat",repoUrl:"https://github.com/cosanlab/py-feat",docsUrl:"https://py-feat.org/",filter:!1},pythae:{prettyLabel:"pythae",repoName:"pythae",repoUrl:"https://github.com/clementchadebec/benchmark_VAE",snippets:e=>[`from pythae.models import AutoModel\n\nmodel = AutoModel.load_from_hf_hub("${e.id}")`],filter:!1},recurrentgemma:{prettyLabel:"RecurrentGemma",repoName:"recurrentgemma",repoUrl:"https://github.com/google-deepmind/recurrentgemma",filter:!1,countDownloads:'path:"tokenizer.model"'},relik:{prettyLabel:"Relik",repoName:"Relik",repoUrl:"https://github.com/SapienzaNLP/relik",snippets:e=>[`from relik import Relik\n \nrelik = Relik.from_pretrained("${e.id}")`],filter:!1},refiners:{prettyLabel:"Refiners",repoName:"Refiners",repoUrl:"https://github.com/finegrain-ai/refiners",docsUrl:"https://refine.rs/",filter:!1,countDownloads:'path:"model.safetensors"'},reverb:{prettyLabel:"Reverb",repoName:"Reverb",repoUrl:"https://github.com/revdotcom/reverb",filter:!1},saelens:{prettyLabel:"SAELens",repoName:"SAELens",repoUrl:"https://github.com/jbloomAus/SAELens",snippets:()=>['# pip install sae-lens\nfrom sae_lens import SAE\n\nsae, cfg_dict, sparsity = SAE.from_pretrained(\n    release = "RELEASE_ID", # e.g., "gpt2-small-res-jb". See other options in https://github.com/jbloomAus/SAELens/blob/main/sae_lens/pretrained_saes.yaml\n    sae_id = "SAE_ID", # e.g., "blocks.8.hook_resid_pre". Won\'t always be a hook point\n)'],filter:!1},sam2:{prettyLabel:"sam2",repoName:"sam2",repoUrl:"https://github.com/facebookresearch/segment-anything-2",filter:!1,snippets:e=>[`# Use SAM2 with images\nimport torch\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\n\npredictor = SAM2ImagePredictor.from_pretrained(${e.id})\n\nwith torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):\n    predictor.set_image(<your_image>)\n    masks, _, _ = predictor.predict(<input_prompts>)`,`# Use SAM2 with videos\nimport torch\nfrom sam2.sam2_video_predictor import SAM2VideoPredictor\n\t\npredictor = SAM2VideoPredictor.from_pretrained(${e.id})\n\nwith torch.inference_mode(), torch.autocast("cuda", dtype=torch.bfloat16):\n    state = predictor.init_state(<your_video>)\n\n    # add new prompts and instantly get the output on the same frame\n    frame_idx, object_ids, masks = predictor.add_new_points(state, <your_prompts>):\n\n    # propagate the prompts to get masklets throughout the video\n    for frame_idx, object_ids, masks in predictor.propagate_in_video(state):\n        ...`],countDownloads:'path_extension:"pt"'},"sample-factory":{prettyLabel:"sample-factory",repoName:"sample-factory",repoUrl:"https://github.com/alex-petrenko/sample-factory",docsUrl:"https://huggingface.co/docs/hub/sample-factory",snippets:e=>[`python -m sample_factory.huggingface.load_from_hub -r ${e.id} -d ./train_dir`],filter:!0,countDownloads:'path:"cfg.json"'},sapiens:{prettyLabel:"sapiens",repoName:"sapiens",repoUrl:"https://github.com/facebookresearch/sapiens",filter:!1,countDownloads:'path_extension:"pt2" OR path_extension:"pth" OR path_extension:"onnx"'},"sentence-transformers":{prettyLabel:"sentence-transformers",repoName:"sentence-transformers",repoUrl:"https://github.com/UKPLab/sentence-transformers",docsUrl:"https://huggingface.co/docs/hub/sentence-transformers",snippets:e=>{const t=e.tags.includes(qt)?", trust_remote_code=True":"";if(e.tags.includes("cross-encoder")||"text-ranking"==e.pipeline_tag)return[`from sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder("${e.id}"${t})\n\nquery = "Which planet is known as the Red Planet?"\npassages = [\n\t"Venus is often called Earth's twin because of its similar size and proximity.",\n\t"Mars, known for its reddish appearance, is often referred to as the Red Planet.",\n\t"Jupiter, the largest planet in our solar system, has a prominent red spot.",\n\t"Saturn, famous for its rings, is sometimes mistaken for the Red Planet."\n]\n\nscores = model.predict([(query, passage) for passage in passages])\nprint(scores)`];const n=function(e){var t,n;const i=null==(t=e.widgetData)?void 0:t[0];if((null==i?void 0:i.source_sentence)&&(null==(n=null==i?void 0:i.sentences)?void 0:n.length))return[i.source_sentence,...i.sentences]}(e)??["The weather is lovely today.","It's so sunny outside!","He drove to the stadium."];return[`from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer("${e.id}"${t})\n\nsentences = ${JSON.stringify(n,null,4)}\nembeddings = model.encode(sentences)\n\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities.shape)\n# [${n.length}, ${n.length}]`]},filter:!0},setfit:{prettyLabel:"setfit",repoName:"setfit",repoUrl:"https://github.com/huggingface/setfit",docsUrl:"https://huggingface.co/docs/hub/setfit",snippets:e=>[`from setfit import SetFitModel\n\nmodel = SetFitModel.from_pretrained("${e.id}")`],filter:!0},sklearn:{prettyLabel:"Scikit-learn",repoName:"Scikit-learn",repoUrl:"https://github.com/scikit-learn/scikit-learn",snippets:e=>{var t,n,i,a,o;if(e.tags.includes("skops")){const r=null==(i=null==(n=null==(t=e.config)?void 0:t.sklearn)?void 0:n.model)?void 0:i.file,s=null==(o=null==(a=e.config)?void 0:a.sklearn)?void 0:o.model_format;return r?"pickle"===s?((e,t)=>[`import joblib\nfrom skops.hub_utils import download\ndownload("${e.id}", "path_to_folder")\nmodel = joblib.load(\n\t"${t}"\n)\n# only load pickle files from sources you trust\n# read more about it here https://skops.readthedocs.io/en/stable/persistence.html`])(e,r):((e,t)=>[`from skops.hub_utils import download\nfrom skops.io import load\ndownload("${e.id}", "path_to_folder")\n# make sure model file is in skops format\n# if model is a pickle file, make sure it's from a source you trust\nmodel = load("path_to_folder/${t}")`])(e,r):["# ⚠️ Model filename not specified in config.json"]}return(e=>[`from huggingface_hub import hf_hub_download\nimport joblib\nmodel = joblib.load(\n\thf_hub_download("${e.id}", "sklearn_model.joblib")\n)\n# only load pickle files from sources you trust\n# read more about it here https://skops.readthedocs.io/en/stable/persistence.html`])(e)},filter:!0,countDownloads:'path:"sklearn_model.joblib"'},spacy:{prettyLabel:"spaCy",repoName:"spaCy",repoUrl:"https://github.com/explosion/spaCy",docsUrl:"https://huggingface.co/docs/hub/spacy",snippets:e=>[`!pip install https://huggingface.co/${e.id}/resolve/main/${Nt(e.id)}-any-py3-none-any.whl\n\n# Using spacy.load().\nimport spacy\nnlp = spacy.load("${Nt(e.id)}")\n\n# Importing as module.\nimport ${Nt(e.id)}\nnlp = ${Nt(e.id)}.load()`],filter:!0,countDownloads:'path_extension:"whl"'},"span-marker":{prettyLabel:"SpanMarker",repoName:"SpanMarkerNER",repoUrl:"https://github.com/tomaarsen/SpanMarkerNER",docsUrl:"https://huggingface.co/docs/hub/span_marker",snippets:e=>[`from span_marker import SpanMarkerModel\n\nmodel = SpanMarkerModel.from_pretrained("${e.id}")`],filter:!0},speechbrain:{prettyLabel:"speechbrain",repoName:"speechbrain",repoUrl:"https://github.com/speechbrain/speechbrain",docsUrl:"https://huggingface.co/docs/hub/speechbrain",snippets:e=>{var t,n;const i=null==(n=null==(t=e.config)?void 0:t.speechbrain)?void 0:n.speechbrain_interface;if(void 0===i)return["# interface not specified in config.json"];const a=(e=>{switch(e){case"EncoderClassifier":return"classify_file";case"EncoderDecoderASR":case"EncoderASR":return"transcribe_file";case"SpectralMaskEnhancement":return"enhance_file";case"SepformerSeparation":return"separate_file";default:return}})(i);return void 0===a?["# interface in config.json invalid"]:[`from speechbrain.pretrained import ${i}\nmodel = ${i}.from_hparams(\n  "${e.id}"\n)\nmodel.${a}("file.wav")`]},filter:!0,countDownloads:'path:"hyperparams.yaml"'},"ssr-speech":{prettyLabel:"SSR-Speech",repoName:"SSR-Speech",repoUrl:"https://github.com/WangHelin1997/SSR-Speech",filter:!1,countDownloads:'path_extension:".pth"'},"stable-audio-tools":{prettyLabel:"Stable Audio Tools",repoName:"stable-audio-tools",repoUrl:"https://github.com/Stability-AI/stable-audio-tools.git",filter:!1,countDownloads:'path:"model.safetensors"',snippets:e=>[`import torch\nimport torchaudio\nfrom einops import rearrange\nfrom stable_audio_tools import get_pretrained_model\nfrom stable_audio_tools.inference.generation import generate_diffusion_cond\n\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\n\n# Download model\nmodel, model_config = get_pretrained_model("${e.id}")\nsample_rate = model_config["sample_rate"]\nsample_size = model_config["sample_size"]\n\nmodel = model.to(device)\n\n# Set up text and timing conditioning\nconditioning = [{\n\t"prompt": "128 BPM tech house drum loop",\n}]\n\n# Generate stereo audio\noutput = generate_diffusion_cond(\n\tmodel,\n\tconditioning=conditioning,\n\tsample_size=sample_size,\n\tdevice=device\n)\n\n# Rearrange audio batch to a single sequence\noutput = rearrange(output, "b d n -> d (b n)")\n\n# Peak normalize, clip, convert to int16, and save to file\noutput = output.to(torch.float32).div(torch.max(torch.abs(output))).clamp(-1, 1).mul(32767).to(torch.int16).cpu()\ntorchaudio.save("output.wav", output, sample_rate)`]},"diffusion-single-file":{prettyLabel:"Diffusion Single File",repoName:"diffusion-single-file",repoUrl:"https://github.com/comfyanonymous/ComfyUI",filter:!1,countDownloads:'path_extension:"safetensors"'},"seed-story":{prettyLabel:"SEED-Story",repoName:"SEED-Story",repoUrl:"https://github.com/TencentARC/SEED-Story",filter:!1,countDownloads:'path:"cvlm_llama2_tokenizer/tokenizer.model"',snippets:()=>["# seed_story_cfg_path refers to 'https://github.com/TencentARC/SEED-Story/blob/master/configs/clm_models/agent_7b_sft.yaml'\n# llm_cfg_path refers to 'https://github.com/TencentARC/SEED-Story/blob/master/configs/clm_models/llama2chat7b_lora.yaml'\nfrom omegaconf import OmegaConf\nimport hydra\n\n# load Llama2\nllm_cfg = OmegaConf.load(llm_cfg_path)\nllm = hydra.utils.instantiate(llm_cfg, torch_dtype=\"fp16\")\n\n# initialize seed_story\nseed_story_cfg = OmegaConf.load(seed_story_cfg_path)\nseed_story = hydra.utils.instantiate(seed_story_cfg, llm=llm) "]},soloaudio:{prettyLabel:"SoloAudio",repoName:"SoloAudio",repoUrl:"https://github.com/WangHelin1997/SoloAudio",filter:!1,countDownloads:'path:"soloaudio_v2.pt"'},"stable-baselines3":{prettyLabel:"stable-baselines3",repoName:"stable-baselines3",repoUrl:"https://github.com/huggingface/huggingface_sb3",docsUrl:"https://huggingface.co/docs/hub/stable-baselines3",snippets:e=>[`from huggingface_sb3 import load_from_hub\ncheckpoint = load_from_hub(\n\trepo_id="${e.id}",\n\tfilename="{MODEL FILENAME}.zip",\n)`],filter:!0,countDownloads:'path_extension:"zip"'},stanza:{prettyLabel:"Stanza",repoName:"stanza",repoUrl:"https://github.com/stanfordnlp/stanza",docsUrl:"https://huggingface.co/docs/hub/stanza",snippets:e=>[`import stanza\n\nstanza.download("${Nt(e.id).replace("stanza-","")}")\nnlp = stanza.Pipeline("${Nt(e.id).replace("stanza-","")}")`],filter:!0,countDownloads:'path:"models/default.zip"'},swarmformer:{prettyLabel:"SwarmFormer",repoName:"SwarmFormer",repoUrl:"https://github.com/takara-ai/SwarmFormer",snippets:e=>[`from swarmformer import SwarmFormerModel\n\nmodel = SwarmFormerModel.from_pretrained("${e.id}")\n`],filter:!1},"f5-tts":{prettyLabel:"F5-TTS",repoName:"F5-TTS",repoUrl:"https://github.com/SWivid/F5-TTS",filter:!1,countDownloads:'path_extension:"safetensors" OR path_extension:"pt"'},genmo:{prettyLabel:"Genmo",repoName:"Genmo",repoUrl:"https://github.com/genmoai/models",filter:!1,countDownloads:'path:"vae_stats.json"'},tensorflowtts:{prettyLabel:"TensorFlowTTS",repoName:"TensorFlowTTS",repoUrl:"https://github.com/TensorSpeech/TensorFlowTTS",snippets:e=>e.tags.includes("text-to-mel")?(e=>[`from tensorflow_tts.inference import AutoProcessor, TFAutoModel\n\nprocessor = AutoProcessor.from_pretrained("${e.id}")\nmodel = TFAutoModel.from_pretrained("${e.id}")\n`])(e):e.tags.includes("mel-to-wav")?(e=>[`from tensorflow_tts.inference import TFAutoModel\n\nmodel = TFAutoModel.from_pretrained("${e.id}")\naudios = model.inference(mels)\n`])(e):(e=>[`from tensorflow_tts.inference import TFAutoModel\n\nmodel = TFAutoModel.from_pretrained("${e.id}")\n`])(e)},tabpfn:{prettyLabel:"TabPFN",repoName:"TabPFN",repoUrl:"https://github.com/PriorLabs/TabPFN"},terratorch:{prettyLabel:"TerraTorch",repoName:"TerraTorch",repoUrl:"https://github.com/IBM/terratorch",docsUrl:"https://ibm.github.io/terratorch/",filter:!1,countDownloads:'path_extension:"pt"',snippets:e=>[`from terratorch.registry import BACKBONE_REGISTRY\n\nmodel = BACKBONE_REGISTRY.build("${e.id}")`]},"tic-clip":{prettyLabel:"TiC-CLIP",repoName:"TiC-CLIP",repoUrl:"https://github.com/apple/ml-tic-clip",filter:!1,countDownloads:'path_extension:"pt" AND path_prefix:"checkpoints/"'},timesfm:{prettyLabel:"TimesFM",repoName:"timesfm",repoUrl:"https://github.com/google-research/timesfm",filter:!1,countDownloads:'path:"checkpoints/checkpoint_1100000/state/checkpoint"'},timm:{prettyLabel:"timm",repoName:"pytorch-image-models",repoUrl:"https://github.com/rwightman/pytorch-image-models",docsUrl:"https://huggingface.co/docs/hub/timm",snippets:e=>[`import timm\n\nmodel = timm.create_model("hf_hub:${e.id}", pretrained=True)`],filter:!0,countDownloads:'path:"pytorch_model.bin" OR path:"model.safetensors"'},transformers:{prettyLabel:"Transformers",repoName:"🤗/transformers",repoUrl:"https://github.com/huggingface/transformers",docsUrl:"https://huggingface.co/docs/hub/transformers",snippets:e=>{var t,n,i,a,o;const r=e.transformersInfo;if(!r)return["# ⚠️ Type of model unknown"];const s=e.tags.includes(qt)?", trust_remote_code=True":"";let l;if(r.processor){const t="AutoTokenizer"===r.processor?"tokenizer":"AutoFeatureExtractor"===r.processor?"extractor":"processor";l=["# Load model directly",`from transformers import ${r.processor}, ${r.auto_model}`,"",`${t} = ${r.processor}.from_pretrained("${e.id}"`+s+")",`model = ${r.auto_model}.from_pretrained("${e.id}"`+s+")"].join("\n")}else l=["# Load model directly",`from transformers import ${r.auto_model}`,`model = ${r.auto_model}.from_pretrained("${e.id}"`+s+")"].join("\n");if(e.pipeline_tag&&(null==(t=Tt)?void 0:t.includes(e.pipeline_tag))){const t=["# Use a pipeline as a high-level helper","from transformers import pipeline",""];return e.tags.includes("conversational")&&(null==(i=null==(n=e.config)?void 0:n.tokenizer_config)?void 0:i.chat_template)&&t.push("messages = [",'    {"role": "user", "content": "Who are you?"},',"]"),t.push(`pipe = pipeline("${e.pipeline_tag}", model="${e.id}"`+s+")"),e.tags.includes("conversational")&&(null==(o=null==(a=e.config)?void 0:a.tokenizer_config)?void 0:o.chat_template)&&t.push("pipe(messages)"),[t.join("\n"),l]}return[l]},filter:!0},"transformers.js":{prettyLabel:"Transformers.js",repoName:"transformers.js",repoUrl:"https://github.com/huggingface/transformers.js",docsUrl:"https://huggingface.co/docs/hub/transformers-js",snippets:e=>{if(!e.pipeline_tag)return["// ⚠️ Unknown pipeline tag"];const t="@huggingface/transformers";return[`// npm i ${t}\nimport { pipeline } from '${t}';\n\n// Allocate pipeline\nconst pipe = await pipeline('${e.pipeline_tag}', '${e.id}');`]},filter:!0},trellis:{prettyLabel:"Trellis",repoName:"Trellis",repoUrl:"https://github.com/microsoft/TRELLIS",countDownloads:'path_extension:"safetensors"'},ultralytics:{prettyLabel:"ultralytics",repoName:"ultralytics",repoUrl:"https://github.com/ultralytics/ultralytics",docsUrl:"https://github.com/ultralytics/ultralytics",filter:!1,countDownloads:'path_extension:"pt"',snippets:Vt},"uni-3dar":{prettyLabel:"Uni-3DAR",repoName:"Uni-3DAR",repoUrl:"https://github.com/dptech-corp/Uni-3DAR",docsUrl:"https://github.com/dptech-corp/Uni-3DAR",countDownloads:'path_extension:"pt"'},"unity-sentis":{prettyLabel:"unity-sentis",repoName:"unity-sentis",repoUrl:"https://github.com/Unity-Technologies/sentis-samples",snippets:()=>['string modelName = "[Your model name here].sentis";\nModel model = ModelLoader.Load(Application.streamingAssetsPath + "/" + modelName);\nIWorker engine = WorkerFactory.CreateWorker(BackendType.GPUCompute, model);\n// Please see provided C# file for more details\n'],filter:!0,countDownloads:'path_extension:"sentis"'},sana:{prettyLabel:"Sana",repoName:"Sana",repoUrl:"https://github.com/NVlabs/Sana",countDownloads:'path_extension:"pth"',snippets:e=>[`\n# Load the model and infer image from text\nimport torch\nfrom app.sana_pipeline import SanaPipeline\nfrom torchvision.utils import save_image\n\nsana = SanaPipeline("configs/sana_config/1024ms/Sana_1600M_img1024.yaml")\nsana.from_pretrained("hf://${e.id}")\n\nimage = sana(\n    prompt='a cyberpunk cat with a neon sign that says "Sana"',\n    height=1024,\n    width=1024,\n    guidance_scale=5.0,\n    pag_guidance_scale=2.0,\n    num_inference_steps=18,\n) `]},"vfi-mamba":{prettyLabel:"VFIMamba",repoName:"VFIMamba",repoUrl:"https://github.com/MCG-NJU/VFIMamba",countDownloads:'path_extension:"pkl"',snippets:e=>[`from Trainer_finetune import Model\n\nmodel = Model.from_pretrained("${e.id}")`]},voicecraft:{prettyLabel:"VoiceCraft",repoName:"VoiceCraft",repoUrl:"https://github.com/jasonppy/VoiceCraft",docsUrl:"https://github.com/jasonppy/VoiceCraft",snippets:e=>[`from voicecraft import VoiceCraft\n\nmodel = VoiceCraft.from_pretrained("${e.id}")`]},wham:{prettyLabel:"WHAM",repoName:"wham",repoUrl:"https://huggingface.co/microsoft/wham",docsUrl:"https://huggingface.co/microsoft/wham/blob/main/README.md",countDownloads:'path_extension:"ckpt"'},whisperkit:{prettyLabel:"WhisperKit",repoName:"WhisperKit",repoUrl:"https://github.com/argmaxinc/WhisperKit",docsUrl:"https://github.com/argmaxinc/WhisperKit?tab=readme-ov-file#homebrew",snippets:()=>['# Install CLI with Homebrew on macOS device\nbrew install whisperkit-cli\n\n# View all available inference options\nwhisperkit-cli transcribe --help\n\t\n# Download and run inference using whisper base model\nwhisperkit-cli transcribe --audio-path /path/to/audio.mp3\n\n# Or use your preferred model variant\nwhisperkit-cli transcribe --model "large-v3" --model-prefix "distil" --audio-path /path/to/audio.mp3 --verbose'],countDownloads:'path_filename:"model" AND path_extension:"mil" AND _exists_:"path_prefix"'},yolov10:{prettyLabel:"YOLOv10",repoName:"YOLOv10",repoUrl:"https://github.com/THU-MIG/yolov10",docsUrl:"https://github.com/THU-MIG/yolov10",countDownloads:'path_extension:"pt" OR path_extension:"safetensors"',snippets:Vt},"3dtopia-xl":{prettyLabel:"3DTopia-XL",repoName:"3DTopia-XL",repoUrl:"https://github.com/3DTopia/3DTopia-XL",filter:!1,countDownloads:'path:"model_vae_fp16.pt"',snippets:e=>[`from threedtopia_xl.models import threedtopia_xl\n\nmodel = threedtopia_xl.from_pretrained("${e.id}")\nmodel.generate(cond="path/to/image.png")`]}};var Gt,Qt;Object.entries(Ht).filter((([e,t])=>t.filter)).map((([e])=>e)),(Qt=Gt||(Gt={}))[Qt.F32=0]="F32",Qt[Qt.F16=1]="F16",Qt[Qt.Q4_0=2]="Q4_0",Qt[Qt.Q4_1=3]="Q4_1",Qt[Qt.Q5_0=6]="Q5_0",Qt[Qt.Q5_1=7]="Q5_1",Qt[Qt.Q8_0=8]="Q8_0",Qt[Qt.Q8_1=9]="Q8_1",Qt[Qt.Q2_K=10]="Q2_K",Qt[Qt.Q3_K=11]="Q3_K",Qt[Qt.Q4_K=12]="Q4_K",Qt[Qt.Q5_K=13]="Q5_K",Qt[Qt.Q6_K=14]="Q6_K",Qt[Qt.Q8_K=15]="Q8_K",Qt[Qt.IQ2_XXS=16]="IQ2_XXS",Qt[Qt.IQ2_XS=17]="IQ2_XS",Qt[Qt.IQ3_XXS=18]="IQ3_XXS",Qt[Qt.IQ1_S=19]="IQ1_S",Qt[Qt.IQ4_NL=20]="IQ4_NL",Qt[Qt.IQ3_S=21]="IQ3_S",Qt[Qt.IQ2_S=22]="IQ2_S",Qt[Qt.IQ4_XS=23]="IQ4_XS",Qt[Qt.I8=24]="I8",Qt[Qt.I16=25]="I16",Qt[Qt.I32=26]="I32",Qt[Qt.I64=27]="I64",Qt[Qt.F64=28]="F64",Qt[Qt.IQ1_M=29]="IQ1_M",Qt[Qt.BF16=30]="BF16";const Wt=Object.values(Gt).filter((e=>"string"==typeof e));new RegExp(`(?<quant>${Wt.join("|")})(_(?<sizeVariation>[A-Z]+))?`);const Kt=["python","js","sh"];var Jt=Object.defineProperty,Xt=(e,t)=>{for(var n in t)Jt(e,n,{get:t[n],enumerable:!0})},Yt={};Xt(Yt,{audioClassification:()=>Dn,audioToAudio:()=>Pn,automaticSpeechRecognition:()=>Nn,chatCompletion:()=>Kn,chatCompletionStream:()=>Jn,documentQuestionAnswering:()=>ci,featureExtraction:()=>Xn,fillMask:()=>Yn,imageClassification:()=>zn,imageSegmentation:()=>Bn,imageToImage:()=>Fn,imageToText:()=>Vn,objectDetection:()=>Hn,questionAnswering:()=>Zn,request:()=>jn,sentenceSimilarity:()=>ei,streamingRequest:()=>Rn,summarization:()=>ti,tableQuestionAnswering:()=>ni,tabularClassification:()=>pi,tabularRegression:()=>ui,textClassification:()=>ii,textGeneration:()=>ai,textGenerationStream:()=>oi,textToImage:()=>Gn,textToSpeech:()=>$n,textToVideo:()=>Qn,tokenClassification:()=>ri,translation:()=>si,visualQuestionAnswering:()=>di,zeroShotClassification:()=>li,zeroShotImageClassification:()=>Wn});var Zt="@huggingface/inference",en="3.7.1",tn="https://huggingface.co",nn="https://router.huggingface.co",an="X-HF-Bill-To",on=class extends TypeError{constructor(e){super(`Invalid inference output: ${e}. Use the 'request' method with the same parameters to do a custom call with no type checking.`),this.name="InferenceOutputError"}};function rn(e){return new Promise((t=>{setTimeout((()=>t()),e)}))}function sn(e,t){const n=Array.isArray(t)?t:[t];return function(e,t){return Object.assign({},...t.map((t=>{if(void 0!==e[t])return{[t]:e[t]}})))}(e,Object.keys(e).filter((e=>{return t=e,!n.includes(t);var t})))}function ln(e){return Array.isArray(e)?e:[e]}var cn=class{constructor(e,t,n=!1){this.provider=e,this.baseUrl=t,this.clientSideRoutingOnly=n}makeBaseUrl(e){return"provider-key"!==e.authMethod?`${nn}/${this.provider}`:this.baseUrl}makeBody(e){return"data"in e.args&&e.args.data?e.args.data:JSON.stringify(this.preparePayload(e))}makeUrl(e){return`${this.makeBaseUrl(e)}/${this.makeRoute(e).replace(/^\/+/,"")}`}prepareHeaders(e,t){const n={Authorization:`Bearer ${e.accessToken}`};return t||(n["Content-Type"]="application/json"),n}},dn=class extends cn{constructor(e,t,n=!1){super(e,t,n)}makeRoute(){return"v1/chat/completions"}preparePayload(e){return{...e.args,model:e.model}}async getResponse(e){if("object"==typeof e&&Array.isArray(null==e?void 0:e.choices)&&"number"==typeof(null==e?void 0:e.created)&&"string"==typeof(null==e?void 0:e.id)&&"string"==typeof(null==e?void 0:e.model)&&(void 0===e.system_fingerprint||null===e.system_fingerprint||"string"==typeof e.system_fingerprint)&&"object"==typeof(null==e?void 0:e.usage))return e;throw new on("Expected ChatCompletionOutput")}},pn=class extends cn{constructor(e,t,n=!1){super(e,t,n)}preparePayload(e){return{...e.args,model:e.model}}makeRoute(){return"v1/completions"}async getResponse(e){const t=ln(e);if(Array.isArray(t)&&t.length>0&&t.every((e=>"object"==typeof e&&!!e&&"generated_text"in e&&"string"==typeof e.generated_text)))return t[0];throw new on("Expected Array<{generated_text: string}>")}};function un(e){return/^http(s?):/.test(e)||e.startsWith("/")}var mn=["audio/mpeg","audio/mp4","audio/wav","audio/x-wav"],hn=class extends cn{constructor(e){super("fal-ai",e||"https://fal.run")}preparePayload(e){return e.args}makeRoute(e){return`/${e.model}`}prepareHeaders(e,t){const n={Authorization:"provider-key"!==e.authMethod?`Bearer ${e.accessToken}`:`Key ${e.accessToken}`};return t||(n["Content-Type"]="application/json"),n}},gn=class extends cn{constructor(){super("hf-inference",`${nn}/hf-inference`)}preparePayload(e){return e.args}makeUrl(e){return e.model.startsWith("http://")||e.model.startsWith("https://")?e.model:super.makeUrl(e)}makeRoute(e){return e.task&&["feature-extraction","sentence-similarity"].includes(e.task)?`pipeline/${e.task}/${e.model}`:`models/${e.model}`}async getResponse(e){return e}},fn=class extends gn{static validate(e){return"object"==typeof e&&!!e&&"aggregator"in e&&"string"==typeof e.aggregator&&"answer"in e&&"string"==typeof e.answer&&"cells"in e&&Array.isArray(e.cells)&&e.cells.every((e=>"string"==typeof e))&&"coordinates"in e&&Array.isArray(e.coordinates)&&e.coordinates.every((e=>Array.isArray(e)&&e.every((e=>"number"==typeof e))))}async getResponse(e){if(Array.isArray(e)&&Array.isArray(e)?e.every((e=>fn.validate(e))):fn.validate(e))return Array.isArray(e)?e[0]:e;throw new on("Expected {aggregator: string, answer: string, cells: string[], coordinates: number[][]}")}},bn="https://api.hyperbolic.xyz",yn="https://api.studio.nebius.ai",vn="https://api.novita.ai",wn=class extends cn{constructor(e){super("replicate",e||"https://api.replicate.com")}makeRoute(e){return e.model.includes(":")?"v1/predictions":`v1/models/${e.model}/predictions`}preparePayload(e){return{input:{...sn(e.args,["inputs","parameters"]),...e.args.parameters,prompt:e.args.inputs},version:e.model.includes(":")?e.model.split(":")[1]:void 0}}prepareHeaders(e,t){const n={Authorization:`Bearer ${e.accessToken}`,Prefer:"wait"};return t||(n["Content-Type"]="application/json"),n}makeUrl(e){const t=this.makeBaseUrl(e);return e.model.includes(":")?`${t}/v1/predictions`:`${t}/v1/models/${e.model}/predictions`}},xn="https://api.together.xyz",_n={"black-forest-labs":{"text-to-image":new class extends cn{constructor(){super("black-forest-labs","https://api.us1.bfl.ai")}preparePayload(e){return{...sn(e.args,["inputs","parameters"]),...e.args.parameters,prompt:e.args.inputs}}prepareHeaders(e,t){const n={Authorization:"provider-key"!==e.authMethod?`Bearer ${e.accessToken}`:`X-Key ${e.accessToken}`};return t||(n["Content-Type"]="application/json"),n}makeRoute(e){if(!e)throw new Error("Params are required");return`/v1/${e.model}`}async getResponse(e,t,n,i){const a=new URL(e.polling_url);for(let o=0;o<5;o++){await rn(1e3),a.searchParams.set("attempt",o.toString(10));const e=await fetch(a,{headers:{"Content-Type":"application/json"}});if(!e.ok)throw new on("Failed to fetch result from black forest labs API");const t=await e.json();if("object"==typeof t&&t&&"status"in t&&"string"==typeof t.status&&"Ready"===t.status&&"result"in t&&"object"==typeof t.result&&t.result&&"sample"in t.result&&"string"==typeof t.result.sample){if("url"===i)return t.result.sample;const e=await fetch(t.result.sample);return await e.blob()}}throw new on("Failed to fetch result from black forest labs API")}}},cerebras:{conversational:new class extends dn{constructor(){super("cerebras","https://api.cerebras.ai")}}},cohere:{conversational:new class extends dn{constructor(){super("cohere","https://api.cohere.com")}makeRoute(){return"/compatibility/v1/chat/completions"}}},"fal-ai":{"text-to-image":new class extends hn{preparePayload(e){return{...sn(e.args,["inputs","parameters"]),...e.args.parameters,sync_mode:!0,prompt:e.args.inputs}}async getResponse(e,t){if("object"==typeof e&&"images"in e&&Array.isArray(e.images)&&e.images.length>0&&"url"in e.images[0]&&"string"==typeof e.images[0].url){if("url"===t)return e.images[0].url;const n=await fetch(e.images[0].url);return await n.blob()}throw new on("Expected Fal.ai text-to-image response format")}},"text-to-speech":new class extends hn{preparePayload(e){return{...sn(e.args,["inputs","parameters"]),...e.args.parameters,lyrics:e.args.inputs}}async getResponse(e){var t;const n=e;if("string"!=typeof(null==(t=null==n?void 0:n.audio)?void 0:t.url))throw new on(`Expected { audio: { url: string } } format from Fal.ai Text-to-Speech, got: ${JSON.stringify(e)}`);try{const e=await fetch(n.audio.url);if(!e.ok)throw new Error(`Failed to fetch audio from ${n.audio.url}: ${e.statusText}`);return await e.blob()}catch(i){throw new on(`Error fetching or processing audio from Fal.ai Text-to-Speech URL: ${n.audio.url}. ${i instanceof Error?i.message:String(i)}`)}}},"text-to-video":new class extends hn{constructor(){super("https://queue.fal.run")}makeRoute(e){return"provider-key"!==e.authMethod?`/${e.model}?_subdomain=queue`:`/${e.model}`}preparePayload(e){return{...sn(e.args,["inputs","parameters"]),...e.args.parameters,prompt:e.args.inputs}}async getResponse(e,t,n){if(!t||!n)throw new on("URL and headers are required for text-to-video task");if(!e.request_id)throw new on("No request ID found in the response");let i=e.status;const a=new URL(t),o=`${a.protocol}//${a.host}${"router.huggingface.co"===a.host?"/fal-ai":""}`,r=new URL(e.response_url).pathname,s=a.search,l=`${o}${r}/status${s}`,c=`${o}${r}${s}`;for(;"COMPLETED"!==i;){await rn(500);const e=await fetch(l,{headers:n});if(!e.ok)throw new on("Failed to fetch response status from fal-ai API");try{i=(await e.json()).status}catch(u){throw new on("Failed to parse status response from fal-ai API")}}const d=await fetch(c,{headers:n});let p;try{p=await d.json()}catch(u){throw new on("Failed to parse result response from fal-ai API")}if("object"==typeof p&&p&&"video"in p&&"object"==typeof p.video&&p.video&&"url"in p.video&&"string"==typeof p.video.url&&un(p.video.url)){const e=await fetch(p.video.url);return await e.blob()}throw new on("Expected { video: { url: string } } result format, got instead: "+JSON.stringify(p))}},"automatic-speech-recognition":new class extends hn{prepareHeaders(e,t){const n=super.prepareHeaders(e,t);return n["Content-Type"]="application/json",n}async getResponse(e){const t=e;if("string"!=typeof(null==t?void 0:t.text))throw new on(`Expected { text: string } format from Fal.ai Automatic Speech Recognition, got: ${JSON.stringify(e)}`);return{text:t.text}}}},"hf-inference":{"text-to-image":new class extends gn{async getResponse(e,t,n,i){if(!e)throw new on("response is undefined");if("object"==typeof e){if("data"in e&&Array.isArray(e.data)&&e.data[0].b64_json){const t=e.data[0].b64_json;if("url"===i)return`data:image/jpeg;base64,${t}`;const n=await fetch(`data:image/jpeg;base64,${t}`);return await n.blob()}if("output"in e&&Array.isArray(e.output)){if("url"===i)return e.output[0];const t=await fetch(e.output[0]);return await t.blob()}}if(e instanceof Blob){if("url"===i){return`data:image/jpeg;base64,${await e.arrayBuffer().then((e=>Buffer.from(e).toString("base64")))}`}return e}throw new on("Expected a Blob ")}},conversational:new class extends gn{makeUrl(e){let t;return t=e.model.startsWith("http://")||e.model.startsWith("https://")?e.model.trim():`${this.makeBaseUrl(e)}/models/${e.model}`,t=t.replace(/\/+$/,""),t.endsWith("/v1")?t+="/chat/completions":t.endsWith("/chat/completions")||(t+="/v1/chat/completions"),t}preparePayload(e){return{...e.args,model:e.model}}async getResponse(e){return e}},"text-generation":new class extends gn{async getResponse(e){const t=ln(e);if(Array.isArray(t)&&t.every((e=>"generated_text"in e&&"string"==typeof(null==e?void 0:e.generated_text))))return null==t?void 0:t[0];throw new on("Expected Array<{generated_text: string}>")}},"text-classification":new class extends gn{async getResponse(e){const t=null==e?void 0:e[0];if(Array.isArray(t)&&t.every((e=>"string"==typeof(null==e?void 0:e.label)&&"number"==typeof e.score)))return t;throw new on("Expected Array<{label: string, score: number}>")}},"question-answering":new class extends gn{async getResponse(e){if(Array.isArray(e)?e.every((e=>"object"==typeof e&&!!e&&"string"==typeof e.answer&&"number"==typeof e.end&&"number"==typeof e.score&&"number"==typeof e.start)):"object"==typeof e&&e&&"string"==typeof e.answer&&"number"==typeof e.end&&"number"==typeof e.score&&"number"==typeof e.start)return Array.isArray(e)?e[0]:e;throw new on("Expected Array<{answer: string, end: number, score: number, start: number}>")}},"audio-classification":new class extends gn{async getResponse(e){if(Array.isArray(e)&&e.every((e=>"object"==typeof e&&null!==e&&"string"==typeof e.label&&"number"==typeof e.score)))return e;throw new on("Expected Array<{label: string, score: number}> but received different format")}},"automatic-speech-recognition":new class extends gn{async getResponse(e){return e}},"fill-mask":new class extends gn{async getResponse(e){if(Array.isArray(e)&&e.every((e=>"number"==typeof e.score&&"string"==typeof e.sequence&&"number"==typeof e.token&&"string"==typeof e.token_str)))return e;throw new on("Expected Array<{score: number, sequence: string, token: number, token_str: string}>")}},"feature-extraction":new class extends gn{async getResponse(e){const t=(e,n,i=0)=>!(i>n)&&(e.every((e=>Array.isArray(e)))?e.every((e=>t(e,n,i+1))):e.every((e=>"number"==typeof e)));if(Array.isArray(e)&&t(e,3,0))return e;throw new on("Expected Array<number[][][] | number[][] | number[] | number>")}},"image-classification":new class extends gn{async getResponse(e){if(Array.isArray(e)&&e.every((e=>"string"==typeof e.label&&"number"==typeof e.score)))return e;throw new on("Expected Array<{label: string, score: number}>")}},"image-segmentation":new class extends gn{async getResponse(e){if(Array.isArray(e)&&e.every((e=>"string"==typeof e.label&&"string"==typeof e.mask&&"number"==typeof e.score)))return e;throw new on("Expected Array<{label: string, mask: string, score: number}>")}},"document-question-answering":new class extends gn{async getResponse(e){if(Array.isArray(e)&&e.every((e=>!("object"!=typeof e||!e||"string"!=typeof(null==e?void 0:e.answer)||"number"!=typeof e.end&&void 0!==e.end||"number"!=typeof e.score&&void 0!==e.score||"number"!=typeof e.start&&void 0!==e.start))))return e[0];throw new on("Expected Array<{answer: string, end: number, score: number, start: number}>")}},"image-to-text":new class extends gn{async getResponse(e){if("string"!=typeof(null==e?void 0:e.generated_text))throw new on("Expected {generated_text: string}");return e}},"object-detection":new class extends gn{async getResponse(e){if(Array.isArray(e)&&e.every((e=>"string"==typeof e.label&&"number"==typeof e.score&&"number"==typeof e.box.xmin&&"number"==typeof e.box.ymin&&"number"==typeof e.box.xmax&&"number"==typeof e.box.ymax)))return e;throw new on("Expected Array<{label: string, score: number, box: {xmin: number, ymin: number, xmax: number, ymax: number}}>")}},"audio-to-audio":new class extends gn{async getResponse(e){if(!Array.isArray(e))throw new on("Expected Array");if(!e.every((e=>"object"==typeof e&&e&&"label"in e&&"string"==typeof e.label&&"content-type"in e&&"string"==typeof e["content-type"]&&"blob"in e&&"string"==typeof e.blob)))throw new on("Expected Array<{label: string, audio: Blob}>");return e}},"zero-shot-image-classification":new class extends gn{async getResponse(e){if(Array.isArray(e)&&e.every((e=>"string"==typeof e.label&&"number"==typeof e.score)))return e;throw new on("Expected Array<{label: string, score: number}>")}},"zero-shot-classification":new class extends gn{async getResponse(e){if(Array.isArray(e)&&e.every((e=>Array.isArray(e.labels)&&e.labels.every((e=>"string"==typeof e))&&Array.isArray(e.scores)&&e.scores.every((e=>"number"==typeof e))&&"string"==typeof e.sequence)))return e;throw new on("Expected Array<{labels: string[], scores: number[], sequence: string}>")}},"image-to-image":new class extends gn{async getResponse(e){if(e instanceof Blob)return e;throw new on("Expected Blob")}},"sentence-similarity":new class extends gn{async getResponse(e){if(Array.isArray(e)&&e.every((e=>"number"==typeof e)))return e;throw new on("Expected Array<number>")}},"table-question-answering":new fn,"tabular-classification":new class extends gn{async getResponse(e){if(Array.isArray(e)&&e.every((e=>"number"==typeof e)))return e;throw new on("Expected Array<number>")}},"text-to-speech":new class extends gn{async getResponse(e){return e}},"token-classification":new class extends gn{async getResponse(e){if(Array.isArray(e)&&e.every((e=>"number"==typeof e.end&&"string"==typeof e.entity_group&&"number"==typeof e.score&&"number"==typeof e.start&&"string"==typeof e.word)))return e;throw new on("Expected Array<{end: number, entity_group: string, score: number, start: number, word: string}>")}},translation:new class extends gn{async getResponse(e){if(Array.isArray(e)&&e.every((e=>"string"==typeof(null==e?void 0:e.translation_text))))return 1===(null==e?void 0:e.length)?null==e?void 0:e[0]:e;throw new on("Expected Array<{translation_text: string}>")}},summarization:new class extends gn{async getResponse(e){if(Array.isArray(e)&&e.every((e=>"string"==typeof(null==e?void 0:e.summary_text))))return null==e?void 0:e[0];throw new on("Expected Array<{summary_text: string}>")}},"visual-question-answering":new class extends gn{async getResponse(e){if(Array.isArray(e)&&e.every((e=>"object"==typeof e&&!!e&&"string"==typeof(null==e?void 0:e.answer)&&"number"==typeof e.score)))return e[0];throw new on("Expected Array<{answer: string, score: number}>")}},"tabular-regression":new class extends gn{async getResponse(e){if(Array.isArray(e)&&e.every((e=>"number"==typeof e)))return e;throw new on("Expected Array<number>")}},"text-to-audio":new class extends gn{async getResponse(e){return e}}},"fireworks-ai":{conversational:new class extends dn{constructor(){super("fireworks-ai","https://api.fireworks.ai")}makeRoute(){return"/inference/v1/chat/completions"}}},hyperbolic:{"text-to-image":new class extends cn{constructor(){super("hyperbolic",bn)}makeRoute(e){return"/v1/images/generations"}preparePayload(e){return{...sn(e.args,["inputs","parameters"]),...e.args.parameters,prompt:e.args.inputs,model_name:e.model}}async getResponse(e,t,n,i){if("object"==typeof e&&"images"in e&&Array.isArray(e.images)&&e.images[0]&&"string"==typeof e.images[0].image)return"url"===i?`data:image/jpeg;base64,${e.images[0].image}`:fetch(`data:image/jpeg;base64,${e.images[0].image}`).then((e=>e.blob()));throw new on("Expected Hyperbolic text-to-image response format")}},conversational:new class extends dn{constructor(){super("hyperbolic",bn)}},"text-generation":new class extends pn{constructor(){super("hyperbolic",bn)}makeRoute(){return"v1/chat/completions"}preparePayload(e){return{messages:[{content:e.args.inputs,role:"user"}],...e.args.parameters?{max_tokens:e.args.parameters.max_new_tokens,...sn(e.args.parameters,"max_new_tokens")}:void 0,...sn(e.args,["inputs","parameters"]),model:e.model}}async getResponse(e){if("object"==typeof e&&"choices"in e&&Array.isArray(null==e?void 0:e.choices)&&"string"==typeof(null==e?void 0:e.model)){return{generated_text:e.choices[0].message.content}}throw new on("Expected Hyperbolic text generation response format")}}},nebius:{"text-to-image":new class extends cn{constructor(){super("nebius",yn)}preparePayload(e){return{...sn(e.args,["inputs","parameters"]),...e.args.parameters,response_format:"b64_json",prompt:e.args.inputs,model:e.model}}makeRoute(e){return"v1/images/generations"}async getResponse(e,t,n,i){if("object"==typeof e&&"data"in e&&Array.isArray(e.data)&&e.data.length>0&&"b64_json"in e.data[0]&&"string"==typeof e.data[0].b64_json){const t=e.data[0].b64_json;return"url"===i?`data:image/jpeg;base64,${t}`:fetch(`data:image/jpeg;base64,${t}`).then((e=>e.blob()))}throw new on("Expected Nebius text-to-image response format")}},conversational:new class extends dn{constructor(){super("nebius",yn)}},"text-generation":new class extends pn{constructor(){super("nebius",yn)}}},novita:{conversational:new class extends dn{constructor(){super("novita",vn)}makeRoute(){return"/v3/openai/chat/completions"}},"text-generation":new class extends pn{constructor(){super("novita",vn)}makeRoute(){return"/v3/openai/chat/completions"}}},openai:{conversational:new class extends dn{constructor(){super("openai","https://api.openai.com",!0)}}},replicate:{"text-to-image":new class extends wn{async getResponse(e,t,n,i){if("object"==typeof e&&"output"in e&&Array.isArray(e.output)&&e.output.length>0&&"string"==typeof e.output[0]){if("url"===i)return e.output[0];const t=await fetch(e.output[0]);return await t.blob()}throw new on("Expected Replicate text-to-image response format")}},"text-to-speech":new class extends wn{preparePayload(e){const t=super.preparePayload(e),n=t.input;if("object"==typeof n&&null!==n&&"prompt"in n){const e=n;e.text=e.prompt,delete e.prompt}return t}async getResponse(e){if(e instanceof Blob)return e;if(e&&"object"==typeof e&&"output"in e){if("string"==typeof e.output){const t=await fetch(e.output);return await t.blob()}if(Array.isArray(e.output)){const t=await fetch(e.output[0]);return await t.blob()}}throw new on("Expected Blob or object with output")}},"text-to-video":new class extends wn{async getResponse(e){if("object"==typeof e&&e&&"output"in e&&"string"==typeof e.output&&un(e.output)){const t=await fetch(e.output);return await t.blob()}throw new on("Expected { output: string }")}}},sambanova:{conversational:new class extends dn{constructor(){super("sambanova","https://api.sambanova.ai")}}},together:{"text-to-image":new class extends cn{constructor(){super("together",xn)}makeRoute(){return"v1/images/generations"}preparePayload(e){return{...sn(e.args,["inputs","parameters"]),...e.args.parameters,prompt:e.args.inputs,response_format:"base64",model:e.model}}async getResponse(e,t){if("object"==typeof e&&"data"in e&&Array.isArray(e.data)&&e.data.length>0&&"b64_json"in e.data[0]&&"string"==typeof e.data[0].b64_json){const n=e.data[0].b64_json;return"url"===t?`data:image/jpeg;base64,${n}`:fetch(`data:image/jpeg;base64,${n}`).then((e=>e.blob()))}throw new on("Expected Together text-to-image response format")}},conversational:new class extends dn{constructor(){super("together",xn)}},"text-generation":new class extends pn{constructor(){super("together",xn)}preparePayload(e){return{model:e.model,...e.args,prompt:e.args.inputs}}async getResponse(e){if("object"==typeof e&&"choices"in e&&Array.isArray(null==e?void 0:e.choices)&&"string"==typeof(null==e?void 0:e.model)){return{generated_text:e.choices[0].text}}throw new on("Expected Together text generation response format")}}}};function kn(e,t){if("hf-inference"===e&&!t)return new gn;if(!t)throw new Error("you need to provide a task name when using an external provider, e.g. 'text-to-image'");if(!(e in _n))throw new Error(`Provider '${e}' not supported. Available providers: ${Object.keys(_n)}`);const n=_n[e];if(!n||!(t in n))throw new Error(`Task '${t}' not supported for provider '${e}'. Available tasks: ${Object.keys(n??{})}`);return n[t]}var An={"black-forest-labs":{},cerebras:{},cohere:{},"fal-ai":{},"fireworks-ai":{},"hf-inference":{},hyperbolic:{},nebius:{},novita:{},openai:{},replicate:{},sambanova:{},together:{}},Sn=new Map;var In=null;async function Tn(e,t){const{provider:n,model:i}=e,a=n??"hf-inference",{task:o}=t??{};if(e.endpointUrl&&"hf-inference"!==a)throw new Error("Cannot use endpointUrl with a third-party provider.");if(i&&un(i))throw new Error("Model URLs are no longer supported. Use endpointUrl instead.");if(!i&&!o)throw new Error("No model provided, and no task has been specified.");const r=i??await async function(e){In||(In=await async function(){const e=await fetch(`${tn}/api/tasks`);if(!e.ok)throw new Error("Failed to load tasks definitions from Hugging Face Hub.");return await e.json()}());const t=In[e];if(((null==t?void 0:t.models.length)??0)<=0)throw new Error(`No default model defined for task ${e}, please define the model explicitly.`);return t.models[0].id}(o),s=kn(a,o);if(s.clientSideRoutingOnly&&!i)throw new Error(`Provider ${a} requires a model ID to be passed directly.`);const l=s.clientSideRoutingOnly?function(e,t){if(!e.startsWith(`${t}/`))throw new Error(`Models from ${t} must be prefixed by "${t}/". Got "${e}".`);return e.slice(t.length+1)}(i,a):await async function(e,t,n={}){var i,a;if("hf-inference"===e.provider)return e.model;if(!n.task)throw new Error("task must be specified when using a third-party provider");const o="text-generation"===n.task&&n.chatCompletion?"conversational":n.task;if(null==(i=An[e.provider])?void 0:i[e.model])return An[e.provider][e.model];let r;if(r=Sn.has(e.model)?Sn.get(e.model):await((null==n?void 0:n.fetch)??fetch)(`${tn}/api/models/${e.model}?expand[]=inferenceProviderMapping`,{headers:(null==(a=t.accessToken)?void 0:a.startsWith("hf_"))?{Authorization:`Bearer ${t.accessToken}`}:{}}).then((e=>e.json())).then((e=>e.inferenceProviderMapping)).catch((()=>null)),!r)throw new Error(`We have not been able to find inference provider information for model ${e.model}.`);const s=r[e.provider];if(s){if(s.task!==o)throw new Error(`Model ${e.model} is not supported for task ${o} and provider ${e.provider}. Supported task: ${s.task}.`);return s.status,s.providerId}throw new Error(`Model ${e.model} is not supported provider ${e.provider}.`)}({model:r,provider:a},e,{task:o,fetch:null==t?void 0:t.fetch});return En(l,e,t)}function En(e,t,n){const{accessToken:i,endpointUrl:a,provider:o,model:r,...s}=t,l=o??"hf-inference",{includeCredentials:c,task:d,signal:p,billTo:u}=n??{},m=kn(l,d),h=(()=>{if(m.clientSideRoutingOnly){if(i&&i.startsWith("hf_"))throw new Error(`Provider ${l} is closed-source and does not support HF tokens.`);return"provider-key"}return i?i.startsWith("hf_")?"hf-token":"provider-key":"include"===c?"credentials-include":"none"})(),g=a??e,f=m.makeUrl({authMethod:h,model:g,task:d}),b=m.prepareHeaders({accessToken:i,authMethod:h},"data"in t&&!!t.data);u&&(b[an]=u);const y=[`${Zt}/${en}`,"undefined"!=typeof navigator?navigator.userAgent:void 0].filter((e=>void 0!==e)).join(" ");b["User-Agent"]=y;const v=m.makeBody({args:s,model:e,task:d});let w;"string"==typeof c?w=c:!0===c&&(w="include");return{url:f,info:{headers:b,method:"POST",body:v,...w?{credentials:w}:void 0,signal:p}}}function Cn(e){let t,n,i,a=!1;return function(o){void 0===t?(t=o,n=0,i=-1):t=function(e,t){const n=new Uint8Array(e.length+t.length);return n.set(e),n.set(t,e.length),n}(t,o);const r=t.length;let s=0;for(;n<r;){a&&(10===t[n]&&(s=++n),a=!1);let o=-1;for(;n<r&&-1===o;++n)switch(t[n]){case 58:-1===i&&(i=n-s);break;case 13:a=!0;case 10:o=n}if(-1===o)break;e(t.subarray(s,o),i),s=n,i=-1}s===r?t=void 0:0!==s&&(t=t.subarray(s),n-=s)}}async function Mn(e,t){var n;const{url:i,info:a}=await Tn(e,t),o=await((null==t?void 0:t.fetch)??fetch)(i,a),r={url:i,info:a};if(!1!==(null==t?void 0:t.retry_on_error)&&503===o.status)return Mn(e,t);if(!o.ok){const n=o.headers.get("Content-Type");if(["application/json","application/problem+json"].some((e=>null==n?void 0:n.startsWith(e)))){const n=await o.json();if([400,422,404,500].includes(o.status)&&(null==t?void 0:t.chatCompletion))throw new Error(`Server ${e.model} does not seem to support chat completion. Error: ${JSON.stringify(n.error)}`);throw n.error||n.detail?new Error(JSON.stringify(n.error??n.detail)):new Error(n)}const i=(null==n?void 0:n.startsWith("text/plain;"))?await o.text():void 0;throw new Error(i??"An error occurred while fetching the blob")}if(null==(n=o.headers.get("Content-Type"))?void 0:n.startsWith("application/json")){return{data:await o.json(),requestContext:r}}return{data:await o.blob(),requestContext:r}}async function*Ln(e,t){var n,i;const{url:a,info:o}=await Tn({...e,stream:!0},t),r=await((null==t?void 0:t.fetch)??fetch)(a,o);if(!1!==(null==t?void 0:t.retry_on_error)&&503===r.status)return yield*Ln(e,t);if(!r.ok){if(null==(n=r.headers.get("Content-Type"))?void 0:n.startsWith("application/json")){const n=await r.json();if([400,422,404,500].includes(r.status)&&(null==t?void 0:t.chatCompletion))throw new Error(`Server ${e.model} does not seem to support chat completion. Error: ${n.error}`);if("string"==typeof n.error)throw new Error(n.error);if(n.error&&"message"in n.error&&"string"==typeof n.error.message)throw new Error(n.error.message);if("string"==typeof n.message)throw new Error(n.message)}throw new Error(`Server response contains error: ${r.status}`)}if(!(null==(i=r.headers.get("content-type"))?void 0:i.startsWith("text/event-stream")))throw new Error("Server does not support event stream content type, it returned "+r.headers.get("content-type"));if(!r.body)return;const s=r.body.getReader();let l=[];const c=Cn(function(e,t,n){let i={data:"",event:"",id:"",retry:void 0};const a=new TextDecoder;return function(o,r){if(0===o.length)null==n||n(i),i={data:"",event:"",id:"",retry:void 0};else if(r>0){const n=a.decode(o.subarray(0,r)),s=r+(32===o[r+1]?2:1),l=a.decode(o.subarray(s));switch(n){case"data":i.data=i.data?i.data+"\n"+l:l;break;case"event":i.event=l;break;case"id":e(i.id=l);break;case"retry":const n=parseInt(l,10);isNaN(n)||t(i.retry=n)}}}}((()=>{}),(()=>{}),(e=>{l.push(e)})));try{for(;;){const{done:e,value:t}=await s.read();if(e)return;c(t);for(const n of l)if(n.data.length>0){if("[DONE]"===n.data)return;const e=JSON.parse(n.data);if("object"==typeof e&&null!==e&&"error"in e){const t="string"==typeof e.error?e.error:"object"==typeof e.error&&e.error&&"message"in e.error&&"string"==typeof e.error.message?e.error.message:JSON.stringify(e.error);throw new Error("Error forwarded from backend: "+t)}yield e}l=[]}}finally{s.releaseLock()}}async function jn(e,t){return(await Mn(e,t)).data}async function*Rn(e,t){yield*Ln(e,t)}function Un(e){return"data"in e?e:{...sn(e,"inputs"),data:e.inputs}}async function Dn(e,t){const n=kn(e.provider??"hf-inference","audio-classification"),i=Un(e),{data:a}=await Mn(i,{...t,task:"audio-classification"});return n.getResponse(a)}async function Pn(e,t){const n=kn(e.provider??"hf-inference","audio-to-audio"),i=Un(e),{data:a}=await Mn(i,{...t,task:"audio-to-audio"});return n.getResponse(a)}function qn(e){if(globalThis.Buffer)return globalThis.Buffer.from(e).toString("base64");{const t=[];return e.forEach((e=>{t.push(String.fromCharCode(e))})),globalThis.btoa(t.join(""))}}async function Nn(e,t){const n=kn(e.provider??"hf-inference","automatic-speech-recognition"),i=await async function(e){if("fal-ai"===e.provider){const t="data"in e&&e.data instanceof Blob?e.data:"inputs"in e?e.inputs:void 0,n=null==t?void 0:t.type;if(!n)throw new Error("Unable to determine the input's content-type. Make sure your are passing a Blob when using provider fal-ai.");if(!mn.includes(n))throw new Error(`Provider fal-ai does not support blob type ${n} - supported content types are: ${mn.join(", ")}`);const i=qn(new Uint8Array(await t.arrayBuffer()));return{...sn(e,"data"in e?"data":"inputs"),audio_url:`data:${n};base64,${i}`}}return Un(e)}(e),{data:a}=await Mn(i,{...t,task:"automatic-speech-recognition"});if(!("string"==typeof(null==a?void 0:a.text)))throw new on("Expected {text: string}");return n.getResponse(a)}async function $n(e,t){const n=kn(e.provider??"hf-inference","text-to-speech"),{data:i}=await Mn(e,{...t,task:"text-to-speech"});return n.getResponse(i)}function On(e){return"data"in e?e:{...sn(e,"inputs"),data:e.inputs}}async function zn(e,t){const n=kn(e.provider??"hf-inference","image-classification"),i=On(e),{data:a}=await Mn(i,{...t,task:"image-classification"});return n.getResponse(a)}async function Bn(e,t){const n=kn(e.provider??"hf-inference","image-segmentation"),i=On(e),{data:a}=await Mn(i,{...t,task:"image-segmentation"});return n.getResponse(a)}async function Fn(e,t){const n=kn(e.provider??"hf-inference","image-to-image");let i;i=e.parameters?{...e,inputs:qn(new Uint8Array(e.inputs instanceof ArrayBuffer?e.inputs:await e.inputs.arrayBuffer()))}:{accessToken:e.accessToken,model:e.model,data:e.inputs};const{data:a}=await Mn(i,{...t,task:"image-to-image"});return n.getResponse(a)}async function Vn(e,t){const n=kn(e.provider??"hf-inference","image-to-text"),i=On(e),{data:a}=await Mn(i,{...t,task:"image-to-text"});return n.getResponse(a[0])}async function Hn(e,t){const n=kn(e.provider??"hf-inference","object-detection"),i=On(e),{data:a}=await Mn(i,{...t,task:"object-detection"});return n.getResponse(a)}async function Gn(e,t){const n=kn(e.provider??"hf-inference","text-to-image"),{data:i}=await Mn(e,{...t,task:"text-to-image"}),{url:a,info:o}=await Tn(e,{...t,task:"text-to-image"});return n.getResponse(i,a,o.headers,null==t?void 0:t.outputType)}async function Qn(e,t){const n=kn(e.provider??"hf-inference","text-to-video"),{data:i}=await Mn(e,{...t,task:"text-to-video"}),{url:a,info:o}=await Tn(e,{...t,task:"text-to-video"});return n.getResponse(i,a,o.headers)}async function Wn(e,t){const n=kn(e.provider??"hf-inference","zero-shot-image-classification"),i=await async function(e){return e.inputs instanceof Blob?{...e,inputs:{image:qn(new Uint8Array(await e.inputs.arrayBuffer()))}}:{...e,inputs:{image:qn(new Uint8Array(e.inputs.image instanceof ArrayBuffer?e.inputs.image:await e.inputs.image.arrayBuffer()))}}}(e),{data:a}=await Mn(i,{...t,task:"zero-shot-image-classification"});return n.getResponse(a)}async function Kn(e,t){const n=kn(e.provider??"hf-inference","conversational"),{data:i}=await Mn(e,{...t,task:"conversational"});return n.getResponse(i)}async function*Jn(e,t){yield*Ln(e,{...t,task:"conversational"})}async function Xn(e,t){const n=kn(e.provider??"hf-inference","feature-extraction"),{data:i}=await Mn(e,{...t,task:"feature-extraction"});return n.getResponse(i)}async function Yn(e,t){const n=kn(e.provider??"hf-inference","fill-mask"),{data:i}=await Mn(e,{...t,task:"fill-mask"});return n.getResponse(i)}async function Zn(e,t){const n=kn(e.provider??"hf-inference","question-answering"),{data:i}=await Mn(e,{...t,task:"question-answering"});return n.getResponse(i)}async function ei(e,t){const n=kn(e.provider??"hf-inference","sentence-similarity"),{data:i}=await Mn(e,{...t,task:"sentence-similarity"});return n.getResponse(i)}async function ti(e,t){const n=kn(e.provider??"hf-inference","summarization"),{data:i}=await Mn(e,{...t,task:"summarization"});return n.getResponse(i)}async function ni(e,t){const n=kn(e.provider??"hf-inference","table-question-answering"),{data:i}=await Mn(e,{...t,task:"table-question-answering"});return n.getResponse(i)}async function ii(e,t){const n=kn(e.provider??"hf-inference","text-classification"),{data:i}=await Mn(e,{...t,task:"text-classification"});return n.getResponse(i)}async function ai(e,t){const n=kn(e.provider??"hf-inference","text-generation"),{data:i}=await Mn(e,{...t,task:"text-generation"});return n.getResponse(i)}async function*oi(e,t){yield*Ln(e,{...t,task:"text-generation"})}async function ri(e,t){const n=kn(e.provider??"hf-inference","token-classification"),{data:i}=await Mn(e,{...t,task:"token-classification"});return n.getResponse(i)}async function si(e,t){const n=kn(e.provider??"hf-inference","translation"),{data:i}=await Mn(e,{...t,task:"translation"});return n.getResponse(i)}async function li(e,t){const n=kn(e.provider??"hf-inference","zero-shot-classification"),{data:i}=await Mn(e,{...t,task:"zero-shot-classification"});return n.getResponse(i)}async function ci(e,t){const n=kn(e.provider??"hf-inference","document-question-answering"),i={...e,inputs:{question:e.inputs.question,image:qn(new Uint8Array(await e.inputs.image.arrayBuffer()))}},{data:a}=await Mn(i,{...t,task:"document-question-answering"});return n.getResponse(a)}async function di(e,t){const n=kn(e.provider??"hf-inference","visual-question-answering"),i={...e,inputs:{question:e.inputs.question,image:qn(new Uint8Array(await e.inputs.image.arrayBuffer()))}},{data:a}=await Mn(i,{...t,task:"visual-question-answering"});return n.getResponse(a)}async function pi(e,t){const n=kn(e.provider??"hf-inference","tabular-classification"),{data:i}=await Mn(e,{...t,task:"tabular-classification"});return n.getResponse(i)}async function ui(e,t){const n=kn(e.provider??"hf-inference","tabular-regression"),{data:i}=await Mn(e,{...t,task:"tabular-regression"});return n.getResponse(i)}var mi=class{constructor(e="",n={}){t(this,"accessToken"),t(this,"defaultOptions"),this.accessToken=e,this.defaultOptions=n;for(const[t,i]of Object.entries(Yt))Object.defineProperty(this,t,{enumerable:!1,value:(t,a)=>i({...t,accessToken:e},{...n,...a})})}endpoint(e){return new hi(e,this.accessToken,this.defaultOptions)}},hi=class{constructor(e,t="",n={}){for(const[i,a]of Object.entries(Yt))Object.defineProperty(this,i,{enumerable:!1,value:(i,o)=>a({...i,accessToken:t,endpointUrl:e},{...n,...o})})}},gi=class extends mi{};Xt({},{getInferenceSnippets:()=>Ii});var fi={js:{fetch:{basic:'async function query(data) {\n\tconst response = await fetch(\n\t\t"{{ fullUrl }}",\n\t\t{\n\t\t\theaders: {\n\t\t\t\tAuthorization: "{{ authorizationHeader }}",\n\t\t\t\t"Content-Type": "application/json",\n\t\t\t},\n\t\t\tmethod: "POST",\n\t\t\tbody: JSON.stringify(data),\n\t\t}\n\t);\n\tconst result = await response.json();\n\treturn result;\n}\n\nquery({ inputs: {{ providerInputs.asObj.inputs }} }).then((response) => {\n    console.log(JSON.stringify(response));\n});',basicAudio:'async function query(data) {\n\tconst response = await fetch(\n\t\t"{{ fullUrl }}",\n\t\t{\n\t\t\theaders: {\n\t\t\t\tAuthorization: "{{ authorizationHeader }}",\n\t\t\t\t"Content-Type": "audio/flac"\n\t\t\t},\n\t\t\tmethod: "POST",\n\t\t\tbody: JSON.stringify(data),\n\t\t}\n\t);\n\tconst result = await response.json();\n\treturn result;\n}\n\nquery({ inputs: {{ providerInputs.asObj.inputs }} }).then((response) => {\n    console.log(JSON.stringify(response));\n});',basicImage:'async function query(data) {\n\tconst response = await fetch(\n\t\t"{{ fullUrl }}",\n\t\t{\n\t\t\theaders: {\n\t\t\t\tAuthorization: "{{ authorizationHeader }}",\n\t\t\t\t"Content-Type": "image/jpeg"\n\t\t\t},\n\t\t\tmethod: "POST",\n\t\t\tbody: JSON.stringify(data),\n\t\t}\n\t);\n\tconst result = await response.json();\n\treturn result;\n}\n\nquery({ inputs: {{ providerInputs.asObj.inputs }} }).then((response) => {\n    console.log(JSON.stringify(response));\n});',textToAudio:'{% if model.library_name == "transformers" %}\nasync function query(data) {\n\tconst response = await fetch(\n\t\t"{{ fullUrl }}",\n\t\t{\n\t\t\theaders: {\n\t\t\t\tAuthorization: "{{ authorizationHeader }}",\n\t\t\t\t"Content-Type": "application/json",\n\t\t\t},\n\t\t\tmethod: "POST",\n\t\t\tbody: JSON.stringify(data),\n\t\t}\n\t);\n\tconst result = await response.blob();\n    return result;\n}\n\nquery({ inputs: {{ providerInputs.asObj.inputs }} }).then((response) => {\n    // Returns a byte object of the Audio wavform. Use it directly!\n});\n{% else %}\nasync function query(data) {\n\tconst response = await fetch(\n\t\t"{{ fullUrl }}",\n\t\t{\n\t\t\theaders: {\n\t\t\t\tAuthorization: "{{ authorizationHeader }}",\n\t\t\t\t"Content-Type": "application/json",\n\t\t\t},\n\t\t\tmethod: "POST",\n\t\t\tbody: JSON.stringify(data),\n\t\t}\n\t);\n    const result = await response.json();\n    return result;\n}\n\nquery({ inputs: {{ providerInputs.asObj.inputs }} }).then((response) => {\n    console.log(JSON.stringify(response));\n});\n{% endif %} ',textToImage:'async function query(data) {\n\tconst response = await fetch(\n\t\t"{{ fullUrl }}",\n\t\t{\n\t\t\theaders: {\n\t\t\t\tAuthorization: "{{ authorizationHeader }}",\n\t\t\t\t"Content-Type": "application/json",\n\t\t\t},\n\t\t\tmethod: "POST",\n\t\t\tbody: JSON.stringify(data),\n\t\t}\n\t);\n\tconst result = await response.blob();\n\treturn result;\n}\n\n\nquery({ {{ providerInputs.asTsString }} }).then((response) => {\n    // Use image\n});',zeroShotClassification:'async function query(data) {\n    const response = await fetch(\n\t\t"{{ fullUrl }}",\n        {\n            headers: {\n\t\t\t\tAuthorization: "{{ authorizationHeader }}",\n                "Content-Type": "application/json",\n            },\n            method: "POST",\n            body: JSON.stringify(data),\n        }\n    );\n    const result = await response.json();\n    return result;\n}\n\nquery({\n    inputs: {{ providerInputs.asObj.inputs }},\n    parameters: { candidate_labels: ["refund", "legal", "faq"] }\n}).then((response) => {\n    console.log(JSON.stringify(response));\n});'},"huggingface.js":{basic:'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst output = await client.{{ methodName }}({\n\tmodel: "{{ model.id }}",\n\tinputs: {{ inputs.asObj.inputs }},\n\tprovider: "{{ provider }}",\n});\n\nconsole.log(output);',basicAudio:'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst data = fs.readFileSync({{inputs.asObj.inputs}});\n\nconst output = await client.{{ methodName }}({\n\tdata,\n\tmodel: "{{ model.id }}",\n\tprovider: "{{ provider }}",\n});\n\nconsole.log(output);',basicImage:'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst data = fs.readFileSync({{inputs.asObj.inputs}});\n\nconst output = await client.{{ methodName }}({\n\tdata,\n\tmodel: "{{ model.id }}",\n\tprovider: "{{ provider }}",\n});\n\nconsole.log(output);',conversational:'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst chatCompletion = await client.chatCompletion({\n    provider: "{{ provider }}",\n    model: "{{ model.id }}",\n{{ inputs.asTsString }}\n});\n\nconsole.log(chatCompletion.choices[0].message);',conversationalStream:'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nlet out = "";\n\nconst stream = await client.chatCompletionStream({\n    provider: "{{ provider }}",\n    model: "{{ model.id }}",\n{{ inputs.asTsString }}\n});\n\nfor await (const chunk of stream) {\n\tif (chunk.choices && chunk.choices.length > 0) {\n\t\tconst newContent = chunk.choices[0].delta.content;\n\t\tout += newContent;\n\t\tconsole.log(newContent);\n\t}  \n}',textToImage:'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst image = await client.textToImage({\n    provider: "{{ provider }}",\n    model: "{{ model.id }}",\n\tinputs: {{ inputs.asObj.inputs }},\n\tparameters: { num_inference_steps: 5 },\n});\n/// Use the generated image (it\'s a Blob)',textToVideo:'import { InferenceClient } from "@huggingface/inference";\n\nconst client = new InferenceClient("{{ accessToken }}");\n\nconst image = await client.textToVideo({\n    provider: "{{ provider }}",\n    model: "{{ model.id }}",\n\tinputs: {{ inputs.asObj.inputs }},\n});\n// Use the generated video (it\'s a Blob)'},openai:{conversational:'import { OpenAI } from "openai";\n\nconst client = new OpenAI({\n\tbaseURL: "{{ baseUrl }}",\n\tapiKey: "{{ accessToken }}",\n});\n\nconst chatCompletion = await client.chat.completions.create({\n\tmodel: "{{ providerModelId }}",\n{{ inputs.asTsString }}\n});\n\nconsole.log(chatCompletion.choices[0].message);',conversationalStream:'import { OpenAI } from "openai";\n\nconst client = new OpenAI({\n\tbaseURL: "{{ baseUrl }}",\n\tapiKey: "{{ accessToken }}",\n});\n\nconst stream = await client.chat.completions.create({\n    model: "{{ providerModelId }}",\n{{ inputs.asTsString }}\n    stream: true,\n});\n\nfor await (const chunk of stream) {\n    process.stdout.write(chunk.choices[0]?.delta?.content || "");\n}'}},python:{fal_client:{textToImage:'{% if provider == "fal-ai" %}\nimport fal_client\n\nresult = fal_client.subscribe(\n    "{{ providerModelId }}",\n    arguments={\n        "prompt": {{ inputs.asObj.inputs }},\n    },\n)\nprint(result)\n{% endif %} '},huggingface_hub:{basic:'result = client.{{ methodName }}(\n    inputs={{ inputs.asObj.inputs }},\n    model="{{ model.id }}",\n)',basicAudio:'output = client.{{ methodName }}({{ inputs.asObj.inputs }}, model="{{ model.id }}")',basicImage:'output = client.{{ methodName }}({{ inputs.asObj.inputs }}, model="{{ model.id }}")',conversational:'completion = client.chat.completions.create(\n    model="{{ model.id }}",\n{{ inputs.asPythonString }}\n)\n\nprint(completion.choices[0].message) ',conversationalStream:'stream = client.chat.completions.create(\n    model="{{ model.id }}",\n{{ inputs.asPythonString }}\n    stream=True,\n)\n\nfor chunk in stream:\n    print(chunk.choices[0].delta.content, end="") ',documentQuestionAnswering:'output = client.document_question_answering(\n    "{{ inputs.asObj.image }}",\n    question="{{ inputs.asObj.question }}",\n    model="{{ model.id }}",\n) ',imageToImage:'# output is a PIL.Image object\nimage = client.image_to_image(\n    "{{ inputs.asObj.inputs }}",\n    prompt="{{ inputs.asObj.parameters.prompt }}",\n    model="{{ model.id }}",\n) ',importInferenceClient:'from huggingface_hub import InferenceClient\n\nclient = InferenceClient(\n    provider="{{ provider }}",\n    api_key="{{ accessToken }}",\n)',textToImage:'# output is a PIL.Image object\nimage = client.text_to_image(\n    {{ inputs.asObj.inputs }},\n    model="{{ model.id }}",\n) ',textToVideo:'video = client.text_to_video(\n    {{ inputs.asObj.inputs }},\n    model="{{ model.id }}",\n) '},openai:{conversational:'from openai import OpenAI\n\nclient = OpenAI(\n    base_url="{{ baseUrl }}",\n    api_key="{{ accessToken }}"\n)\n\ncompletion = client.chat.completions.create(\n    model="{{ providerModelId }}",\n{{ inputs.asPythonString }}\n)\n\nprint(completion.choices[0].message) ',conversationalStream:'from openai import OpenAI\n\nclient = OpenAI(\n    base_url="{{ baseUrl }}",\n    api_key="{{ accessToken }}"\n)\n\nstream = client.chat.completions.create(\n    model="{{ providerModelId }}",\n{{ inputs.asPythonString }}\n    stream=True,\n)\n\nfor chunk in stream:\n    print(chunk.choices[0].delta.content, end="")'},requests:{basic:'def query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\noutput = query({\n    "inputs": {{ providerInputs.asObj.inputs }},\n}) ',basicAudio:'def query(filename):\n    with open(filename, "rb") as f:\n        data = f.read()\n    response = requests.post(API_URL, headers={"Content-Type": "audio/flac", **headers}, data=data)\n    return response.json()\n\noutput = query({{ providerInputs.asObj.inputs }})',basicImage:'def query(filename):\n    with open(filename, "rb") as f:\n        data = f.read()\n    response = requests.post(API_URL, headers={"Content-Type": "image/jpeg", **headers}, data=data)\n    return response.json()\n\noutput = query({{ providerInputs.asObj.inputs }})',conversational:'def query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\nresponse = query({\n{{ providerInputs.asJsonString }}\n})\n\nprint(response["choices"][0]["message"])',conversationalStream:'def query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload, stream=True)\n    for line in response.iter_lines():\n        if not line.startswith(b"data:"):\n            continue\n        if line.strip() == b"data: [DONE]":\n            return\n        yield json.loads(line.decode("utf-8").lstrip("data:").rstrip("/n"))\n\nchunks = query({\n{{ providerInputs.asJsonString }},\n    "stream": True,\n})\n\nfor chunk in chunks:\n    print(chunk["choices"][0]["delta"]["content"], end="")',documentQuestionAnswering:'def query(payload):\n    with open(payload["image"], "rb") as f:\n        img = f.read()\n        payload["image"] = base64.b64encode(img).decode("utf-8")\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\noutput = query({\n    "inputs": {\n        "image": "{{ inputs.asObj.image }}",\n        "question": "{{ inputs.asObj.question }}",\n    },\n}) ',imageToImage:'def query(payload):\n    with open(payload["inputs"], "rb") as f:\n        img = f.read()\n        payload["inputs"] = base64.b64encode(img).decode("utf-8")\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.content\n\nimage_bytes = query({\n{{ providerInputs.asJsonString }}\n})\n\n# You can access the image with PIL.Image for example\nimport io\nfrom PIL import Image\nimage = Image.open(io.BytesIO(image_bytes)) ',importRequests:'{% if importBase64 %}\nimport base64\n{% endif %}\n{% if importJson %}\nimport json\n{% endif %}\nimport requests\n\nAPI_URL = "{{ fullUrl }}"\nheaders = {"Authorization": "{{ authorizationHeader }}"}',tabular:'def query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.content\n\nresponse = query({\n    "inputs": {\n        "data": {{ providerInputs.asObj.inputs }}\n    },\n}) ',textToAudio:'{% if model.library_name == "transformers" %}\ndef query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.content\n\naudio_bytes = query({\n    "inputs": {{ providerInputs.asObj.inputs }},\n})\n# You can access the audio with IPython.display for example\nfrom IPython.display import Audio\nAudio(audio_bytes)\n{% else %}\ndef query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\naudio, sampling_rate = query({\n    "inputs": {{ providerInputs.asObj.inputs }},\n})\n# You can access the audio with IPython.display for example\nfrom IPython.display import Audio\nAudio(audio, rate=sampling_rate)\n{% endif %} ',textToImage:'{% if provider == "hf-inference" %}\ndef query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.content\n\nimage_bytes = query({\n    "inputs": {{ providerInputs.asObj.inputs }},\n})\n\n# You can access the image with PIL.Image for example\nimport io\nfrom PIL import Image\nimage = Image.open(io.BytesIO(image_bytes))\n{% endif %}',zeroShotClassification:'def query(payload):\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\noutput = query({\n    "inputs": {{ providerInputs.asObj.inputs }},\n    "parameters": {"candidate_labels": ["refund", "legal", "faq"]},\n}) ',zeroShotImageClassification:'def query(data):\n    with open(data["image_path"], "rb") as f:\n        img = f.read()\n    payload={\n        "parameters": data["parameters"],\n        "inputs": base64.b64encode(img).decode("utf-8")\n    }\n    response = requests.post(API_URL, headers=headers, json=payload)\n    return response.json()\n\noutput = query({\n    "image_path": {{ providerInputs.asObj.inputs }},\n    "parameters": {"candidate_labels": ["cat", "dog", "llama"]},\n}) '}},sh:{curl:{basic:"curl {{ fullUrl }} \\\n    -X POST \\\n    -H 'Authorization: {{ authorizationHeader }}' \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n{{ providerInputs.asCurlString }}\n    }'",basicAudio:"curl {{ fullUrl }} \\\n    -X POST \\\n    -H 'Authorization: {{ authorizationHeader }}' \\\n    -H 'Content-Type: audio/flac' \\\n    --data-binary @{{ providerInputs.asObj.inputs }}",basicImage:"curl {{ fullUrl }} \\\n    -X POST \\\n    -H 'Authorization: {{ authorizationHeader }}' \\\n    -H 'Content-Type: image/jpeg' \\\n    --data-binary @{{ providerInputs.asObj.inputs }}",conversational:"curl {{ fullUrl }} \\\n    -H 'Authorization: {{ authorizationHeader }}' \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n{{ providerInputs.asCurlString }},\n        \"stream\": false\n    }'",conversationalStream:"curl {{ fullUrl }} \\\n    -H 'Authorization: {{ authorizationHeader }}' \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n{{ providerInputs.asCurlString }},\n        \"stream\": true\n    }'",zeroShotClassification:'curl {{ fullUrl }} \\\n    -X POST \\\n    -d \'{"inputs": {{ providerInputs.asObj.inputs }}, "parameters": {"candidate_labels": ["refund", "legal", "faq"]}}\' \\\n    -H \'Content-Type: application/json\' \\\n    -H \'Authorization: {{ authorizationHeader }}\''}}},bi={js:["fetch","huggingface.js","openai"],python:["huggingface_hub","fal_client","requests","openai"],sh:["curl"]},yi=(e,t,n)=>{var i,a;const o=null==(a=null==(i=fi[e])?void 0:i[t])?void 0:a[n];if(!o)throw new Error(`Template not found: ${e}/${t}/${n}`);return e=>new It(o).render({...e})},vi=yi("python","huggingface_hub","importInferenceClient"),wi=yi("python","requests","importRequests"),xi={"audio-classification":"audio_classification","audio-to-audio":"audio_to_audio","automatic-speech-recognition":"automatic_speech_recognition","document-question-answering":"document_question_answering","feature-extraction":"feature_extraction","fill-mask":"fill_mask","image-classification":"image_classification","image-segmentation":"image_segmentation","image-to-image":"image_to_image","image-to-text":"image_to_text","object-detection":"object_detection","question-answering":"question_answering","sentence-similarity":"sentence_similarity",summarization:"summarization","table-question-answering":"table_question_answering","tabular-classification":"tabular_classification","tabular-regression":"tabular_regression","text-classification":"text_classification","text-generation":"text_generation","text-to-image":"text_to_image","text-to-speech":"text_to_speech","text-to-video":"text_to_video","token-classification":"token_classification",translation:"translation","visual-question-answering":"visual_question_answering","zero-shot-classification":"zero_shot_classification","zero-shot-image-classification":"zero_shot_image_classification"},_i={"automatic-speech-recognition":"automaticSpeechRecognition","feature-extraction":"featureExtraction","fill-mask":"fillMask","image-classification":"imageClassification","question-answering":"questionAnswering","sentence-similarity":"sentenceSimilarity",summarization:"summarization","table-question-answering":"tableQuestionAnswering","text-classification":"textClassification","text-generation":"textGeneration","text2text-generation":"textGeneration","token-classification":"tokenClassification",translation:"translation"},ki=(e,t)=>(n,i,a,o,r)=>{var s;let l=n.pipeline_tag;n.pipeline_tag&&["text-generation","image-text-to-text"].includes(n.pipeline_tag)&&n.tags.includes("conversational")&&(e=(null==r?void 0:r.streaming)?"conversationalStream":"conversational",t=Ai,l="conversational");const c=t?t(n,r):{inputs:Pt(n)},d=En(o??n.id,{accessToken:i,provider:a,...c},{task:l});let p=c;const u=d.info.body;if("string"==typeof u)try{p=JSON.parse(u)}catch(f){}const m={accessToken:i,authorizationHeader:null==(s=d.info.headers)?void 0:s.Authorization,baseUrl:(h=d.url,g="/chat/completions",h.endsWith(g)?h.slice(0,-g.length):h),fullUrl:d.url,inputs:{asObj:c,asCurlString:Ti(c,"curl"),asJsonString:Ti(c,"json"),asPythonString:Ti(c,"python"),asTsString:Ti(c,"ts")},providerInputs:{asObj:p,asCurlString:Ti(p,"curl"),asJsonString:Ti(p,"json"),asPythonString:Ti(p,"python"),asTsString:Ti(p,"ts")},model:n,provider:a,providerModelId:o??n.id};var h,g;return Kt.map((t=>bi[t].map((i=>{if(!((e,t,n)=>{var i,a;return void 0!==(null==(a=null==(i=fi[e])?void 0:i[t])?void 0:a[n])})(t,i,e))return;const a=yi(t,i,e);if("huggingface_hub"===i&&e.includes("basic")){if(!n.pipeline_tag||!(n.pipeline_tag in xi))return;m.methodName=xi[n.pipeline_tag]}if("huggingface.js"===i&&e.includes("basic")){if(!n.pipeline_tag||!(n.pipeline_tag in _i))return;m.methodName=_i[n.pipeline_tag]}let o=a(m).trim();if(o){if("huggingface_hub"===i){o=`${vi({...m})}\n\n${o}`}else if("requests"===i){const e=wi({...m,importBase64:o.includes("base64"),importJson:o.includes("json.")});o=`${e}\n\n${o}`}return{language:t,client:i,content:o}}})).filter((e=>void 0!==e)))).flat()},Ai=(e,t)=>({messages:(null==t?void 0:t.messages)??Pt(e),...(null==t?void 0:t.temperature)?{temperature:null==t?void 0:t.temperature}:void 0,max_tokens:(null==t?void 0:t.max_tokens)??512,...(null==t?void 0:t.top_p)?{top_p:null==t?void 0:t.top_p}:void 0}),Si={"audio-classification":ki("basicAudio"),"audio-to-audio":ki("basicAudio"),"automatic-speech-recognition":ki("basicAudio"),"document-question-answering":ki("documentQuestionAnswering",(e=>JSON.parse(Pt(e)))),"feature-extraction":ki("basic"),"fill-mask":ki("basic"),"image-classification":ki("basicImage"),"image-segmentation":ki("basicImage"),"image-text-to-text":ki("conversational"),"image-to-image":ki("imageToImage",(e=>{const t=JSON.parse(Pt(e));return{inputs:t.image,parameters:{prompt:t.prompt}}})),"image-to-text":ki("basicImage"),"object-detection":ki("basicImage"),"question-answering":ki("basic"),"sentence-similarity":ki("basic"),summarization:ki("basic"),"tabular-classification":ki("tabular"),"tabular-regression":ki("tabular"),"table-question-answering":ki("basic"),"text-classification":ki("basic"),"text-generation":ki("basic"),"text-to-audio":ki("textToAudio"),"text-to-image":ki("textToImage"),"text-to-speech":ki("textToAudio"),"text-to-video":ki("textToVideo"),"text2text-generation":ki("basic"),"token-classification":ki("basic"),translation:ki("basic"),"zero-shot-classification":ki("zeroShotClassification"),"zero-shot-image-classification":ki("zeroShotImageClassification")};function Ii(e,t,n,i,a){var o;return e.pipeline_tag&&e.pipeline_tag in Si?(null==(o=Si[e.pipeline_tag])?void 0:o.call(Si,e,t,n,i,a))??[]:[]}function Ti(e,t){switch(t){case"curl":return Ci(Ti(e,"json"));case"json":return JSON.stringify(e,null,4).split("\n").slice(1,-1).join("\n");case"python":return Ci(Object.entries(e).map((([e,t])=>`${e}=${JSON.stringify(t,null,4).replace(/"/g,'"')},`)).join("\n"));case"ts":return Ei(e).split("\n").slice(1,-1).join("\n");default:throw new Error(`Unsupported format: ${t}`)}}function Ei(e,t){if(t=t??0,"object"!=typeof e||null===e)return JSON.stringify(e);if(Array.isArray(e)){return`[\n${e.map((e=>{const n=Ei(e,t+1);return`${" ".repeat(4*(t+1))}${n},`})).join("\n")}\n${" ".repeat(4*t)}]`}return`{\n${Object.entries(e).map((([e,n])=>{const i=Ei(n,t+1),a=/^[a-zA-Z_$][a-zA-Z0-9_$]*$/.test(e)?e:`"${e}"`;return`${" ".repeat(4*(t+1))}${a}: ${i},`})).join("\n")}\n${" ".repeat(4*t)}}`}function Ci(e){return e.split("\n").map((e=>" ".repeat(4)+e)).join("\n")}const Mi={class:"container mx-auto px-4 py-8"},Li={class:"bg-gray-800 rounded-lg p-6 mb-6"},ji={class:"mb-4"},Ri={class:"bg-blue-900 rounded-lg p-4 mb-6"},Ui={class:"text-white"},Di={class:"bg-gray-800 rounded-lg p-6"},Pi={class:"mb-4"},qi={class:"mb-4"},Ni=["disabled"],$i={key:0,class:"mt-6 bg-gray-700 p-4 rounded"},Oi=["innerHTML"],zi={key:1,class:"mt-4 text-red-500"};const Bi=[{path:"/",component:q},{path:"/revers",component:Y},{path:"/coup-droit",component:Z},{path:"/filet",component:ie},{path:"/footwork",component:ee},{path:"/service&retour",component:te},{path:"/tactique",component:ne},{path:"/musculation",component:ae},{path:"/timer",component:be},{path:"/entrainement",component:we},{path:"/shadow",component:Ee},{path:"/gpt-training",component:T({name:"GptTraining",data:()=>({focusArea:"technique",duration:60,workout:null,loading:!1,error:null,hf:null,apiKey:"",apiKeyStatus:null,monthlyCount:0,monthKey:(new Date).toISOString().slice(0,7)}),created(){const e=localStorage.getItem("hf_api_key");e&&(this.apiKey=e,this.initializeHf(e)),this.loadMonthlyCount()},computed:{formattedWorkout(){return this.workout?this.workout.replace(/\n/g,"<br>"):""}},methods:{initializeHf(e){try{this.hf=new gi(e),this.apiKeyStatus={message:"Clé API configurée avec succès",color:"text-green-400"}}catch(t){this.apiKeyStatus={message:"Erreur de configuration de la clé API",color:"text-red-400"}}},updateApiKey(){this.apiKey?(localStorage.setItem("hf_api_key",this.apiKey),this.initializeHf(this.apiKey)):this.apiKeyStatus={message:"Veuillez entrer une clé API",color:"text-red-400"}},loadMonthlyCount(){const e=(new Date).toISOString().slice(0,7),t=localStorage.getItem("workoutGeneration");if(t){const n=JSON.parse(t);n.month===e?this.monthlyCount=n.count:(this.monthlyCount=0,this.saveMonthlyCount())}},saveMonthlyCount(){localStorage.setItem("workoutGeneration",JSON.stringify({month:this.monthKey,count:this.monthlyCount}))},async generateWorkout(){if(!this.hf)return void(this.error="Veuillez configurer votre clé API d'abord");if(this.monthlyCount>=30)return void(this.error="Limite mensuelle de générations atteinte");this.loading=!0,this.error=null;const e=`En tant qu'entraîneur de badminton national, génère un programme d'entraînement structuré avec les paramètres suivants:\n    \n    Informations générales:\n    - Durée totale: ${this.duration} minutes\n    - Type d'entraînement: ${this.focusArea}\n    - Niveau: National\n    \n    Structure requise:\n    1. Échauffement (15 minutes):\n       - Échauffement articulaire\n       - Déplacements spécifiques\n       - Shadow boxing progressive\n    \n    2. Corps de séance (${this.duration-20} minutes):\n       Inclure 3 exercices parmi les types suivants:\n    \n       A. Routine technique:\n       - Exemple: "6 coins" - 1 dégagé fond / 1 amorti / 1 lob / 1 smash / 1 contre-amorti\n       - Préciser: nombre de séries, durée, intensité\n    \n       B. Multi-volants:\n       - Exemple: "20 volants d'attaque" - alternance smash/amorti, zones ciblées\n       - Préciser: nombre de volants, récupération, nombre de séries\n    \n       C. Matchs à thème:\n       - Exemple: "Match 30 points" - perte de 5 points si volant au sol\n       - Préciser: règles spéciales, objectifs techniques\n    \n    3. Retour au calme (5 minutes):\n       - Étirements\n       - Débriefing\n    \n    Pour chaque exercice, détailler:\n    - Durée exacte\n    - Nombre de répétitions/séries\n    - Temps de récupération\n    - Points techniques à surveiller\n    - Objectifs spécifiques`;try{const t=(await this.hf.textGeneration({model:"mistralai/Mixtral-8x7B-Instruct-v0.1",inputs:e,parameters:{max_new_tokens:500,temperature:.7}})).generated_text;this.workout=t.substring(t.indexOf("Programme d'entraînement:")),this.workout||(this.workout=t.substring(e.length).trim()),this.monthlyCount++,this.saveMonthlyCount()}catch(t){this.error="Erreur lors de la génération de l'entraînement."}finally{this.loading=!1}}}},[["render",function(e,t,n,o,r,s){return p(),i("div",Mi,[t[11]||(t[11]=a("h1",{class:"text-3xl font-bold text-white mb-8"},"Assistant d'Entraînement IA",-1)),a("div",Li,[t[6]||(t[6]=a("h2",{class:"text-xl font-bold text-white mb-4"},"Configuration API",-1)),a("div",ji,[t[5]||(t[5]=a("label",{class:"block text-white mb-2"},"Clé API HuggingFace:",-1)),y(a("input",{type:"password","onUpdate:modelValue":t[0]||(t[0]=e=>r.apiKey=e),class:"w-full bg-gray-700 text-white rounded p-2",placeholder:"Entrez votre clé API"},null,512),[[v,r.apiKey]])]),a("button",{onClick:t[1]||(t[1]=(...e)=>s.updateApiKey&&s.updateApiKey(...e)),class:"bg-green-500 text-white px-4 py-2 rounded hover:bg-green-600"}," Mettre à jour la clé API "),r.apiKeyStatus?(p(),i("p",{key:0,class:_(["mt-2 text-sm",r.apiKeyStatus.color])},m(r.apiKeyStatus.message),3)):h("",!0)]),a("div",Ri,[a("p",Ui,[u(" Générations ce mois-ci: "+m(r.monthlyCount)+"/30 ",1),t[7]||(t[7]=a("span",{class:"text-sm text-gray-300 block mt-1"}," (Limite mensuelle: 30 générations) ",-1))])]),a("div",Di,[a("div",Pi,[t[9]||(t[9]=a("label",{class:"block text-white mb-2"},"Spécificité de l'entraînement:",-1)),y(a("select",{"onUpdate:modelValue":t[2]||(t[2]=e=>r.focusArea=e),class:"w-full bg-gray-700 text-white rounded p-2"},t[8]||(t[8]=[a("option",{value:"technique"},"Technique",-1),a("option",{value:"physique"},"Physique",-1),a("option",{value:"tactique"},"Tactique",-1),a("option",{value:"mixte"},"Entraînement complet",-1)]),512),[[k,r.focusArea]])]),a("div",qi,[t[10]||(t[10]=a("label",{class:"block text-white mb-2"},"Durée (minutes):",-1)),y(a("input",{type:"number","onUpdate:modelValue":t[3]||(t[3]=e=>r.duration=e),class:"w-full bg-gray-700 text-white rounded p-2",min:"30",max:"180"},null,512),[[v,r.duration]])]),a("button",{onClick:t[4]||(t[4]=(...e)=>s.generateWorkout&&s.generateWorkout(...e)),class:"bg-blue-500 text-white px-4 py-2 rounded hover:bg-blue-600",disabled:r.loading||!r.hf},m(r.loading?"Génération en cours...":"Générer un entraînement"),9,Ni),r.workout?(p(),i("div",$i,[a("div",{innerHTML:s.formattedWorkout,class:"text-white"},null,8,Oi)])):h("",!0),r.error?(p(),i("div",zi,m(r.error),1)):h("",!0)])])}]])}],Fi=A({history:S("/BadmintonTraining/"),routes:Bi}),Vi=I(N);Vi.use(Fi),Vi.mount("#app");
